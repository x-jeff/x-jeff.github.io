---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘Bag of Tricks for Image Classification with Convolutional Neural Networks
subtitle:   ResNet-vcï¼ŒResNet-vd
date:       2024-07-10
author:     x-jeff
header-img: blogimg/20220414.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨å‡ ä¹ä¸æ”¹å˜è®¡ç®—å¤æ‚åº¦çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å°è¯•ä¸åŒçš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹æ¶æ„refineæ¥æå‡æ¨¡å‹ç²¾åº¦ã€‚æˆ‘ä»¬æ‰€ç”¨çš„å¾ˆå¤šéƒ½æ˜¯ä¸€äº›å°æŠ€å·§ï¼Œæ¯”å¦‚ä¿®æ”¹å·ç§¯å±‚çš„æ­¥é•¿æˆ–è€…è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥ç­‰ã€‚æˆ‘ä»¬å°†æ‰€æœ‰çš„å°æŠ€å·§åº”ç”¨äº[ResNet-50](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ä¸Šï¼Œåœ¨ImageNetä¸Šçš„å®éªŒç»“æœè§è¡¨1ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/1.png)

æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¿™äº›æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„æ³›åŒ–åˆ°å…¶ä»–ç½‘ç»œæ¡†æ¶æˆ–ä»»åŠ¡é¢†åŸŸä¸­ã€‚æ¨¡å‹åŠæºç åœ°å€ï¼š[GluonCV](https://github.com/dmlc/gluon-cv)ã€‚

# 2.Training Procedures

å¸¸è§„çš„ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹æ³•å¦‚ä¸‹ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/2.png)

## 2.1.Baseline Training Procedure

æˆ‘ä»¬ä½¿ç”¨["Training and investigating Residual Nets"](http://torch.ch/blog/2016/02/04/resnets.html)ä¸­çš„ResNetå®ç°æ–¹å¼ä½œä¸ºbaselineã€‚è®­ç»ƒå’ŒéªŒè¯çš„é¢„å¤„ç†pipelineæ˜¯ä¸åŒçš„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

1. éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒï¼Œå°†åƒç´ å€¼è½¬ä¸º$[0,255]$èŒƒå›´å†…çš„32ä½æµ®ç‚¹æ•°ã€‚
2. éšæœºè£å‰ªä¸€å—çŸ©å½¢åŒºåŸŸï¼Œè¿™ä¸ªåŒºåŸŸçš„é•¿å®½æ¯”ä¸º$[3/4]$æˆ–$[4/3]$ï¼Œä¸”åŒºåŸŸé¢ç§¯å’Œæ•´å¹…å›¾åƒçš„æ¯”å€¼åœ¨$[8\%,100\%]$ä¹‹é—´ã€‚ç„¶åå°†è£å‰ªåŒºåŸŸresizeåˆ°$224 \times 224$ã€‚
3. æœ‰50%çš„å‡ ç‡è¿›è¡Œæ°´å¹³ç¿»è½¬ã€‚
4. å¯¹hueã€é¥±å’Œåº¦å’Œäº®åº¦è¿›è¡Œç¼©æ”¾ï¼Œç¼©æ”¾ç³»æ•°åœ¨$[0.6,1.4]$ä¹‹é—´å‡åŒ€é‡‡æ ·ã€‚
5. æ·»åŠ PCAå™ªå£°ï¼Œå…¶ç³»æ•°ä»æ­£æ€åˆ†å¸ƒ$\mathcal{N}(0,0.1)$ä¸­é‡‡æ ·ã€‚
6. å¯¹RGBé€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼Œå½’ä¸€åŒ–çš„æ–¹å¼ä¸ºRGBå¯¹åº”çš„ä¸‰ä¸ªé€šé“åˆ†åˆ«å‡å»123.68ã€116.779ã€103.939ï¼Œå†åˆ†åˆ«é™¤ä»¥58.393ã€57.12ã€57.375ã€‚

åœ¨éªŒè¯é˜¶æ®µï¼Œåœ¨ä¿æŒé•¿å®½æ¯”ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå°†çŸ­è¾¹resizeåˆ°256ä¸ªåƒç´ ã€‚ç„¶åï¼Œåœ¨ä¸­å¿ƒåŒºåŸŸè£å‰ªå‡º$224 \times 244$ï¼Œå¹¶å¯¹RGBé€šé“åšå½’ä¸€åŒ–ã€‚åœ¨éªŒè¯é˜¶æ®µï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ä»»ä½•éšæœºçš„æ•°æ®æ‰©å±•ã€‚

å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚çš„å‚æ•°åˆå§‹åŒ–éƒ½ä½¿ç”¨äº†[Xavierç®—æ³•](https://shichaoxin.com/2020/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%89%E8%AF%BE-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/)ã€‚å…·ä½“æ¥è¯´ï¼Œå‚æ•°åœ¨$[-a,a]$ä¹‹é—´å‡åŒ€é‡‡æ ·ï¼Œå…¶ä¸­ï¼Œ$a=\sqrt{6 / (d_{in} + d_{out})}$ã€‚è¿™é‡Œçš„$d_{in}$å’Œ$d_{out}$æ˜¯è¾“å…¥ã€è¾“å‡ºé€šé“çš„å¤§å°ã€‚æ‰€æœ‰çš„åç½®é¡¹éƒ½åˆå§‹åŒ–ä¸º0ã€‚å¯¹äº[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚ï¼Œ$\gamma$å‘é‡åˆå§‹åŒ–ä¸º1ï¼Œ$\beta$å‘é‡åˆå§‹åŒ–ä¸º0ã€‚

è®­ç»ƒä½¿ç”¨äº†NAGï¼ˆNesterov Accelerated Gradientï¼‰æ¢¯åº¦ä¸‹é™æ³•ã€‚æ¯ä¸ªæ¨¡å‹éƒ½è®­ç»ƒäº†120ä¸ªepochï¼Œä½¿ç”¨äº†8å—Nvidia V100 GPUï¼Œbatch size=256ã€‚å­¦ä¹ ç‡åˆå§‹åŒ–ä¸º0.1ï¼Œåœ¨ç¬¬30ã€60ã€90ä¸ªepochæ—¶é™¤ä»¥10ã€‚

>NAGåŸæ–‡ï¼šY. E. Nesterov. A method for solving the convex programming problem with convergence rate o (1/kË† 2). In Dokl.Akad. Nauk SSSR, volume 269, pages 543â€“547, 1983.

## 2.2.Experiment Results

æˆ‘ä»¬è¯„ä¼°äº†3ä¸ªCNNï¼š[ResNet-50](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ã€[Inception-V3](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/)å’ŒMobileNetã€‚å¯¹äº[Inception-V3](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/)ï¼Œæˆ‘ä»¬å°†è¾“å…¥å›¾åƒresizeåˆ°$299 \times 299$ã€‚æˆ‘ä»¬ä½¿ç”¨ISLVRC2012æ•°æ®é›†ï¼Œå…¶è®­ç»ƒé›†åŒ…å«1.3Må¼ å›¾åƒå’Œ1000ä¸ªç±»åˆ«ã€‚éªŒè¯ç²¾åº¦è§è¡¨2ã€‚

>MobileNetåŸæ–‡ï¼šA. G. Howard, M. Zhu, B. Chen, D. Kalenichenko,W.Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/3.png)

è¡¨2ä¸­ï¼ŒBaselineæ˜¯æˆ‘ä»¬å®ç°çš„ç»“æœï¼ŒReferenceæ˜¯åŸè®ºæ–‡ç»™å‡ºçš„ç»“æœã€‚

# 3.Efficient Training

ç¡¬ä»¶ï¼Œå°¤å…¶æ˜¯GPUï¼Œè¿‘å¹´æ¥å‘å±•è¿…é€Ÿã€‚å› æ­¤ï¼Œè®¸å¤šä¸æ€§èƒ½ç›¸å…³çš„æƒè¡¡å‘ç”Ÿäº†å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œç°åœ¨åœ¨è®­ç»ƒä¸­ï¼Œä½¿ç”¨è¾ƒä½çš„æ•°å€¼ç²¾åº¦å’Œè¾ƒå¤§çš„batch sizeæ›´æœ‰æ•ˆç‡ã€‚åœ¨æœ¬éƒ¨åˆ†ï¼Œæˆ‘ä»¬å›é¡¾äº†åœ¨ä¸ç‰ºç‰²æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹å®ç°ä½ç²¾åº¦å’Œlarge batchè®­ç»ƒçš„å„ç§æŠ€æœ¯ã€‚æœ‰äº›æŠ€æœ¯ç”šè‡³å¯ä»¥æé«˜å‡†ç¡®æ€§å’Œè®­ç»ƒé€Ÿåº¦ã€‚

## 3.1.Large-batch training

[mini-batch SGD](https://shichaoxin.com/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%BA%94%E8%AF%BE-mini-batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)å°†å¤šä¸ªæ ·æœ¬æ”¾åœ¨ä¸€ä¸ªmini-batchä¸­ï¼Œä»¥æé«˜å¹¶è¡Œæ€§å¹¶é™ä½æˆæœ¬ã€‚å¦‚æœä½¿ç”¨large batch sizeï¼Œå¯èƒ½ä¼šæ‹–æ…¢è®­ç»ƒè¿›ç¨‹ã€‚æ­¤å¤–ï¼Œåœ¨åŒæ ·çš„epochæ•°é‡ä¸‹ï¼Œlarge batch sizeè¿˜ä¼šå¯¼è‡´éªŒè¯ç²¾åº¦çš„ä¸‹é™ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»4ç§æ–¹æ³•æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

ğŸ‘‰**Linear scaling learning rate.**

åœ¨[mini-batch SGD](https://shichaoxin.com/2020/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%BA%94%E8%AF%BE-mini-batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)ä¸­ï¼Œæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œå› ä¸ºæ¯ä¸ªbatchä¸­çš„æ ·æœ¬éƒ½æ˜¯éšæœºé€‰æ‹©çš„ã€‚å¢åŠ batch sizeä¸ä¼šæ”¹å˜éšæœºæ¢¯åº¦çš„æœŸæœ›ï¼Œä½†ä¼šé™ä½å…¶æ–¹å·®ã€‚æ¢è¨€ä¹‹ï¼Œlarge batch sizeå‡å°‘äº†æ¢¯åº¦ä¸­çš„å™ªå£°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æé«˜å­¦ä¹ ç‡ã€‚è®ºæ–‡"P. Goyal, P. DollÂ´ar, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017."æŒ‡å‡ºï¼Œéšç€batch sizeçš„å¢åŠ ï¼Œå­¦ä¹ ç‡ä¹Ÿåº”è¯¥çº¿æ€§å¢åŠ ï¼Œè¿™å¯¹è®­ç»ƒ[ResNet-50](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)æ˜¯æœ‰æ•ˆçš„ã€‚[ResNetåŸæ–‡](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ä½¿ç”¨0.1ä½œä¸ºåˆå§‹å­¦ä¹ ç‡ï¼Œå…¶batch sizeä¸º256ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ç”¨æ›´å¤§çš„batch size $b$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å°†åˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º$0.1 \times b / 256$ã€‚

ğŸ‘‰**Learning rate warmup.**

åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ‰€æœ‰å‚æ•°éƒ½æ˜¯éšæœºå€¼ï¼Œå› æ­¤è¿œç¦»æœ€ç»ˆè§£ã€‚æ­¤æ—¶ä½¿ç”¨è¿‡å¤§çš„å­¦ä¹ ç‡å¯èƒ½ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚warmupçš„ç­–ç•¥æ˜¯ï¼Œä¸€å¼€å§‹ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œç„¶åå¾…è®­ç»ƒè¿‡ç¨‹ç¨³å®šåï¼Œå†åˆ‡æ¢å›åˆå§‹å­¦ä¹ ç‡ã€‚è®ºæ–‡"P. Goyal, P. DollÂ´ar, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017."æå‡ºäº†ä¸€ç§gradual warmupçš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†å­¦ä¹ ç‡ä»0çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡ã€‚æ¢è¨€ä¹‹ï¼Œå‡å®šæˆ‘ä»¬ç”¨å‰$m$ä¸ªbatchï¼ˆæ¯”å¦‚åˆšå¥½æ˜¯5ä¸ªepochï¼‰æ¥è¿›è¡Œwarmupï¼Œåˆå§‹å­¦ä¹ ç‡æ˜¯$\eta$ï¼Œå¯¹äºç¬¬$i$ä¸ªbatchï¼ˆ$1 \leqslant i \leqslant m$ï¼‰ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ç‡è®¾ç½®ä¸º$i\eta / m$ã€‚

ğŸ‘‰**Zero $\gamma$.**

[ResNetç½‘ç»œ](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)åŒ…å«å¤šä¸ªæ®‹å·®å—ï¼Œæ¯ä¸ªå—åŒ…å«å¤šä¸ªå·ç§¯å±‚ã€‚ç»™å®šè¾“å…¥$x$ï¼Œå—æœ€åä¸€å±‚çš„è¾“å‡ºä¸º$\text{block}(x)$ï¼Œåˆ™æ®‹å·®å—çš„è¾“å‡ºå¯è¡¨ç¤ºä¸º$x+\text{block}(x)$ã€‚å—æœ€åä¸€å±‚æ˜¯[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚ã€‚[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚é¦–å…ˆä¼šæ ‡å‡†åŒ–å®ƒçš„è¾“å…¥ï¼Œè®°ä¸º$\hat{x}$ï¼Œç„¶åæ‰§è¡Œ$\gamma \hat{x} + \beta$ã€‚å…¶ä¸­ï¼Œ$\gamma$å’Œ$\beta$éƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œåˆ†åˆ«è¢«åˆå§‹åŒ–ä¸º1å’Œ0ã€‚å¦‚æœæˆ‘ä»¬å°†$\gamma$åˆå§‹åŒ–ä¸º0ï¼Œä¼šä½¿å¾—ç½‘ç»œåœ¨åˆå§‹é˜¶æ®µæ›´å®¹æ˜“è¢«è®­ç»ƒã€‚

ğŸ‘‰**No bias decay.**

[weight decay](https://shichaoxin.com/2020/02/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE-%E6%AD%A3%E5%88%99%E5%8C%96/#314l1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8Cl2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB)é€šå¸¸åº”ç”¨äºæ‰€æœ‰å¯å­¦ä¹ çš„å‚æ•°ä¸Šï¼ŒåŒ…æ‹¬æƒé‡é¡¹å’Œåç½®é¡¹ã€‚è¿™é‡Œæˆ‘ä»¬ä»…å°†[weight decay](https://shichaoxin.com/2020/02/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE-%E6%AD%A3%E5%88%99%E5%8C%96/#314l1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8Cl2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB)åº”ç”¨äºå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚çš„æƒé‡ä¸Šä»¥é¿å…è¿‡æ‹Ÿåˆï¼Œåç½®é¡¹ä¸ä½¿ç”¨[weight decay](https://shichaoxin.com/2020/02/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE-%E6%AD%A3%E5%88%99%E5%8C%96/#314l1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8Cl2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB)ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒLARSæä¾›äº†layer-wiseçš„è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå£°ç§°å¯¹è¶…å¤§çš„batch sizeï¼ˆè¶…è¿‡16Kï¼‰ä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œè€ƒè™‘åˆ°åœ¨å•ä¸ªæœºå™¨ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å°†batch sizeé™åˆ¶åœ¨2Kä»¥å†…ã€‚

>LARSï¼šB. Ginsburg, I. Gitman, and Y. You. Large batch training of convolutional networks with layer-wise adaptive rate scaling. 2018.

## 3.2.Low-precision training

ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€šå¸¸ä½¿ç”¨32ä½æµ®ç‚¹çš„ç²¾åº¦ï¼ˆFP32ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰çš„æ•°å­—éƒ½ä»¥FP32çš„æ ¼å¼å­˜å‚¨ã€‚å¯¹äºNvidia V100 GPUæ¥è¯´ï¼ŒFP32æ”¯æŒ14 TFLOPSï¼Œè€ŒFP16æ”¯æŒ100 TFLOPSã€‚å¦‚è¡¨3æ‰€ç¤ºï¼Œåœ¨V100ä¸Šï¼Œå°†FP32æ”¹ä¸ºFP16åï¼Œè®­ç»ƒé€Ÿåº¦åŠ å¿«äº†2-3å€ã€‚

>FLOPï¼ˆFloating-Point Operationï¼Œæµ®ç‚¹è¿ç®—ï¼‰ï¼šè®¡ç®—æœºç”¨æ¥å¤„ç†å°æ•°çš„è¿ç®—ã€‚
>
>FLOPSï¼ˆFloating-Point Operations Per Secondï¼Œæ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰ï¼šè¡¡é‡è®¡ç®—æœºå¤„ç†å™¨çš„è®¡ç®—èƒ½åŠ›ï¼Œå•ä½æ˜¯æ¯ç§’èƒ½å®Œæˆçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚FLOPSæ˜¯è®¡ç®—èƒ½åŠ›çš„åŸºæœ¬å•ä½ï¼Œå¸¸ç”¨çš„å•ä½è¿˜æœ‰GFLOPSï¼ˆGigaFLOPSï¼Œ$10^9$æ¬¡æ¯ç§’ï¼‰ã€TFLOPSï¼ˆTeraFLOPSï¼Œ$10^{12}$æ¬¡æ¯ç§’ï¼‰ã€PFLOPSï¼ˆPetaFLOPSï¼Œ$10^{15}$æ¬¡æ¯ç§’ï¼‰ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/4.png)

## 3.3.Experiment Results

æ¶ˆèå®éªŒè§è¡¨4ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/5.png)

# 4.Model Tweaks

Model TweaksæŒ‡çš„æ˜¯å¯¹ç½‘ç»œæ¡†æ¶è¿›è¡Œå¾®å°çš„è°ƒæ•´ï¼Œæ¯”å¦‚æ”¹å˜ç‰¹å®šå·ç§¯å±‚çš„æ­¥é•¿ã€‚è¿™æ ·çš„å¾®è°ƒå‡ ä¹ä¸ä¼šæ”¹å˜è®¡ç®—å¤æ‚åº¦ï¼Œä½†å¯èƒ½ä¼šå¯¹æ¨¡å‹ç²¾åº¦äº§ç”Ÿä¸å¯å¿½è§†çš„å½±å“ã€‚åœ¨æœ¬éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»¥[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ä¸ºä¾‹æ¥ç ”ç©¶æ¨¡å‹å¾®è°ƒçš„æ•ˆæœã€‚

## 4.1.ResNet Architecture

åŸå§‹çš„[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)æ¡†æ¶è§ä¸‹ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/6.png)

å¯ä»¥å’Œ[ResNetåŸæ–‡ä¸­çš„è¡¨1](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ç»“åˆèµ·æ¥çœ‹ã€‚æ¯ä¸ªstageçš„å¼€å§‹éƒ½æ˜¯ä¸€ä¸ªä¸‹é‡‡æ ·å—ï¼Œç„¶åæ¥å¤šä¸ªæ®‹å·®å—ã€‚ä¸‹é‡‡æ ·å—æœ‰Aã€Bä¸¤æ¡è·¯å¾„ã€‚è·¯å¾„AåŒ…å«3ä¸ªå·ç§¯ï¼Œæ ¸å¤§å°åˆ†åˆ«ä¸º$1 \times 1$ã€$3 \times 3$å’Œ$1 \times 1$ã€‚ç¬¬1ä¸ªå·ç§¯çš„æ­¥é•¿ä¸º2ï¼Œç”¨äºå°†è¾“å…¥çš„é•¿å’Œå®½å‡åŠï¼Œæœ€åä¸€ä¸ªå·ç§¯è¾“å‡ºçš„é€šé“æ•°æ˜¯å‰é¢çš„4å€ï¼Œå³bottleneckç»“æ„ã€‚è·¯å¾„Bçš„å·ç§¯æ­¥é•¿ä¸º2ï¼Œè¾“å‡ºé€šé“æ•°å’Œè·¯å¾„Aä¸€æ ·ï¼Œè¿™æ ·æ–¹ä¾¿å’Œè·¯å¾„Açš„è¾“å‡ºåŠ åœ¨ä¸€èµ·ã€‚æ®‹å·®å—å’Œä¸‹é‡‡æ ·å—çš„ç»“æ„åŸºæœ¬ä¸€æ ·ï¼Œåªä¸è¿‡æ­¥é•¿éƒ½æ˜¯1ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´æ¯ä¸ªstageæ®‹å·®å—çš„æ•°é‡æ¥è·å¾—ä¸åŒçš„[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)æ¨¡å‹ï¼Œæ¯”å¦‚[ResNet-50](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)å’Œ[ResNet-152](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ã€‚

## 4.2.ResNet Tweaks

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å›é¡¾äº†ä¸¤ç§æ¯”è¾ƒæµè¡Œçš„ResNetè°ƒæ•´æ–¹æ¡ˆï¼Œæˆ‘ä»¬åˆ†åˆ«å°†å®ƒä»¬ç§°ä¸ºResNet-Bå’ŒResNet-Cã€‚ä¹‹åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è°ƒæ•´æ–¹æ¡ˆï¼Œç§°ä¸ºResNet-Dã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/7.png)

ğŸ‘‰**ResNet-B.**

è¿™ç§è°ƒæ•´æœ€æ—©å‡ºè‡ªTorchçš„å®ç°ï¼šS. Gross and M. Wilber. Training and investigating residual nets. http://torch.ch/blog/2016/02/04/resnets.html.ã€‚å®ƒä¿®æ”¹äº†ResNetçš„ä¸‹é‡‡æ ·å—ã€‚å› ä¸ºåŸæ¥æ­¥é•¿ä¸º2çš„$1\times 1$å·ç§¯ä½¿å¾—è·¯å¾„Aå¿½ç•¥äº†å››åˆ†ä¹‹ä¸‰çš„è¾“å…¥feature mapã€‚æ‰€ä»¥ï¼Œå¦‚Fig2(a)æ‰€ç¤ºï¼ŒResNet-Bå°†æ­¥é•¿ä¸º2æ”¾åœ¨äº†$3\times 3$å·ç§¯ä¸­ï¼Œè¿™æ ·å°±æ²¡æœ‰ä¿¡æ¯è¢«å¿½ç•¥äº†ã€‚

ğŸ‘‰**ResNet-C.**

è¿™ç§è°ƒæ•´æœ€æ—©æ˜¯[Inception-V2](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/#6inception-v2)æå‡ºæ¥çš„ï¼ˆè§[Inception-V2åŸæ–‡ä¸­çš„è¡¨1](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/#6inception-v2)ï¼‰ã€‚è¿™ä¸ªè°ƒæ•´ä¸»è¦æ˜¯é’ˆå¯¹ResNetä¸­çš„conv1ï¼Œå°†$7 \times 7$å·ç§¯æ‹†åˆ†ä¸ºå¤šä¸ªè¿ç»­çš„$3 \times 3$å·ç§¯ï¼Œå¦‚Fig2(b)æ‰€ç¤ºã€‚

ğŸ‘‰**ResNet-D.**

å—åˆ°ResNet-Bçš„å¯å‘ï¼Œæˆ‘ä»¬æ„è¯†åˆ°è·¯å¾„Bä¸­çš„$1\times 1$ä¸‹é‡‡æ ·ä¹Ÿä¼šå¿½ç•¥$3/4$çš„è¾“å…¥feature mapï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œä¿®æ”¹ä½¿å¾—æ²¡æœ‰ä¿¡æ¯å†è¢«å¿½ç•¥ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œåœ¨å·ç§¯ä¹‹å‰ï¼Œæ·»åŠ ä¸€ä¸ªæ­¥é•¿ä¸º2çš„$2\times 2$å¹³å‡æ± åŒ–ï¼Œå¹¶æŠŠå·ç§¯çš„æ­¥é•¿æ”¹ä¸º1ï¼Œå¦‚Fig2(c)æ‰€ç¤ºã€‚è¿™ä¸ªæ”¹åŠ¨åœ¨å®é™…åº”ç”¨ä¸­æ•ˆæœå¾ˆå¥½ï¼Œä¸”å¯¹è®¡ç®—æˆæœ¬å½±å“å¾ˆå°ã€‚

>åœ¨[PaddlePaddleæ–‡æ¡£](https://paddleclas.readthedocs.io/zh-cn/latest/models/ResNet_and_vd.html)ä¸­ï¼ŒResNet-Cè¢«è®°ä¸ºResNet-vcï¼ŒResNet-Dè¢«è®°ä¸ºResNet-vdã€‚

## 4.3.Experiment Results

ç»“æœè§è¡¨5ï¼Œæ¨¡å‹batch size=1024ï¼Œä½¿ç”¨FP16ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/8.png)

# 5.Training Refinements

æœ¬éƒ¨åˆ†ä»‹ç»4ç§è®­ç»ƒrefineæ–¹æ³•æ¥æå‡æ¨¡å‹ç²¾åº¦ã€‚

## 5.1.Cosine Learning Rate Decay

å­¦ä¹ ç‡çš„è°ƒæ•´å¯¹è®­ç»ƒè‡³å…³é‡è¦ã€‚åœ¨ç»è¿‡ç¬¬3.1éƒ¨åˆ†æåˆ°çš„warmupä¹‹åï¼Œåˆå§‹å­¦ä¹ ç‡ä¼šå¼€å§‹ç¨³æ­¥é™ä½ã€‚

è®ºæ–‡"I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with restarts. CoRR, abs/1608.03983, 2016."æå‡ºäº†ä½™å¼¦é€€ç«ç­–ç•¥ï¼ˆcosine annealing strategyï¼‰ã€‚ä¸€ä¸ªç®€å•çš„ç‰ˆæœ¬æ˜¯éµå¾ªä½™å¼¦å‡½æ•°ï¼Œå°†åˆå§‹å­¦ä¹ ç‡é™ä¸º0ã€‚å‡å®šæ€»çš„batchæ•°é‡ä¸º$T$ï¼ˆä¸è€ƒè™‘warmupï¼‰ï¼Œåœ¨ç¬¬$t$ä¸ªbatchï¼Œå­¦ä¹ ç‡$\eta _t$ä¸ºï¼š

$$\eta _t = \frac{1}{2}\left( 1 + \cos \left( \frac{t \pi}{T} \right) \right) \eta \tag{1}$$

å…¶ä¸­ï¼Œ$\eta$æ˜¯åˆå§‹å­¦ä¹ ç‡ã€‚æˆ‘ä»¬ä¹Ÿå°†è¿™ç§æ–¹æ³•ç§°ä¸ºä½™å¼¦è¡°å‡ï¼ˆâ€œcosineâ€ decayï¼‰ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/9.png)

## 5.2.Label Smoothing

è§["Model Regularization via Label Smoothing"](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/#7model-regularization-via-label-smoothing)ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/10.png)

## 5.3.Knowledge Distillation

åœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•™å¸ˆæ¨¡å‹æ¥å¸®åŠ©è®­ç»ƒç°åœ¨çš„æ¨¡å‹ï¼ˆå³å­¦ç”Ÿæ¨¡å‹ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨ResNet-152ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ŒResNet-50ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ã€‚

>çŸ¥è¯†è’¸é¦ï¼šG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

## 5.4.Mixup Training

è§[mixup](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#34yolov4)ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚

## 5.5.Experiment Results

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/11.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/12.png)

# 6.Transfer Learning

## 6.1.Object Detection

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/13.png)

## 6.2.Semantic Segmentation

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ResNetvd/14.png)

# 7.Conclusion

å¯¹å…¨æ–‡çš„æ€»ç»“ï¼Œä¸å†è¯¦è¿°ã€‚

# 8.åŸæ–‡é“¾æ¥

ğŸ‘½[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://github.com/x-jeff/AI_Papers/blob/master/2024/Bag%20of%20Tricks%20for%20Image%20Classification%20with%20Convolutional%20Neural%20Networks.pdf)