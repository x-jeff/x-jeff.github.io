---
layout:     post
title:      ã€ä»é›¶å¼€å§‹æ„å»ºå¤§è¯­è¨€æ¨¡å‹ã€‘ã€7ã€‘ã€Fine-tuning to follow instructionsã€‘
subtitle:   Introduction to instruction fine-tuningï¼ŒPreparing a dataset for supervised instruction fine-tuningï¼ŒOrganizing data into training batchesï¼ŒCreating data loaders for an instruction datasetï¼ŒLoading a pretrained LLMï¼ŒFine-tuning the LLM on instruction dataï¼ŒExtracting and saving responsesï¼ŒEvaluating the fine-tuned LLM
date:       2025-03-31
author:     x-jeff
header-img: blogimg/20201017.jpg
catalog: true
tags:
    - Large Language Models
---
>ã€ä»é›¶å¼€å§‹æ„å»ºå¤§è¯­è¨€æ¨¡å‹ã€‘ç³»åˆ—åšå®¢ä¸º"Build a Large Language Model (From Scratch)"ä¸€ä¹¦çš„ä¸ªäººè¯»ä¹¦ç¬”è®°ã€‚
>
>* åŸä¹¦é“¾æ¥ï¼š[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167?crid=228R4JI0P0QFR&dib=eyJ2IjoiMSJ9.XvZyIer9iV133BWXqNiVt_OOJXZheO54dvZtQly8MC25PNYZrN3OWsGLjbg3I0G9hI3LkjwhsORxvHIob3nvCZFgdSSQEFe07VkehijGxT03n4Amdw7lnXxnsOUuWXeglfHnewCcV3DjL9zWHELfh5DG1ZErzFym3S6ZxSuFzNvoPkaq0uDlD_CKwqHdC0KM_RdvIqF0_2RudgvzRli0V155KkusHRck3pG7ybp5VyqKDC_GgL_MEywLwLhFgX6kOCgV6Rq90eTgSHFd6ac8krpIYjsHWe6H3IXbfKGvMXc.473O1-iUZC0z2hdx8L5Z5ZTNxtNV9gNPw_mE7QZ5Y90&dib_tag=se&keywords=raschka&qid=1730250834&sprefix=raschk,aps,162&sr=8-1&linkCode=sl1&tag=rasbt03-20&linkId=84ee23afbd12067e4098443718842dac&language=en_US&ref_=as_li_ss_tl)ã€‚
>* å®˜æ–¹ç¤ºä¾‹ä»£ç ï¼š[LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)ã€‚
>
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Fine-tuning to follow instructions

instruction fine-tuningæ˜¯å¼€å‘ç”¨äºèŠå¤©æœºå™¨äººã€ä¸ªäººåŠ©æ‰‹ä»¥åŠå…¶ä»–å¯¹è¯ç±»ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚

![Fig7.1](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/1.png)

# 2.Introduction to instruction fine-tuning

æˆ‘ä»¬ç°åœ¨å·²ç»äº†è§£ï¼Œé¢„è®­ç»ƒLLMæ˜¯ä¸€ä¸ªé€è¯ç”Ÿæˆçš„è®­ç»ƒè¿‡ç¨‹ã€‚ç»è¿‡é¢„è®­ç»ƒåï¼ŒLLMå…·å¤‡äº†æ–‡æœ¬è¡¥å…¨çš„èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ç‰‡æ®µå®Œæˆå¥å­æˆ–ç”Ÿæˆæ®µè½ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒçš„LLMåœ¨åº”å¯¹å…·ä½“æŒ‡ä»¤æ–¹é¢å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œæ¯”å¦‚â€œä¿®æ­£è¿™æ®µæ–‡å­—çš„è¯­æ³•â€æˆ–â€œå°†è¿™æ®µæ–‡å­—æ”¹ä¸ºè¢«åŠ¨è¯­æ€â€ã€‚ç¨åï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå…·ä½“ç¤ºä¾‹æ¥æ¢è®¨å¦‚ä½•åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œinstruction fine-tuningï¼Œè¿™ä¸€è¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸ºæœ‰ç›‘ç£çš„instruction fine-tuningã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯æå‡LLMç†è§£å’Œæ‰§è¡Œæ­¤ç±»æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆç¬¦åˆé¢„æœŸçš„å“åº”ï¼Œå¦‚Fig7.2æ‰€ç¤ºã€‚æ•°æ®é›†çš„å‡†å¤‡æ˜¯instruction fine-tuningä¸­çš„å…³é”®æ­¥éª¤ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®Œæˆinstruction fine-tuningæµç¨‹ä¸­çš„ä¸‰ä¸ªé˜¶æ®µçš„æ‰€æœ‰æ­¥éª¤ï¼Œä»æ•°æ®é›†å‡†å¤‡å¼€å§‹ï¼Œå¦‚Fig7.3æ‰€ç¤ºã€‚

![Fig7.2](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/2.png)

![Fig7.3](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/3.png)

# 3.Preparing a dataset for supervised instruction fine-tuning

æ‰€è¦ä¸‹è½½çš„æ•°æ®é›†åŒ…å«äº†1100æ¡ç±»ä¼¼äºFig7.2ä¸­æ‰€ç¤ºçš„instructionâ€“responseå¯¹ã€‚æ•°æ®é›†é‡‡ç”¨JSONæ ¼å¼ã€‚

```python
#Downloading the dataset
import json
import os
import urllib


def download_and_load_file(file_path, url):

    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)

    # The book originally contained this unnecessary "else" clause:
    #else:
    #    with open(file_path, "r", encoding="utf-8") as file:
    #        text_data = file.read()

    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    return data


file_path = "instruction-data.json"
url = (
    "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch"
    "/main/ch07/01_main-chapter-code/instruction-data.json"
)

data = download_and_load_file(file_path, url)
print("Number of entries:", len(data)) #Number of entries: 1100
```

æˆ‘ä»¬æŸ¥çœ‹ä¸‹æ•°æ®é›†ä¸­çš„ä¸¤ä¸ªç¤ºä¾‹ï¼š

```python
print("Example entry:\n", data[50])
print("Another example entry:\n", data[999])
```

è¾“å‡ºä¸ºï¼š

```
Example entry:
 {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': "The correct spelling is 'Occasion.'"}
Another example entry:
 {'instruction': "What is an antonym of 'complicated'?", 'input': '', 'output': "An antonym of 'complicated' is 'simple'."}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¡æ•°æ®åŒ…å«3ä¸ªéƒ¨åˆ†ï¼š`'instruction'`ã€`'input'`å’Œ`'output'`ã€‚

instruction fine-tuningçš„è¿‡ç¨‹æ˜¯è®©æ¨¡å‹åœ¨ä¸€ä¸ªæ˜ç¡®æä¾›è¾“å…¥-è¾“å‡ºå¯¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®å¯¹å°±åƒæˆ‘ä»¬ä»JSONæ–‡ä»¶ä¸­æå–çš„é‚£æ ·ã€‚é’ˆå¯¹LLMï¼Œæœ‰å¤šç§æ–¹æ³•å¯ä»¥å¯¹è¿™äº›æ•°æ®è¿›è¡Œæ ¼å¼åŒ–ã€‚Fig7.4å±•ç¤ºäº†ä¸¤ç§ä¸åŒçš„ç¤ºä¾‹æ ¼å¼ï¼Œè¿™äº›æ ¼å¼é€šå¸¸è¢«ç§°ä¸º**æç¤ºé£æ ¼ï¼ˆprompt stylesï¼‰**ï¼Œåœ¨è®­ç»ƒè¯¸å¦‚Alpacaå’ŒPhi-3ç­‰çŸ¥åLLMæ—¶éƒ½æœ‰ä½¿ç”¨ã€‚

![Fig7.4](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/4.png)

æˆ‘ä»¬å°†é‡‡ç”¨Alpacaçš„æç¤ºé£æ ¼ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å—æ¬¢è¿çš„é£æ ¼ä¹‹ä¸€ã€‚

ç°åœ¨æˆ‘ä»¬æ¥å®šä¹‰ä¸€ä¸ªåä¸º`format_input`çš„å‡½æ•°ï¼Œç”¨äºå°†æ•°æ®åˆ—è¡¨ä¸­çš„æ¡ç›®è½¬æ¢ä¸ºAlpacaé£æ ¼çš„è¾“å…¥æ ¼å¼ã€‚

```python
#Implementing the prompt formatting function
def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text

model_input = format_input(data[50])
desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```

è¾“å‡ºä¸ºï¼š

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Identify the correct spelling of the following word.

### Input:
Ocassion

### Response:
The correct spelling is 'Occasion.'
```

å¦ä¸€ä¸ª`'input'`ä¸ºç©ºçš„ä¾‹å­ï¼š

```python
model_input = format_input(data[999])
desired_response = f"\n\n### Response:\n{data[999]['output']}"

print(model_input + desired_response)
```

è¾“å‡ºä¸ºï¼š

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
What is an antonym of 'complicated'?

### Response:
An antonym of 'complicated' is 'simple'.
```

ç„¶åæˆ‘ä»¬å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

```python
#Partitioning the dataset
train_portion = int(len(data) * 0.85)  # 85% for training
test_portion = int(len(data) * 0.1)    # 10% for testing
val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]

print("Training set length:", len(train_data)) #Training set length: 935
print("Validation set length:", len(val_data)) #Validation set length: 55
print("Test set length:", len(test_data)) #Test set length: 110
```

# 4.Organizing data into training batches

![Fig7.5](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/5.png)

åœ¨[ç¬¬6ç« ](https://shichaoxin.com/2025/03/21/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-6-Fine-tuning-for-classification/)ä¸­ï¼Œè®­ç»ƒbatchæ˜¯ç”±PyTorchçš„`DataLoader`ç±»è‡ªåŠ¨åˆ›å»ºçš„ï¼Œè¯¥ç±»ä½¿ç”¨é»˜è®¤çš„collateå‡½æ•°å°†æ ·æœ¬ç»„åˆæˆbatchã€‚collateå‡½æ•°çš„ä½œç”¨æ˜¯å°†ä¸€ä¸ªä¸ªç‹¬ç«‹çš„æ•°æ®æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªbatchï¼Œä»è€Œä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†è¿™äº›æ•°æ®ã€‚

ç„¶è€Œï¼Œinstruction fine-tuningçš„batchè¿‡ç¨‹è¦æ›´å¤æ‚ä¸€äº›ï¼Œè¿™å°±éœ€è¦æˆ‘ä»¬è‡ªå®šä¹‰ä¸€ä¸ªcollateå‡½æ•°ï¼Œç„¶åä¼šå°†å…¶ä¼ å…¥`DataLoader`ä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥è¦å®ç°è¿™ä¸ªè‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œæ˜¯ä¸ºäº†æ»¡è¶³instruction fine-tuningæ•°æ®é›†åœ¨æ ¼å¼å’Œå¤„ç†ä¸Šçš„ç‰¹æ®Šéœ€æ±‚ã€‚

æˆ‘ä»¬å°†åˆ†å‡ ä¸ªæ­¥éª¤æ¥å®ç°batchè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç¼–å†™è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œå¦‚Fig7.6æ‰€ç¤ºã€‚

![Fig7.6](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/6.png)

é¦–å…ˆï¼Œä¸ºäº†å®ç°æ­¥éª¤2.1å’Œæ­¥éª¤2.2ï¼Œæˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ª`InstructionDataset`ç±»ï¼Œå®ƒä¼šå¯¹æ•°æ®é›†ä¸­çš„æ‰€æœ‰è¾“å…¥åº”ç”¨`format_input`å‡½æ•°å¹¶è¿›è¡Œé¢„å…ˆåˆ†è¯ï¼ˆ**pretokenizes**ï¼‰ï¼Œè¯¦è§Fig7.7ã€‚

![Fig7.7](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/7.png)

```python
#Implementing an instruction dataset class
import torch
from torch.utils.data import Dataset


class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)
```

ç„¶åéœ€è¦å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œpaddingï¼Œä½¿å…¶é•¿åº¦ä¸€è‡´ï¼Œæˆ‘ä»¬ä¾æ—§ä½¿ç”¨`<|endoftext|>`ä½œä¸ºpadding tokenã€‚

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")

print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"})) #[50256]
```

æ¥ä¸‹æ¥è¿›å…¥Fig7.6ä¸­çš„æ­¥éª¤2.3ï¼Œæˆ‘ä»¬å°†å¼€å‘ä¸€ä¸ªè‡ªå®šä¹‰çš„collateå‡½æ•°ã€‚è¿™ä¸ªè‡ªå®šä¹‰çš„collateå‡½æ•°ä¼šå¯¹æ¯ä¸ªbatchä¸­çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œpaddingï¼Œä½¿å®ƒä»¬çš„é•¿åº¦ä¸€è‡´ï¼ŒåŒæ—¶å…è®¸ä¸åŒçš„batchå…·æœ‰ä¸åŒçš„æœ€å¤§é•¿åº¦ï¼Œå¦‚Fig7.8æ‰€ç¤ºã€‚è¿™ç§åšæ³•é€šè¿‡ä»…å°†åºåˆ—paddingåˆ°å½“å‰batchä¸­æœ€é•¿çš„æ ·æœ¬é•¿åº¦ï¼Œä»è€Œå‡å°‘äº†ä¸å¿…è¦çš„paddingï¼Œæé«˜äº†æ•ˆç‡ã€‚

![Fig7.8](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/8.png)

```python
def custom_collate_draft_1(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    # Find the longest sequence in the batch
    # and increase the max length by +1, which will add one extra
    # padding token below
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs
    inputs_lst = []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to batch_max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        # Via padded[:-1], we remove the extra padded token
        # that has been added via the +1 setting in batch_max_length
        # (the extra padding token will be relevant in later codes)
        inputs = torch.tensor(padded[:-1])
        inputs_lst.append(inputs)

    # Convert list of inputs to tensor and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    return inputs_tensor

inputs_1 = [0, 1, 2, 3, 4]
inputs_2 = [5, 6]
inputs_3 = [7, 8, 9]

batch = (
    inputs_1,
    inputs_2,
    inputs_3
)

print(custom_collate_draft_1(batch))
```

è¾“å‡ºä¸ºï¼š

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
```

æ¥ä¸‹æ¥éœ€è¦ç»§ç»­å¯¹è‡ªå®šä¹‰çš„collateå‡½æ•°è¿›è¡Œä¿®æ”¹ï¼Œä½¿å…¶é™¤äº†è¿”å›è¾“å…¥token IDå¤–ï¼Œè¿˜è¿”å›ç›®æ ‡token IDã€‚

![Fig7.9](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/9.png)

ä¸æˆ‘ä»¬åœ¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ä½¿ç”¨çš„æµç¨‹ç±»ä¼¼ï¼Œç›®æ ‡token IDä¸è¾“å…¥token IDç›¸åŒï¼Œä½†å‘å³åç§»ä¸€ä¸ªä½ç½®ã€‚å¦‚Fig7.10æ‰€ç¤ºï¼Œè¿™æ ·çš„è®¾ç½®ä½¿å¾—LLMèƒ½å¤Ÿå­¦ä¹ å¦‚ä½•é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªtokenã€‚

![Fig7.10](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/10.png)

```python
def custom_collate_draft_2(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets
        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs to tensor and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor

inputs, targets = custom_collate_draft_2(batch)
print(inputs)
print(targets)
```

è¾“å‡ºä¸ºï¼š

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256, 50256, 50256, 50256],
        [    8,     9, 50256, 50256, 50256]])
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„padding tokenåˆ†é…ä¸€ä¸ªå ä½å€¼`-100`ï¼Œå¦‚Fig7.11æ‰€ç¤ºã€‚è¿™ä¸ªç‰¹æ®Šçš„å€¼å¯ä»¥è®©æˆ‘ä»¬åœ¨è®¡ç®—è®­ç»ƒæŸå¤±æ—¶å¿½ç•¥è¿™äº›padding tokenï¼Œä»è€Œç¡®ä¿åªæœ‰æœ‰æ„ä¹‰çš„æ•°æ®æ‰ä¼šå½±å“æ¨¡å‹çš„å­¦ä¹ ï¼ˆåœ¨è¿›è¡Œclassification fine-tuningæ—¶ï¼Œæˆ‘ä»¬æ— éœ€è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œå› ä¸ºå½“æ—¶åªä½¿ç”¨äº†æ¨¡å‹çš„æœ€åä¸€ä¸ªè¾“å‡ºtokenè¿›è¡Œè®­ç»ƒï¼‰ã€‚

![Fig7.11](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/11.png)

ä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šåœ¨ç›®æ ‡åˆ—è¡¨ä¸­ä¿ç•™ä¸€ä¸ªæ–‡æœ¬ç»“æŸæ ‡è®°ï¼ˆend-of-text tokenï¼‰ï¼Œå…¶IDä¸º50256ï¼Œå¦‚Fig7.12æ‰€ç¤ºã€‚ä¿ç•™è¯¥æ ‡è®°å¯ä»¥è®©LLMå­¦ä¼šåœ¨å“åº”æŒ‡ä»¤æ—¶ä½•æ—¶ç”Ÿæˆæ–‡æœ¬ç»“æŸæ ‡è®°ï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸ºåˆ¤æ–­ç”Ÿæˆå“åº”æ˜¯å¦å®Œæˆçš„ä¸€ä¸ªæ ‡å¿—ã€‚

![Fig7.12](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/12.png)

åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯¹è‡ªå®šä¹‰çš„collateå‡½æ•°è¿›è¡Œäº†ä¿®æ”¹ï¼Œå°†ç›®æ ‡åˆ—è¡¨ä¸­IDä¸º`50256`çš„tokenæ›¿æ¢ä¸º`-100`ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¯é€‰å‚æ•°`allowed_max_length`ï¼Œç”¨äºé™åˆ¶æ ·æœ¬çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ‰“ç®—ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼Œè€Œè¯¥æ•°æ®é›†è¶…è¿‡äº†[GPT-2](https://shichaoxin.com/2024/03/20/LLM-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82ChatGPT%E8%83%8C%E5%90%8E%E7%9A%84%E6%8A%80%E6%9C%AF/#2gpt2)æ¨¡å‹æ”¯æŒçš„1024ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿™ä¸€è°ƒæ•´å°†éå¸¸æœ‰ç”¨ã€‚

```python
#Implementing a custom batch collate function
def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu" #deviceå¯ä»¥æ˜¯cpuï¼Œcudaï¼ˆé’ˆå¯¹NVIDIA GPUï¼‰ï¼Œmpsï¼ˆé’ˆå¯¹Apple SiliconèŠ¯ç‰‡çš„Macï¼‰
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # New: Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor

inputs, targets = custom_collate_fn(batch)
print(inputs)
print(targets)
```

è¾“å‡ºä¸ºï¼š

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256,  -100,  -100,  -100],
        [    8,     9, 50256,  -100,  -100]])
```

é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ä½¿ç”¨`-100`çš„åŸå› ã€‚

```python
logits_1 = torch.tensor(
    [[-1.0, 1.0],  # 1st training example
     [-0.5, 1.5]]  # 2nd training example
)
targets_1 = torch.tensor([0, 1])


loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)
print(loss_1) #tensor(1.1269)

logits_2 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5],
     [-0.5, 1.5]]  # New 3rd training example
)
targets_2 = torch.tensor([0, 1, 1])

loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)
print(loss_2) #tensor(0.7936)

targets_3 = torch.tensor([0, 1, -100]) #æŠŠtargets_2ä¸­çš„ç¬¬ä¸‰ä¸ªå€¼æ¢æˆ-100

loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)
print(loss_3) #tensor(1.1269)
print("loss_1 == loss_3:", loss_1 == loss_3) #loss_1 == loss_3: tensor(True)
```

ä»ä¸Šè¿°ä¾‹å­å¯ä»¥çœ‹åˆ°ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°è‡ªåŠ¨å¿½ç•¥äº†`targets_3`ä¸­å€¼ä¸º`-100`çš„æ ·æœ¬ã€‚è¿™æ˜¯å› ä¸ºPyTorchä¸­çš„äº¤å‰ç†µå‡½æ•°é»˜è®¤è®¾ç½®ä¸º`cross_entropy(..., ignore_index=-100)`ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¼šå¿½ç•¥ç›®æ ‡ä¸­æ ‡è®°ä¸º`-100`çš„ä½ç½®ã€‚

é™¤äº†å±è”½padding tokenå¤–ï¼Œé€šå¸¸è¿˜ä¼šå±è”½æŒ‡ä»¤éƒ¨åˆ†ï¼Œå¦‚Fig7.13æ‰€ç¤ºã€‚è¿™æ ·ï¼Œæ¨¡å‹çš„è®­ç»ƒé‡ç‚¹å°†æ”¾åœ¨ç”Ÿæˆå‡†ç¡®çš„å“åº”ä¸Šï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒæŒ‡ä»¤å†…å®¹ï¼Œä»è€Œæœ‰åŠ©äºé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚

![Fig7.13](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/13.png)

# 5.Creating data loaders for an instruction dataset

![Fig7.14](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/14.png)

æˆ‘ä»¬ä½¿ç”¨Pythonæ ‡å‡†åº“`functools`æä¾›çš„`partial`å‡½æ•°ï¼Œæ¥å›ºå®šä¸€ä¸ªå‡½æ•°çš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œè¿™ä¸ªæ–°å‡½æ•°å¯ä»¥åƒåŸå‡½æ•°ä¸€æ ·è°ƒç”¨ï¼Œä½†å…¶ä¸­æŸäº›å‚æ•°çš„å€¼å·²ç»é¢„å…ˆè®¾ç½®å¥½äº†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†`allowed_max_length`è®¾ç½®ä¸º`1024`ï¼Œä»¥ä¾¿å°†æ•°æ®æˆªæ–­åˆ°[GPT-2](https://shichaoxin.com/2024/03/20/LLM-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82ChatGPT%E8%83%8C%E5%90%8E%E7%9A%84%E6%8A%80%E6%9C%AF/#2gpt2)æ¨¡å‹æ‰€æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ä¹‹åè¦è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼š

```python
from functools import partial

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

customized_collate_fn = partial(
    custom_collate_fn,
    device=device,
    allowed_max_length=1024
)
```

```python
#Initializing the data loaders
from torch.utils.data import DataLoader


num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)

val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

print("Train loader:")
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)
```

è¾“å‡ºä¸ºï¼š

```
Train loader:
torch.Size([8, 61]) torch.Size([8, 61])
torch.Size([8, 76]) torch.Size([8, 76])
torch.Size([8, 73]) torch.Size([8, 73])
torch.Size([8, 68]) torch.Size([8, 68])
torch.Size([8, 65]) torch.Size([8, 65])
torch.Size([8, 72]) torch.Size([8, 72])
torch.Size([8, 80]) torch.Size([8, 80])
torch.Size([8, 67]) torch.Size([8, 67])
torch.Size([8, 62]) torch.Size([8, 62])
torch.Size([8, 75]) torch.Size([8, 75])
torch.Size([8, 62]) torch.Size([8, 62])
torch.Size([8, 68]) torch.Size([8, 68])
torch.Size([8, 67]) torch.Size([8, 67])
torch.Size([8, 77]) torch.Size([8, 77])
torch.Size([8, 69]) torch.Size([8, 69])
torch.Size([8, 79]) torch.Size([8, 79])
torch.Size([8, 71]) torch.Size([8, 71])
torch.Size([8, 66]) torch.Size([8, 66])
torch.Size([8, 83]) torch.Size([8, 83])
torch.Size([8, 68]) torch.Size([8, 68])
torch.Size([8, 80]) torch.Size([8, 80])
torch.Size([8, 71]) torch.Size([8, 71])
torch.Size([8, 69]) torch.Size([8, 69])
torch.Size([8, 65]) torch.Size([8, 65])
...
torch.Size([8, 83]) torch.Size([8, 83])
torch.Size([8, 66]) torch.Size([8, 66])
torch.Size([8, 74]) torch.Size([8, 74])
torch.Size([8, 69]) torch.Size([8, 69])
```

å€ŸåŠ©äºæˆ‘ä»¬è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œæ•°æ®åŠ è½½å™¨èƒ½å¤Ÿç”Ÿæˆé•¿åº¦ä¸åŒçš„batchã€‚

```python
print(inputs[0])
print(targets[0])
```

è¾“å‡ºä¸ºï¼š

```
tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,
          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,
        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,
          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,
         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,
          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],
       device='cuda:0')
tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,
         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,
        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,
          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,
          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,
          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],
       device='cuda:0')
```

# 6.Loading a pretrained LLM

åœ¨å¼€å§‹instruction fine-tuningä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„GPTæ¨¡å‹ä½œä¸ºfine-tuneçš„åŸºç¡€ï¼Œå¦‚Fig7.15æ‰€ç¤ºã€‚è¿™æ¬¡æˆ‘ä»¬ä¼šåŠ è½½ä¸€ä¸ªä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º3.55äº¿ã€‚

![Fig7.15](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/15.png)

```python
#Loading the pretrained model
from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt


BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

CHOOSE_MODEL = "gpt2-medium (355M)"

BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(
    model_size=model_size,
    models_dir="gpt2"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval();
```

`GPTModel`çš„å®šä¹‰è§ï¼š[Coding the GPT model](https://shichaoxin.com/2025/03/03/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-4-Implementing-a-GPT-model-from-scratch-to-generate-text/#7coding-the-gpt-model)ï¼Œ`load_weights_into_gpt`çš„å®šä¹‰è§ï¼š[Loading pretrained weights from OpenAI](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#6loading-pretrained-weights-from-openai)ã€‚

ä¸‹è½½è¿‡ç¨‹ï¼š

```
checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 38.3kiB/s]
encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:01<00:00, 728kiB/s] 
hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91.0/91.0 [00:00<00:00, 45.7kiB/s]
model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [02:31<00:00, 9.36MiB/s]  
model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.4k/10.4k [00:00<00:00, 5.20MiB/s]
model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927k/927k [00:01<00:00, 642kiB/s]  
vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 475kiB/s]  
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´è¯„ä¼°ä¸€ä¸‹é¢„è®­ç»ƒLLMåœ¨æŸä¸ªéªŒè¯ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé€šè¿‡å°†å…¶è¾“å‡ºä¸é¢„æœŸå“åº”è¿›è¡Œå¯¹æ¯”ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªåŸºçº¿ï¼Œäº†è§£æ¨¡å‹åœ¨æœªç»fine-tuneçš„æƒ…å†µä¸‹ï¼Œå¯¹æŒ‡ä»¤ä»»åŠ¡çš„åˆå§‹æ‰§è¡Œèƒ½åŠ›ï¼Œä¹Ÿèƒ½è®©æˆ‘ä»¬åœ¨åç»­æ›´å¥½åœ°ç†è§£fine-tuneæ‰€å¸¦æ¥çš„æ”¹è¿›æ•ˆæœã€‚æˆ‘ä»¬å°†ä½¿ç”¨éªŒè¯é›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ¥è¿›è¡Œè¿™é¡¹è¯„ä¼°ï¼š

```python
torch.manual_seed(123)

input_text = format_input(val_data[0])
print(input_text)
```

è¾“å‡ºä¸ºï¼š

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨`generate`å‡½æ•°ï¼ˆå®šä¹‰è§ï¼š[Modifying the text generation function](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#43modifying-the-text-generation-function)ï¼‰ç”Ÿæˆå“åº”ï¼š

```python
from previous_chapters import (
    generate,
    text_to_token_ids,
    token_ids_to_text
)

token_ids = generate(
    model=model,
    idx=text_to_token_ids(input_text, tokenizer),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)
generated_text = token_ids_to_text(token_ids, tokenizer)
print(generated_text)
```

è¾“å‡ºä¸ºï¼š

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'

### Response:

The chef cooks the meal every day.

### Instruction:

Convert the active sentence to passive: 'The chef cooks the
```

[`generate`å‡½æ•°](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#43modifying-the-text-generation-function)è¿”å›çš„æ˜¯è¾“å…¥æ–‡æœ¬å’Œè¾“å‡ºæ–‡æœ¬çš„ç»„åˆã€‚è¿™ç§è¡Œä¸ºåœ¨ä¹‹å‰æ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„LLMä¸»è¦è¢«è®¾è®¡ä¸ºæ–‡æœ¬è¡¥å…¨æ¨¡å‹ï¼Œå®ƒä¼šå°†è¾“å…¥å’Œè¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œç”Ÿæˆè¿è´¯ã€æ˜“è¯»çš„æ–‡æœ¬ã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬æƒ³è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„è¡¨ç°æ—¶ï¼Œé€šå¸¸æ›´å¸Œæœ›å•ç‹¬å…³æ³¨æ¨¡å‹ç”Ÿæˆçš„å“åº”éƒ¨åˆ†ã€‚

ä¸ºäº†å•ç‹¬æå–æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦ä»`generated_text`çš„å¼€å¤´å»é™¤è¾“å…¥æŒ‡ä»¤éƒ¨åˆ†ï¼š

```python
response_text = generated_text[len(input_text):].strip()
print(response_text)
```

è¾“å‡ºä¸ºï¼š

```
### Response:

The chef cooks the meal every day.

### Instruction:

Convert the active sentence to passive: 'The chef cooks the
```

è¿™ä¸ªè¾“å‡ºè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹å°šæœªå…·å¤‡æ­£ç¡®æ‰§è¡Œæ‰€ç»™æŒ‡ä»¤çš„èƒ½åŠ›ã€‚

# 7.Fine-tuning the LLM on instruction data

![Fig7.16](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/16.png)

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè®¡ç®—ä¸‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„åˆå§‹lossï¼š

```python
from previous_chapters import (
    calc_loss_loader,
    train_model_simple
)

model.to(device)

torch.manual_seed(123)

with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)

print("Training loss:", train_loss) #Training loss: 3.825910234451294
print("Validation loss:", val_loss) #Validation loss: 3.7619343757629395
```

`calc_loss_loader`çš„å®šä¹‰è§ï¼š[Calculating the training and validation set losses](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#23calculating-the-training-and-validation-set-losses)ã€‚

ä¸‹è¡¨æ˜¯ä¸åŒçš„[GPT-2](https://shichaoxin.com/2024/03/20/LLM-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82ChatGPT%E8%83%8C%E5%90%8E%E7%9A%84%E6%8A%80%E6%9C%AF/#2gpt2)æ¨¡å‹åœ¨ä¸åŒdeviceä¸Šçš„è€—æ—¶ï¼š

![table](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/17.png)

```python
#Instruction fine-tuning the pretrained LLM
import time

start_time = time.time()

torch.manual_seed(123)

optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)

num_epochs = 2

train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data[0]), tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

`train_model_simple`çš„å®šä¹‰è§ï¼š[Training an LLM](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#3training-an-llm)ã€‚è®­ç»ƒè¿‡ç¨‹ï¼š

```
Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626
Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102
Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944
Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906
Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881
Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859
Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836
Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808
Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806
Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789
Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783
Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763
Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743
Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735
Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729
Ep 1 (Step 000075): Train loss 0.569, Val loss 0.728
Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725
Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709
Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691
Ep 1 (Step 000095): Train loss 0.500, Val loss 0.682
Ep 1 (Step 000100): Train loss 0.503, Val loss 0.677
Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670
Ep 1 (Step 000110): Train loss 0.555, Val loss 0.666
Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664
Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:
...
Ep 2 (Step 000225): Train loss 0.347, Val loss 0.661
Ep 2 (Step 000230): Train loss 0.294, Val loss 0.656
Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom
Training completed in 88.43 minutes.
```

>`gpt2-medium (355M)`æ¨¡å‹åœ¨æˆ‘è‡ªå·±ç¬”è®°æœ¬ï¼ˆGPUä¸ºNVIDIA T1200ï¼‰ä¸Šï¼Œåªè®­ç»ƒä¸¤ä¸ªepochå°±ç”¨äº†88åˆ†é’ŸğŸ˜“ã€‚

è®­ç»ƒè¾“å‡ºè¡¨æ˜æ¨¡å‹æ­£åœ¨æœ‰æ•ˆåœ°å­¦ä¹ ï¼Œå› ä¸ºåœ¨ä¸¤ä¸ªepochä¸­ï¼Œè®­ç»ƒlosså’ŒéªŒè¯losséƒ½åœ¨æŒç»­ä¸‹é™ã€‚è¿™ä¸€ç»“æœè¯´æ˜æ¨¡å‹åœ¨ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›æ­£åœ¨é€æ­¥æå‡ã€‚ç”±äºæ¨¡å‹åœ¨ä¸¤ä¸ªepochå†…å°±å·²å±•ç°å‡ºè‰¯å¥½çš„å­¦ä¹ æ•ˆæœï¼Œå› æ­¤ç»§ç»­è®­ç»ƒåˆ°ç¬¬ä¸‰ä¸ªepochæˆ–æ›´é•¿æ—¶é—´å¹¶ä¸æ˜¯å¿…è¦çš„ï¼Œç”šè‡³å¯èƒ½é€‚å¾—å…¶åï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆçš„é£é™©å¢åŠ ã€‚

ç»å†2ä¸ªepochçš„è®­ç»ƒä¹‹åï¼Œæ¨¡å‹æˆåŠŸçš„ç”Ÿæˆäº†æ­£ç¡®çš„å“åº”ï¼Œå°†å¥å­`"The chef cooks the meal every day."`è½¬æ¢ä¸ºäº†è¢«åŠ¨è¯­æ€ï¼š`"The meal is cooked every day by the chef."`ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸‹è®­ç»ƒlosså’ŒéªŒè¯lossçš„å˜åŒ–æ›²çº¿ã€‚

```python
from previous_chapters import plot_losses

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```

`plot_losses`çš„å®šä¹‰è§ï¼š[Training an LLM](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#3training-an-llm)ã€‚

![Fig7.17](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/18.png)

# 8.Extracting and saving responses

æˆ‘ä»¬ç°åœ¨å¯ä»¥è¯„ä¼°å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°äº†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸ºæµ‹è¯•é›†ä¸­æ¯æ¡è¾“å…¥æå–æ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼Œå¹¶å°†å…¶æ”¶é›†èµ·æ¥ä»¥ä¾¿è¿›è¡Œäººå·¥åˆ†æï¼›éšåï¼Œæˆ‘ä»¬å°†å¯¹LLMçš„è¡¨ç°è¿›è¡Œè¯„ä¼°ï¼Œä»¥é‡åŒ–å…¶ç”Ÿæˆå“åº”çš„è´¨é‡ï¼Œå¦‚Fig7.18æ‰€ç¤ºã€‚

![Fig7.18](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/19.png)

ä¸ºäº†å®ŒæˆæŒ‡ä»¤å“åº”çš„æå–ï¼Œæˆ‘ä»¬ä½¿ç”¨[`generate`å‡½æ•°](https://shichaoxin.com/2025/03/13/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-5-Pretraining-on-unlabeled-data/#43modifying-the-text-generation-function)ã€‚éšåï¼Œæˆ‘ä»¬å°†æ¨¡å‹ç”Ÿæˆçš„å“åº”ä¸æµ‹è¯•é›†ä¸­å‰ä¸‰ä¸ªæ ·æœ¬çš„æœŸæœ›ç­”æ¡ˆå¹¶æ’æ‰“å°å‡ºæ¥ï¼Œæ–¹ä¾¿è¿›è¡Œå¯¹æ¯”ï¼š

```python
torch.manual_seed(123)


for entry in test_data[:3]:

    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
)

    print(input_text)
    print(f"\nCorrect response:\n>> {entry['output']}")
    print(f"\nModel response:\n>> {response_text.strip()}")
    print("-------------------------------------")
```

è¾“å‡ºä¸ºï¼š

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Rewrite the sentence using a simile.

### Input:
The car is very fast.

Correct response:
>> The car is as fast as lightning.

Model response:
>> The car is as fast as a cheetah.
-------------------------------------
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
What type of cloud is typically associated with thunderstorms?

Correct response:
>> The type of cloud typically associated with thunderstorms is cumulonimbus.

Model response:
>> The type of cloud associated with thunderstorms is a cumulus cloud.
-------------------------------------
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Name the author of 'Pride and Prejudice'.

Correct response:
>> Jane Austen.

Model response:
>> The author of 'Pride and Prejudice' is Jane Austen.
-------------------------------------
```

ä»ä¸Šè¿°è¾“å‡ºæ¥çœ‹ï¼Œæ¨¡å‹çš„æ•ˆæœè¿˜ä¸é”™ã€‚ç¬¬ä¸€æ¡æŒ‡ä»¤å’Œç¬¬ä¸‰æ¡æŒ‡ä»¤ï¼Œæ¨¡å‹éƒ½ç»™å‡ºäº†æ­£ç¡®çš„ç­”æ¡ˆã€‚ç¬¬äºŒæ¡æŒ‡ä»¤ï¼Œæ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆæ˜¯ç§¯äº‘ï¼ˆcumulus cloudï¼‰ï¼Œè€Œæ­£ç¡®ç­”æ¡ˆæ˜¯ç§¯é›¨äº‘ï¼ˆcumulonimbusï¼‰ï¼Œæ¨¡å‹çš„é¢„æµ‹ç»“æœè™½ç„¶ä¸å®Œå…¨å‡†ç¡®ï¼Œä½†ä¹Ÿå¾ˆæ¥è¿‘ï¼Œå¹¶ä¸”å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç§¯äº‘ç¡®å®å¯ä»¥å‘å±•æˆç§¯é›¨äº‘ã€‚

ç»è¿‡instruction fine-tuningåçš„LLMå¯ä»¥é€šè¿‡ä»¥ä¸‹å¤šç§æ–¹å¼è¿›è¡Œè¯„ä¼°ï¼š

* é€šè¿‡å’Œæ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒæ¥è¯„ä¼°æ¨¡å‹ã€‚æ¯”å¦‚MMLUï¼ˆMeasuring Massive Multitask Language Understandingï¼Œ[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)ï¼‰ï¼Œå…¶æ¶‰åŠå¤šä¸ªå­¦ç§‘é¢†åŸŸï¼Œé¢˜ç›®å½¢å¼åŸºæœ¬éƒ½æ˜¯é€‰æ‹©é¢˜ï¼ˆä»…æœ‰å°‘é‡ç®€ç­”é¢˜ï¼‰ï¼Œä¸”å‡é…æœ‰æ ‡å‡†ç­”æ¡ˆã€‚
* é€šè¿‡äººç±»æ‰“åˆ†æ¯”è¾ƒå¤šä¸ªå¤§æ¨¡å‹ä¹‹é—´çš„å¯¹è¯è´¨é‡ã€‚æ¯”å¦‚LMSYS chatbot arenaï¼š[https://lmarena.ai/](https://lmarena.ai/)ã€‚
* ç”¨å¦ä¸€ä¸ªå¼ºå¤§çš„LLMï¼ˆå¦‚GPT-4ï¼‰æ¥è¯„åˆ¤å¤šä¸ªæ¨¡å‹çš„å›ç­”è´¨é‡ã€‚æ¯”å¦‚AlpacaEvalï¼š[https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/)ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥åŒæ—¶è€ƒè™‘è¿™ä¸‰ç§è¯„ä¼°æ–¹æ³•ã€‚

è€ƒè™‘åˆ°å½“å‰ä»»åŠ¡çš„è§„æ¨¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¦ä¸€ä¸ªæ›´å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹å¯¹å“åº”è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é«˜æ•ˆåœ°è¯„ä¼°ç”Ÿæˆå“åº”çš„è´¨é‡ï¼Œæ— éœ€å¤§é‡äººå·¥å‚ä¸ï¼Œä»è€ŒèŠ‚çœæ—¶é—´å’Œèµ„æºï¼ŒåŒæ—¶ä»èƒ½è·å¾—å…·æœ‰å‚è€ƒä»·å€¼çš„æ€§èƒ½æŒ‡æ ‡ã€‚

æˆ‘ä»¬å°†æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆå’Œä¹‹å‰çš„æµ‹è¯•é›†ä¸€èµ·æ”¾åœ¨`test_data`ä¸­ï¼š

```python
#åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¯¹æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½ç”Ÿæˆç­”æ¡ˆ
from tqdm import tqdm

for i, entry in tqdm(enumerate(test_data), total=len(test_data)):

    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = generated_text[len(input_text):].replace("### Response:", "").strip()

    test_data[i]["model_response"] = response_text

#ä¿å­˜åˆ°jsonæ–‡ä»¶
with open("instruction-data-with-response.json", "w") as file:
    json.dump(test_data, file, indent=4)  # "indent" for pretty-printing
```

è¾“å‡ºä¸ºï¼š

```
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [09:12<00:00,  5.02s/it] 
```

æŸ¥çœ‹å…¶ä¸­çš„ä¸€æ¡è®°å½•ï¼š

```python
print(test_data[0])
```

è¾“å‡ºä¸ºï¼š

```
{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}
```

æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹ä¿å­˜ä¸º`gpt2-medium355M-sft.pth`ï¼Œæ–¹ä¾¿ä»¥åå¤ç”¨ï¼š

```python
import re


file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth"
torch.save(model.state_dict(), file_name)
print(f"Model saved as {file_name}") #Model saved as gpt2-medium355M-sft.pth

# Load model via
# model.load_state_dict(torch.load("gpt2-medium355M-sft.pth"))
```

# 9.Evaluating the fine-tuned LLM

![Fig7.19](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/20.png)

æˆ‘ä»¬ä½¿ç”¨[Ollama](https://ollama.com/)åœ¨æœ¬åœ°è¿è¡Œå‚æ•°é‡ä¸º8Bçš„Llama 3æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±Meta AIå¼€å‘ã€‚

>Ollamaæ˜¯å¯¹å¼€æºåº“[llama.cpp](https://github.com/ggml-org/llama.cpp)çš„å°è£…ï¼Œè¯¥åº“ä½¿ç”¨çº¯C/C++å®ç°LLMï¼Œä»¥æœ€å¤§åŒ–è¿è¡Œæ•ˆç‡ã€‚ç„¶è€Œï¼ŒOllamaä»…ç”¨äºä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬ï¼ˆå³æ¨ç†ï¼‰ï¼Œä¸æ”¯æŒè®­ç»ƒæˆ–fine-tune LLMã€‚

Ollamaå®‰è£…å®Œæˆä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯åŠ¨Ollamaåº”ç”¨ç¨‹åºæˆ–è€…åœ¨ç»ˆç«¯è¾“å…¥`ollama serve`æ¥å¯ç”¨Ollamaï¼Œç„¶ååœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸­è¿è¡Œ`ollama run llama3`æ¥æ‰§è¡ŒLlama 3æ¨¡å‹çš„ä¸‹è½½ï¼Œå¦‚Fig7.20æ‰€ç¤ºã€‚

![Fig7.20](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/21.png)

æ¨¡å‹ä¸‹è½½è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š

![Fig](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/22.png)

æˆ‘ä»¬å¯ä»¥æµ‹è¯•ä¸€ä¸‹æ¨¡å‹æ˜¯å¦å¥½ç”¨ï¼š

![Fig](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/23.png)

æ¨¡å‹çš„åŠ è½½å’Œè¿è¡Œéƒ½æ²¡é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸€ç›´ä¿æŒ`ollama serve`å‘½ä»¤æˆ–Ollamaåº”ç”¨ç¨‹åºå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œä¸‹é¢çš„ä»£ç ç”¨äºéªŒè¯Ollama sessionæ˜¯å¦æ­£å¸¸è¿è¡Œï¼š

```python
import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")

if not ollama_running:
    raise RuntimeError("Ollama not running. Launch ollama before proceeding.")
print("Ollama running:", check_if_running("ollama")) #Ollama running: True
```

é™¤äº†åœ¨ç»ˆç«¯å’Œæ¨¡å‹è¿›è¡Œäº¤äº’ä¹‹å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨Pythoné€šè¿‡å…¶REST APIä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ä¸‹è¿°ä»£ç ä¸­çš„`query_model`å‡½æ•°æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¯¥APIã€‚

```python
import urllib.request

def query_model(
    prompt,
    model="llama3",
    url="http://localhost:11434/api/chat"
):
    # Create the data payload as a dictionary
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "options": {     # Settings below are required for deterministic responses
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }


    # Convert the dictionary to a JSON formatted string and encode it to bytes
    payload = json.dumps(data).encode("utf-8")

    # Create a request object, setting the method to POST and adding necessary headers
    request = urllib.request.Request(
        url,
        data=payload,
        method="POST"
    )
    request.add_header("Content-Type", "application/json")

    # Send the request and capture the response
    response_data = ""
    with urllib.request.urlopen(request) as response:
        # Read and decode the response
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]

    return response_data


model = "llama3"
result = query_model("What do Llamas eat?", model)
print(result)
```

è¾“å‡ºä¸ºï¼š

```
Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:

1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.
2. Hay: Hay is a staple in a llama's diet. They enjoy eating timothy hay, alfalfa hay, or other types of hay as a source of fiber and nutrients.
3. Grains: Llamas may be fed grains like oats, barley, or corn as a treat or to supplement their diet.
4. Fruits and vegetables: Fresh fruits and vegetables can be given to llamas as a treat or to add variety to their diet. Some examples include apples, carrots, sweet potatoes, and leafy greens like kale or spinach.
5. Minerals: Llamas need access to minerals like calcium, phosphorus, and salt to stay healthy. These can be provided through mineral blocks or loose minerals.

In the wild, llamas would typically eat whatever plants are available in their habitat, including:

* Leaves
* Twigs
* Bark
* Fruits
* Flowers

Domesticated llamas may have a more controlled diet, but they still require access to a variety of plant-based foods to stay healthy and thrive.
```

ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨Llama 3æ¨¡å‹å¯¹æµ‹è¯•é›†ç”Ÿæˆå‚è€ƒç­”æ¡ˆï¼Œå¹¶åŸºäºæ­¤å¯¹æˆ‘ä»¬ä¹‹å‰fine-tuneæ¨¡å‹çš„è¾“å‡ºç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†èŒƒå›´ä¸º0åˆ°100åˆ†ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹ä¸‹æµ‹è¯•é›†ä¸­å‰ä¸‰ä¸ªæ ·æœ¬çš„ç»“æœï¼š

```python
for entry in test_data[:3]:
    prompt = (
        f"Given the input `{format_input(entry)}` "
        f"and correct output `{entry['output']}`, "
        f"score the model response `{entry['model_response']}`"
        f" on a scale from 0 to 100, where 100 is the best score. "
    )
    print("\nDataset response:")
    print(">>", entry['output'])
    print("\nModel response:")
    print(">>", entry["model_response"])
    print("\nScore:")
    print(">>", query_model(prompt)) #ç»™é¢„æµ‹è¾“å‡ºå’ŒGTï¼Œç›´æ¥è®©Llama 3æ‰“åˆ†
    print("\n-------------------------")
```

è¾“å‡ºä¸ºï¼š

```

Dataset response:
>> The car is as fast as lightning.

Model response:
>> The car is as fast as a cheetah.

Score:
>> I'd rate the model response "The car is as fast as a cheetah." an 85 out of 100.

Here's why:

* The response uses a simile correctly, comparing the speed of the car to that of a cheetah.
* The comparison is relevant and makes sense, as cheetahs are known for their incredible speed.
* The phrase "as fast as" is used correctly to introduce the simile.

The only reason I wouldn't give it a perfect score is that lightning is often used as an example of extremely fast movement in English language, so using a more common or relatable comparison like a cheetah is still a good choice. However, if the goal was specifically to use lightning as the comparison, then the model response would be 0 out of 100!

-------------------------

Dataset response:
>> The type of cloud typically associated with thunderstorms is cumulonimbus.

Model response:
>> The type of cloud associated with thunderstorms is a cumulus cloud.

Score:
>> I'd score this model response as 40 out of 100.

Here's why:

* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).
* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.
* The response lacks precision and accuracy in its description.

Overall, while the model attempts to address the instruction, it provides an incorrect answer, which is a significant error.

-------------------------

Dataset response:
>> Jane Austen.

Model response:
>> The author of 'Pride and Prejudice' is Jane Austen.

Score:
>> I'd rate my own response as 95 out of 100. Here's why:

* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.
* The response is concise and clear, making it easy to understand.
* There are no grammatical errors or ambiguities that could lead to confusion.

The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it simply repeats the question back in a different form. However, this redundancy doesn't detract from the accuracy and clarity of the response, so I'm still confident in my score!

-------------------------

```

æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æç¤ºè¯ï¼Œè®©å…¶åªè¿”å›ä¸€ä¸ªæ•´å‹çš„åˆ†æ•°å³å¯ï¼š

```python
#Evaluating the instruction fine-tuning LLM
def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
            f"Given the input `{format_input(entry)}` "
            f"and correct output `{entry['output']}`, "
            f"score the model response `{entry[json_key]}`"
            f" on a scale from 0 to 100, where 100 is the best score. "
            f"Respond with the integer number only."
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"Could not convert score: {score}")
            continue

    return scores


scores = generate_model_scores(test_data, "model_response")
print(f"Number of scores: {len(scores)} of {len(test_data)}")
print(f"Average score: {sum(scores)/len(scores):.2f}\n")
```

è¾“å‡ºä¸ºï¼š

```
Scoring entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [05:54<00:00,  3.22s/it]
Number of scores: 110 of 110
Average score: 46.83
```

ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹å‡ ç§ç­–ç•¥ï¼š

* è°ƒæ•´fine-tuneè¿‡ç¨‹ä¸­çš„è¶…å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€batch sizeæˆ–epochæ¬¡æ•°ã€‚
* å¢åŠ è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ï¼Œæˆ–ä½¿æ ·æœ¬æ›´åŠ å¤šæ ·åŒ–ï¼Œä»¥æ¶µç›–æ›´å¹¿æ³›çš„è¯é¢˜å’Œè¡¨è¾¾é£æ ¼ã€‚
* å°è¯•ä¸åŒçš„æç¤ºè¯æˆ–æŒ‡ä»¤æ ¼å¼ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹ç”Ÿæˆå“åº”ã€‚
* ä½¿ç”¨æ›´å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶é€šå¸¸å…·å¤‡æ›´å¼ºçš„èƒ½åŠ›æ¥æ•æ‰å¤æ‚æ¨¡å¼å¹¶ç”Ÿæˆæ›´å‡†ç¡®çš„å“åº”ã€‚

# 10.Conclusions

![Fig7.21](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/LLMsFromScratch/7/24.png)
