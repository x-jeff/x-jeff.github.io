---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘YOLOv7ï¼šTrainable bag-of-freebies sets new state-of-the-art for real-time object detectors
subtitle:   YOLOv7
date:       2025-10-28
author:     x-jeff
header-img: blogimg/20220530.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

>githubæºç åœ°å€ï¼š[https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7)ã€‚

æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œè¿˜å°†é‡ç‚¹å…³æ³¨è®­ç»ƒè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬å°†å¼•å…¥ä¸€äº›ä¼˜åŒ–æ¨¡å—å’Œè®­ç»ƒæ–¹æ³•ï¼Œè™½ç„¶è¿™äº›æ–¹æ³•å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæˆæœ¬ï¼Œä½†ä¸ä¼šæå‡æ¨ç†æˆæœ¬ï¼Œä»è€Œåœ¨ä¸å½±å“æ¨ç†é€Ÿåº¦çš„å‰æä¸‹æé«˜æ£€æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬å°†è¿™äº›ä¼˜åŒ–æ¨¡å—å’Œè®­ç»ƒæ–¹æ³•ç§°ä¸º**trainable bag-of-freebies**ã€‚

![Fig1](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/1.png)

# 2.Related work

ä¸å†èµ˜è¿°ã€‚

# 3.Architecture

## 3.1.Extended efficient layer aggregation networks

åŸºäº[ELAN](https://shichaoxin.com/2025/07/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Designing-Network-Design-Strategies-Through-Gradient-Path-Analysis/)æå‡ºäº†Extended-ELANï¼ˆE-ELANï¼‰ï¼Œå¦‚Fig2(d)æ‰€ç¤ºã€‚

![Fig2](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/2.png)

åœ¨Fig2(d)ä¸­ï¼Œå…ˆè§£é‡Šå‡ ä¸ªæ¦‚å¿µï¼Œé¦–å…ˆæ˜¯cardinalityï¼Œè¿™ä¸ªæ¦‚å¿µæ¥è‡ª[ResNeXt](https://shichaoxin.com/2023/12/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/)ï¼Œè¡¨ç¤ºåˆ†æ”¯è·¯å¾„çš„æ•°é‡ã€‚Fig2(d)ä¸­çš„Expand cardinalityæ˜¯[åˆ†ç»„å·ç§¯](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)çš„æ„æ€ï¼Œ"3x3, 2c, 2c, 2"è¡¨ç¤ºä½¿ç”¨$3 \times 3$å·ç§¯ï¼Œè¾“å…¥é€šé“æ•°ä¸º2cï¼Œè¾“å‡ºé€šé“æ•°ä¹Ÿä¸º2cï¼Œgroupæ•°é‡ä¸º2ã€‚Shuffle cardinalityå°±æ˜¯[channel shuffle](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)æ“ä½œã€‚Fig2(d)è¯¦ç»†å±•å¼€å¦‚ä¸‹æ‰€ç¤ºï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/3.png)

ä½†åœ¨å®é™…ä»£ç å®ç°æ—¶ï¼Œä½œè€…ä½¿ç”¨äº†Fig2(d)çš„å¦ä¸€ç§ç­‰ä»·ç»“æ„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/4.png)

ä¸Šä¸‹ä¸¤å¼ å›¾æ˜¯ä¸€ä¸ªæ„æ€ï¼Œå…¶å®å°±æ˜¯å¹¶è¡Œäº†ä¸¤ä¸ª[ELAN](https://shichaoxin.com/2025/07/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Designing-Network-Design-Strategies-Through-Gradient-Path-Analysis/)ã€‚

## 3.2.Model scaling for concatenation-based models

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/5.png)

å¯¹äºconcatenation-basedçš„æ¶æ„ï¼Œå¦‚Fig3(a)æ‰€ç¤ºï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡å¢åŠ ç½‘ç»œæ·±åº¦çš„æ–¹å¼æ¥æ‰©å±•ç½‘ç»œæ¨¡å‹ï¼Œå³Fig3(b)æ‰€ç¤ºçš„å½¢å¼ï¼Œè¿™æ ·ä¼šå¯¼è‡´concatä¹‹åçš„è¾“å‡ºé€šé“æ•°å˜å¤šï¼Œå³ä¸ä»…ä»…å¢åŠ äº†æ·±åº¦ï¼Œç½‘ç»œå®½åº¦ä¹Ÿè¢«è¿«å¢åŠ ï¼Œä»è€Œå¯¼è‡´åç»­å±‚çš„è¾“å…¥é€šé“æ•°å¢åŠ ï¼Œè¿™ä¼šå¢åŠ é¢å¤–è®¡ç®—å’Œå‚æ•°ï¼Œç ´åäº†åŸæœ‰çš„æ¯”ä¾‹å…³ç³»ã€‚å› æ­¤æˆ‘ä»¬æå‡ºå¦‚Fig3(c)æ‰€ç¤ºçš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å°±æ˜¯é€šè¿‡transition layeræ¥æ§åˆ¶è¾“å‡ºé€šé“æ•°ã€‚

# 4.Trainable bag-of-freebies

## 4.1.Planned re-parameterized convolution

å½“æˆ‘ä»¬å°†[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ç›´æ¥åº”ç”¨äº[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)æˆ–[DenseNet](https://shichaoxin.com/2023/11/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Densely-Connected-Convolutional-Networks/)æˆ–å…¶ä»–æ¡†æ¶æ—¶ï¼Œæ¨¡å‹ç²¾åº¦ä¼šå‡ºç°æ˜¾è‘—ä¸‹é™ã€‚å› æ­¤æˆ‘ä»¬é‡æ–°è®¾è®¡äº†[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ã€‚

[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)åœ¨ä¸€ä¸ªå·ç§¯å±‚ä¸­é€šå¸¸åŒ…å«ä¸€ä¸ª$3\times 3$å·ç§¯ã€ä¸€ä¸ª$1\times 1$å·ç§¯å’Œä¸€ä¸ªidentityè¿æ¥ã€‚æˆ‘ä»¬å‘ç°identityè¿æ¥ç ´åäº†[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)çš„æ®‹å·®ç»“æ„å’Œ[DenseNet](https://shichaoxin.com/2023/11/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Densely-Connected-Convolutional-Networks/)çš„concatæ“ä½œï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å»æ‰äº†[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ä¸­çš„identityè¿æ¥ï¼Œè®°ä¸ºRepConvNã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/6.png)

## 4.2.Coarse for auxiliary and fine for lead loss

æ·±åº¦ç›‘ç£ï¼ˆdeep supervisionï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºæ·±åº¦ç½‘ç»œè®­ç»ƒçš„æŠ€æœ¯ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åœ¨ç½‘ç»œçš„ä¸­é—´å±‚æ·»åŠ é¢å¤–çš„auxiliary headï¼Œå¹¶åˆ©ç”¨assistant lossæ¥å¼•å¯¼æµ…å±‚ç½‘ç»œçš„æƒé‡æ›´æ–°ã€‚å³ä½¿å¯¹äºè¯¸å¦‚[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)å’Œ[DenseNet](https://shichaoxin.com/2023/11/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Densely-Connected-Convolutional-Networks/)è¿™ç±»é€šå¸¸æ”¶æ•›è‰¯å¥½çš„æ¶æ„ï¼Œæ·±åº¦ç›‘ç£ä»ç„¶èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚Fig5(a)å’ŒFig5(b)åˆ†åˆ«å±•ç¤ºäº†æ²¡æœ‰æ·±åº¦ç›‘ç£å’Œé‡‡ç”¨æ·±åº¦ç›‘ç£çš„ç›®æ ‡æ£€æµ‹æ¶æ„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è´Ÿè´£æœ€ç»ˆè¾“å‡ºçš„headç§°ä¸ºlead headï¼Œè€Œç”¨äºè¾…åŠ©è®­ç»ƒçš„headç§°ä¸ºauxiliary headã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/7.png)

æ¥ä¸‹æ¥æˆ‘ä»¬è¦è®¨è®ºlabel assignmentçš„é—®é¢˜ã€‚åœ¨è¿‡å»ï¼Œåœ¨æ·±åº¦ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œlabel assignmenté€šå¸¸æ˜¯ç›´æ¥ä¾æ®GTç”Ÿæˆhard labelï¼Œå¹¶æŒ‰ç…§é¢„è®¾è§„åˆ™è¿›è¡Œåˆ†é…ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥ï¼Œä»¥ç›®æ ‡æ£€æµ‹ä¸ºä¾‹ï¼Œç ”ç©¶è€…ä»¬å¾€å¾€ä¼šåˆ©ç”¨ç½‘ç»œé¢„æµ‹è¾“å‡ºçš„è´¨é‡å’Œåˆ†å¸ƒä¿¡æ¯ï¼Œå†ç»“åˆGTï¼Œé€šè¿‡ä¸€å®šçš„è®¡ç®—å’Œä¼˜åŒ–æ–¹æ³•æ¥ç”Ÿæˆsoft labelã€‚ä¾‹å¦‚ï¼Œ[YOLO](https://shichaoxin.com/2022/05/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Look-Once-Unified,-Real-Time-Object-Detection/)ä½¿ç”¨é¢„æµ‹æ¡†å’ŒçœŸå®æ¡†ä¹‹é—´çš„IoUä½œä¸ºsoft labelã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è¿™ç§åŒæ—¶è€ƒè™‘ç½‘ç»œé¢„æµ‹ç»“æœå’ŒGTï¼Œå¹¶æ®æ­¤åˆ†é…soft labelçš„æœºåˆ¶ç§°ä¸ºlabel assignerã€‚

é‚£ä¹ˆæˆ‘ä»¬è¯¥å¦‚ä½•ä¸ºlead headå’Œauxiliary headåˆ†é…soft labelå‘¢ï¼Ÿæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰å°šæœªæœ‰ç›¸å…³æ–‡çŒ®æ¢è®¨è¿‡è¿™ä¸€é—®é¢˜ã€‚ç›®å‰æœ€å¸¸ç”¨çš„æ–¹æ³•å¦‚Fig5(c)æ‰€ç¤ºï¼Œå³å°†lead headå’Œauxiliary headåˆ†å¼€ï¼Œå„è‡ªåˆ©ç”¨è‡ªèº«çš„é¢„æµ‹ç»“æœå’ŒGTæ¥è¿›è¡Œlabel assignmentã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„label assignmentæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨lead headçš„é¢„æµ‹ç»“æœæ¥åŒæ—¶å¼•å¯¼lead headå’Œauxiliary headçš„è®­ç»ƒï¼Œå…¶åŒ…å«ä¸¤ç§ä¸åŒçš„ç­–ç•¥ï¼Œè§Fig5(d)å’ŒFig5(e)ã€‚

ğŸ‘‰**Lead head guided label assigner**

ä¸»è¦åŸºäºlead headçš„é¢„æµ‹ç»“æœä¸GTè¿›è¡Œè®¡ç®—ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ç”Ÿæˆsoft labelã€‚è¿™ç»„soft labelå°†åŒæ—¶ç”¨äºè®­ç»ƒlead headå’Œauxiliary headã€‚è¿™æ ·åšçš„åŸå› æ˜¯lead headå…·æœ‰è¾ƒå¼ºçš„å­¦ä¹ èƒ½åŠ›ï¼Œå› æ­¤ç”±å…¶ç”Ÿæˆçš„soft labelèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æºæ•°æ®ä¸ç›®æ ‡ä¹‹é—´çš„åˆ†å¸ƒå’Œç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ç§å­¦ä¹ æ–¹å¼è§†ä¸ºä¸€ç§å¹¿ä¹‰çš„æ®‹å·®å­¦ä¹ ï¼Œé€šè¿‡è®©è¾ƒæµ…å±‚çš„auxiliary headç›´æ¥å­¦ä¹ lead headå·²ç»æŒæ¡çš„ä¿¡æ¯ï¼Œlead headå°±èƒ½æ›´ä¸“æ³¨äºå­¦ä¹ é‚£äº›å°šæœªè¢«å­¦ä¹ åˆ°çš„æ®‹å·®ä¿¡æ¯ã€‚

ğŸ‘‰**Coarse-to-fine lead head guided label assigner**

åŒæ ·åŸºäºlead headçš„é¢„æµ‹ç»“æœä¸GTç”Ÿæˆsoft labelã€‚ä½†æ˜¯ï¼Œåœ¨è¯¥è¿‡ç¨‹ä¸­æˆ‘ä»¬ç”Ÿæˆäº†ä¸¤ç»„ä¸åŒçš„soft labelï¼Œå³coarse labelå’Œfine labelã€‚å…¶ä¸­ï¼Œfine labelå°±æ˜¯lead headçš„é¢„æµ‹ç»“æœå’ŒGTç›´æ¥ç”Ÿæˆçš„soft labelï¼Œè€Œcoarse labelåˆ™æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ”¾å®½äº†å¯¹æ­£æ ·æœ¬æ ‡ç­¾çš„åˆ†é…çº¦æŸï¼Œå³å…è®¸æ›´å¤šçš„gridè¢«è§†ä¸ºæ­£æ ·æœ¬ã€‚è¿™æ ·åšçš„åŸå› åœ¨äºï¼Œauxiliary headçš„å­¦ä¹ èƒ½åŠ›ä¸å¦‚lead headå¼ºï¼Œä¸ºäº†é¿å…é—æ¼éœ€è¦å­¦ä¹ çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­æ›´å…³æ³¨auxiliary headçš„recallä¼˜åŒ–ï¼Œè€Œå¯¹äºlead headï¼Œæˆ‘ä»¬åˆ™æ›´å…³æ³¨precisionçš„ä¼˜åŒ–ã€‚

## 4.3.Other trainable bag-of-freebies

åœ¨è®­ç»ƒä¸­ç”¨åˆ°çš„ä¸€äº›[BoF](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#22bag-of-freebies)ï¼š

1. å‚è€ƒ[RepVGG](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ï¼Œåœ¨æ¨ç†é˜¶æ®µå°†BNå±‚å’Œå·ç§¯å±‚è¿›è¡Œèåˆã€‚
2. [YOLORä¸­çš„éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ã€‚
3. ä»…åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)æ¨¡å‹ã€‚æ³¨æ„ï¼Œå’Œ[Momentumæ¢¯åº¦ä¸‹é™æ³•](https://shichaoxin.com/2020/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%83%E8%AF%BE-Momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)ä¸åŒï¼Œ[Momentumæ¢¯åº¦ä¸‹é™æ³•](https://shichaoxin.com/2020/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E4%B8%83%E8%AF%BE-Momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/)æ˜¯å¯¹æ¢¯åº¦æ›´æ–°ä½¿ç”¨[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)ï¼Œè€Œè¿™é‡Œè¯´çš„[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)æ¨¡å‹æ˜¯æŒ‡æ›´æ–°æ¨¡å‹å‚æ•°çš„ç­–ç•¥ã€‚è¯¦ç»†æ¥è¯´ï¼Œå°±æ˜¯åœ¨YOLOv7è®­ç»ƒä¸€å¼€å§‹çš„æ—¶å€™ï¼Œä¼šå¤åˆ¶ä¸€ä»½å½“å‰æ¨¡å‹ä½œä¸º[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)æ¨¡å‹ï¼Œä¹‹åæ¯è®­ç»ƒä¸€ä¸ªbatchï¼Œå°±ä¼šæŒ‰ç…§[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)çš„ç­–ç•¥å¯¹[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)æ¨¡å‹çš„å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œæ³¨æ„ï¼Œå¹¶ä¸ä¼šå½±å“åŸæœ‰æ¨¡å‹çš„è®­ç»ƒè¿›ç¨‹ã€‚è¿™æ ·å¾—åˆ°çš„[EMA](https://shichaoxin.com/2020/02/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/)æ¨¡å‹æ›´ä¸ºç¨³å®šï¼Œé²æ£’æ€§æ›´å¥½ã€‚

# 5.Experiments

## 5.1.Experimental setup

ä½¿ç”¨COCOæ•°æ®é›†è¿›è¡Œå®éªŒã€‚æ‰€æœ‰å®éªŒéƒ½æ²¡æœ‰ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„ã€‚æˆ‘ä»¬ä½¿ç”¨train 2017 setç”¨äºè®­ç»ƒï¼Œval 2017 setç”¨äºéªŒè¯å’Œè¶…å‚æ•°é€‰æ‹©ã€‚åœ¨test 2017 setä¸Šè¿›è¡Œè¯„ä¼°ã€‚è¯¦ç»†çš„è®­ç»ƒå‚æ•°è®¾ç½®è§Appendixã€‚

æˆ‘ä»¬è®¾è®¡äº†3ç§åŸºç¡€æ¨¡å‹ï¼š

1. YOLOv7-tinyç”¨äºedge GPUã€‚
2. YOLOv7ç”¨äºnormal GPUã€‚
3. YOLOv7-W6ç”¨äºcloud GPUã€‚

é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹ç¼©æ”¾ï¼Œå¾—åˆ°äº†è®¸å¤šå˜ä½“ï¼Œè¯¦è§Appendixã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒYOLOv7-tinyä½¿ç”¨çš„æ˜¯[leaky ReLU](https://shichaoxin.com/2019/12/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%AC%AC%E4%B8%83%E8%AF%BE-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/#23leaky-relu%E5%87%BD%E6%95%B0)ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚è€Œå…¶ä»–æ¨¡å‹ä½¿ç”¨[SiLU](https://shichaoxin.com/2024/01/14/YOLO%E7%B3%BB%E5%88%97-YOLOv5/#2model-structure)ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚

## 5.2.Baselines

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/23.png)

## 5.3.Comparison with state-of-the-arts

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/24.png)

## 5.4.Ablation study

### 5.4.1.Proposed compound scaling method

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/25.png)

è¡¨3å±•ç¤ºäº†æ¨¡å‹ç¼©æ”¾å¯¹æ€§èƒ½çš„å½±å“ã€‚ç¬¬ä¸€è¡Œæ˜¯baseæ¨¡å‹ï¼Œç¬¬äºŒè¡Œwidth onlyæ˜¯ä»…å°†widthæ‰©å¤§1.25å€ï¼Œç¬¬ä¸‰è¡Œdepth onlyæ˜¯ä»…å°†depthæ‰©å¤§2.0å€ï¼Œç¬¬å››è¡Œcompoundæ˜¯å°†widthæ‰©å¤§1.25å€çš„åŒæ—¶å°†depthæ‰©å¤§1.5å€ã€‚

### 5.4.2.Proposed planned re-parameterized model

ä¸ºäº†éªŒè¯æˆ‘ä»¬æ‰€æå‡ºçš„é‡å‚æ•°åŒ–æ–¹æ³•çš„æ™®é€‚æ€§ã€‚æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨concatenation-basedæ¨¡å‹å’Œresidual-basedæ¨¡å‹ç”¨äºéªŒè¯ã€‚å¯¹äºconcatenation-basedæ¨¡å‹çš„éªŒè¯ï¼Œæˆ‘ä»¬ä½¿ç”¨3ä¸ªå †å çš„ELANï¼›å¯¹äºresidual-basedæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨[CSPDarknet](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#34yolov4)ã€‚

åœ¨éªŒè¯concatenation-basedæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªELANæ¨¡å—ä¸­çš„$3 \times 3$å·ç§¯å±‚æ›¿æ¢ä¸º[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ã€‚å•ä¸ªELANæ¨¡å—çš„ç»“æ„å¦‚Fig6å·¦å›¾æ‰€ç¤ºï¼Œå’ŒåŸå§‹çš„ELANç»“æ„æœ‰æ‰€ä¸åŒï¼Œå…¶ä¸­ä¸€æ¡è·¯å¾„ä¸Šæœ‰è¿ç»­3ä¸ª$3\times 3$å·ç§¯å±‚ï¼ŒFig6å³å›¾è¡¨ç¤ºå°†å¯¹åº”ä½ç½®çš„$3\times 3$å·ç§¯æ›¿æ¢ä¸º[RepConv](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ã€‚æµ‹è¯•ç»“æœè§è¡¨4ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/26.png)

åœ¨éªŒè¯residual-basedæ¨¡å‹æ—¶ï¼Œå¦‚Fig7æ‰€ç¤ºï¼ŒFig7(a)æ˜¯åŸå§‹çš„[Darknet block](https://shichaoxin.com/2022/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv3-An-Incremental-Improvement/#24feature-extractor)ï¼ŒFig7(b)æ˜¯åŸå§‹çš„[CSPDarknet block](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#34yolov4)ã€‚ä¸ºäº†æ–¹ä¾¿åº”ç”¨é‡å‚æ•°åŒ–ï¼Œå°†[Darknet block](https://shichaoxin.com/2022/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv3-An-Incremental-Improvement/#24feature-extractor)å’Œ[CSPDarknet block](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#34yolov4)ä¸­çš„$3 \times 3$å·ç§¯æŒªåˆ°äº†$1 \times 1$å·ç§¯çš„å‰é¢ï¼Œå³Fig7(c)å’ŒFig7(d)ã€‚æµ‹è¯•ç»“æœè§è¡¨5ï¼Œå…¶ä¸­RepCSPå¯å‚é˜…[CSPRepResNet](https://shichaoxin.com/2024/09/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PP-YOLOE-An-evolved-version-of-YOLO/#22improvement-of-pp-yoloe)ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/27.png)

### 5.4.3.Proposed assistant loss for auxiliary head

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/28.png)

è¡¨6ä¸­çš„ç¬¬äºŒè¡Œâ€œindependentâ€è¡¨ç¤ºçš„æ˜¯lead headå’Œauxiliary headå„è‡ªé‡‡ç”¨ç‹¬ç«‹çš„label assignmentã€‚

åœ¨Fig8ä¸­ï¼ŒFig8(a)æ˜¯è¾“å…¥å›¾åƒï¼ŒFig8(b)å’ŒFig8(c)æŒ‡çš„æ˜¯aux headå’Œlead headçš„label assignmentæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼ŒFig8(d)å’ŒFig8(e)æŒ‡çš„æ˜¯ç¬¬4.2éƒ¨åˆ†æåˆ°çš„"Lead head guided label assigner"æ–¹å¼ã€‚ä»¥Fig8(b)ä¸ºä¾‹ï¼Œè§£é‡Šä¸‹å¦‚ä½•çœ‹è¿™ä¸ªå›¾ã€‚çºµå‘"Pyramids"æŒ‡çš„æ˜¯é‡‘å­—å¡”feature mapå±‚çº§ï¼Œæ¨ªå‘"Anchors"æŒ‡çš„æ˜¯ä¸åŒçš„3ç§anchorï¼Œæ‰€ä»¥è¯´ï¼Œ$4 \times 3$ä¸­çš„æ¯ä¸ªæ ¼å­éƒ½æ˜¯ä¸€ä¸ªobjectness mapï¼Œobjectness mapä¸­æ¯ä¸ªgridçš„å€¼è¶Šå¤§ï¼ˆé¢œè‰²è¶Šäº®ï¼‰ï¼Œå°±è¡¨ç¤ºè¯¥gridå­˜åœ¨objectçš„å¯èƒ½æ€§è¶Šå¤§ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/33.png)

ä»Fig8çš„å¯è§†åŒ–ç»“æœå¯ä»¥çœ‹å‡ºï¼ŒLead Guidedç­–ç•¥å‡å°‘äº†å™ªå£°ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚

å¯¹äºaux headï¼Œå¯¹äºè¿œç¦»objectä¸­å¿ƒçš„anchor boxï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶objectnessè¿›è¡Œä¸€ä¸ªä¸Šé™çš„é™åˆ¶ã€‚æ˜¯å¦æ·»åŠ è¿™ä¸ªé™åˆ¶çš„æµ‹è¯•ç»“æœè§è¡¨7ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/29.png)

åœ¨è¡¨7ä¸­ï¼Œ"base"æŒ‡çš„æ˜¯åŸºå‡†æ¨¡å‹ï¼Œä¸åŠ aux headã€‚"aux without constraint"è¡¨ç¤ºçš„æ˜¯æ·»åŠ aux headï¼Œä½†æ²¡æœ‰objectnessçš„é™åˆ¶ã€‚"aux with constraint"æŒ‡çš„æ˜¯æ·»åŠ aux headï¼Œå¹¶ä¸”æœ‰objectnessçš„é™åˆ¶ã€‚

è¡¨8æ¯”è¾ƒäº†"aux"å’Œ"partial aux"ä¹‹é—´çš„æ€§èƒ½åŒºåˆ«ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/30.png)

"aux"å’Œ"partial aux"çš„ä»‹ç»è§é™„å½•FigA4ã€‚

# 6.Conclusions

ä¸å†è¯¦è¿°ã€‚

# 7.More comparison

åœ¨5 FPSåˆ°160 FPSè¿™ä¸ªåŒºé—´å†…ï¼Œä¸ç®¡æ˜¯ç²¾åº¦è¿˜æ˜¯é€Ÿåº¦ï¼ŒYOLOv7è¶…è¿‡äº†æ‰€æœ‰å·²çŸ¥çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/31.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/32.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/34.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/35.png)

# 8.A.Appendix

## 8.A.1.Implementation details

### 8.A.1.1.Architectures

YOLOv7 P5çš„æ¨¡å‹ç»“æ„è§FigA1ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/8.png)

å…¶ä¸­ï¼ŒYOLOv7çš„è¯¦ç»†ç»“æ„å¯å‚è€ƒä¸‹é¢ä¸¤å¼ å›¾ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/9.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/10.png)

FigA1ä¸­çš„ç¬¬ä¸€åˆ—YOLOv7æ˜¯æŒ‰ç…§ä¸‹å›¾çº¢è‰²ç®­å¤´æ‰€ç¤ºçš„è·¯å¾„è¿›è¡Œå±•ç¤ºçš„ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/13.png)

YOLOv7 P6çš„æ¨¡å‹ç»“æ„è§FigA2ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/11.png)

å…¶ä¸­ï¼ŒYOLOv7-E6Eçš„è¯¦ç»†ç»“æ„å¯å‚è€ƒä¸‹é¢è¿™å¼ å›¾ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/12.png)

FigA2ä¸­çš„ç¬¬å››åˆ—YOLOv7-E6Eæ˜¯æŒ‰ç…§ä¸‹å›¾çº¢è‰²ç®­å¤´æ‰€ç¤ºçš„è·¯å¾„è¿›è¡Œå±•ç¤ºçš„ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/14.png)

åœ¨ç¬¬3.1éƒ¨åˆ†æˆ‘ä»¬ä¹Ÿæåˆ°è¿‡ï¼Œå¯¹äºE-ELANæ¡†æ¶ï¼Œç”±äºæˆ‘ä»¬çš„edge devideä¸æ”¯æŒ[åˆ†ç»„å·ç§¯](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)å’Œ[channel shuffle](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)æ“ä½œï¼Œæ‰€ä»¥é‡‡ç”¨äº†å¦‚FigA3(b)ä¸­çš„ç­‰ä»·å½¢å¼ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/15.png)

è¿™ç§ç­‰ä»·ç»“æ„ä¹Ÿä½¿å¾—æˆ‘ä»¬æ›´å®¹æ˜“å®ç°partial auxiliary headï¼Œå¦‚FigA4(b)æ‰€ç¤ºã€‚FigA4(a)æ˜¯æ™®é€šçš„auxiliary headã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/16.png)

æ¥ä¸‹æ¥è§£é‡Šä¸‹ç¬¬4.2éƒ¨åˆ†æåˆ°çš„Coarse-to-fine lead head guided label assignerï¼Œå³Fig5(e)æ‰€ç¤ºçš„è¿™ç§æƒ…å†µã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/17.png)

lead headå’Œaux headéƒ½ç”¨åˆ°äº†[SimOTA](https://shichaoxin.com/2024/01/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOX-Exceeding-YOLO-Series-in-2021/#21yolox-darknet53)ï¼ŒFigA5ä¸­çš„3-NN positiveå’Œ5-NN positiveå¯å‚è€ƒåœ¨[YOLOv4](https://shichaoxin.com/2024/01/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/)ä¸­å¯¹"Using multiple anchors for a single ground truth"éƒ¨åˆ†çš„è®²è§£ï¼Œæ­¤å¤„ä¸å†è¯¦è¿°ã€‚å¯¹äºlead headæ¥è¯´ï¼Œ3-NN positiveå°±ç›¸å½“äºæ˜¯[SimOTA](https://shichaoxin.com/2024/01/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOX-Exceeding-YOLO-Series-in-2021/#21yolox-darknet53)ä¸­çš„`fixed center area`ã€‚å¯¹äºaux headæ¥è¯´ï¼Œå…¶[SimOTA](https://shichaoxin.com/2024/01/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOX-Exceeding-YOLO-Series-in-2021/#21yolox-darknet53)çš„æ­£æ ·æœ¬æœç´¢åŒºåŸŸä¸º`fixed center area`ï¼ˆå³5-NN positiveï¼‰ã€lead headçš„æ­£æ ·æœ¬ä»¥åŠGT boxçš„å¹¶é›†ã€‚

### 8.A.1.2.Hyper-parameters

æˆ‘ä»¬æœ‰3ç§ä¸åŒçš„è®­ç»ƒè¶…å‚æ•°è®¾ç½®ã€‚

ğŸ‘‰ç¬¬ä¸€ç§è¶…å‚æ•°è®¾ç½®ï¼š[`hyp.scratch.tiny.yaml`](https://github.com/WongKinYiu/yolov7/blob/main/data/hyp.scratch.tiny.yaml)ï¼Œé€‚ç”¨äºYOLOv7-tinyã€‚

```yaml
lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # optimizer weight decay 5e-4
warmup_epochs: 3.0  # warmup epochs (fractions ok)
warmup_momentum: 0.8  # warmup initial momentum
warmup_bias_lr: 0.1  # warmup initial bias lr
box: 0.05  # box loss gain
cls: 0.5  # cls loss gain
cls_pw: 1.0  # cls BCELoss positive_weight
obj: 1.0  # obj loss gain (scale with pixels)
obj_pw: 1.0  # obj BCELoss positive_weight
iou_t: 0.20  # IoU training threshold
anchor_t: 4.0  # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # image HSV-Value augmentation (fraction)
degrees: 0.0  # image rotation (+/- deg)
translate: 0.1  # image translation (+/- fraction)
scale: 0.5  # image scale (+/- gain)
shear: 0.0  # image shear (+/- deg)
perspective: 0.0  # image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # image flip up-down (probability)
fliplr: 0.5  # image flip left-right (probability)
mosaic: 1.0  # image mosaic (probability)
mixup: 0.05  # image mixup (probability)
copy_paste: 0.0  # image copy paste (probability)
paste_in: 0.05  # image copy paste (probability), use 0 for faster training
loss_ota: 1 # use ComputeLossOTA, use 0 for faster training
```

ğŸ‘‰ç¬¬äºŒç§è¶…å‚æ•°è®¾ç½®ï¼š[`hyp.scratch.p5.yaml`](https://github.com/WongKinYiu/yolov7/blob/main/data/hyp.scratch.p5.yaml)ï¼Œé€‚ç”¨äºYOLOv7å’ŒYOLOv7xã€‚

```yaml
lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # optimizer weight decay 5e-4
warmup_epochs: 3.0  # warmup epochs (fractions ok)
warmup_momentum: 0.8  # warmup initial momentum
warmup_bias_lr: 0.1  # warmup initial bias lr
box: 0.05  # box loss gain
cls: 0.3  # cls loss gain
cls_pw: 1.0  # cls BCELoss positive_weight
obj: 0.7  # obj loss gain (scale with pixels)
obj_pw: 1.0  # obj BCELoss positive_weight
iou_t: 0.20  # IoU training threshold
anchor_t: 4.0  # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # image HSV-Value augmentation (fraction)
degrees: 0.0  # image rotation (+/- deg)
translate: 0.2  # image translation (+/- fraction)
scale: 0.9  # image scale (+/- gain)
shear: 0.0  # image shear (+/- deg)
perspective: 0.0  # image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # image flip up-down (probability)
fliplr: 0.5  # image flip left-right (probability)
mosaic: 1.0  # image mosaic (probability)
mixup: 0.15  # image mixup (probability)
copy_paste: 0.0  # image copy paste (probability)
paste_in: 0.15  # image copy paste (probability), use 0 for faster training
loss_ota: 1 # use ComputeLossOTA, use 0 for faster training
```

ğŸ‘‰ç¬¬ä¸‰ç§è¶…å‚æ•°è®¾ç½®ï¼š[`hyp.scratch.p6.yaml`](https://github.com/WongKinYiu/yolov7/blob/main/data/hyp.scratch.p6.yaml)ï¼Œé€‚ç”¨äºYOLOv7-W6ã€YOLOv7-E6ã€YOLOv7-D6å’ŒYOLOv7-E6Eã€‚

```yaml
lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)
lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # optimizer weight decay 5e-4
warmup_epochs: 3.0  # warmup epochs (fractions ok)
warmup_momentum: 0.8  # warmup initial momentum
warmup_bias_lr: 0.1  # warmup initial bias lr
box: 0.05  # box loss gain
cls: 0.3  # cls loss gain
cls_pw: 1.0  # cls BCELoss positive_weight
obj: 0.7  # obj loss gain (scale with pixels)
obj_pw: 1.0  # obj BCELoss positive_weight
iou_t: 0.20  # IoU training threshold
anchor_t: 4.0  # anchor-multiple threshold
# anchors: 3  # anchors per output layer (0 to ignore)
fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)
hsv_h: 0.015  # image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # image HSV-Value augmentation (fraction)
degrees: 0.0  # image rotation (+/- deg)
translate: 0.2  # image translation (+/- fraction)
scale: 0.9  # image scale (+/- gain)
shear: 0.0  # image shear (+/- deg)
perspective: 0.0  # image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # image flip up-down (probability)
fliplr: 0.5  # image flip left-right (probability)
mosaic: 1.0  # image mosaic (probability)
mixup: 0.15  # image mixup (probability)
copy_paste: 0.0  # image copy paste (probability)
paste_in: 0.15  # image copy paste (probability), use 0 for faster training
loss_ota: 1 # use ComputeLossOTA, use 0 for faster training
```

æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„è¶…å‚æ•°top kç”¨äº[SimOTA](https://shichaoxin.com/2024/01/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOX-Exceeding-YOLO-Series-in-2021/#21yolox-darknet53)ã€‚åœ¨è®­ç»ƒ$640\times 640$æ¨¡å‹æ—¶ï¼Œè®¾ç½®$k=10$ã€‚åœ¨è®­ç»ƒ$1280 \times 1280$æ¨¡å‹æ—¶ï¼Œè®¾ç½®$k=20$ã€‚

### 8.A.1.3.Re-parameterization

å¯å‚è€ƒ[RepVGG](https://shichaoxin.com/2025/10/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RepVGG-Making-VGG-style-ConvNets-Great-Again/)ï¼Œå°†â€œå·ç§¯-BN-æ¿€æ´»å‡½æ•°â€é‡å‚æ•°åŒ–ä¸ºâ€œå·ç§¯-æ¿€æ´»å‡½æ•°â€çš„å…¬å¼è§FigA6ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/18.png)

FigA7å±•ç¤ºäº†åœ¨[YOLOR](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ä¸­ï¼Œå¦‚ä½•å°†[éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)åˆå¹¶åœ¨å·ç§¯ä¸­ï¼ˆä¸ªäººç†è§£ï¼šåº”è¯¥ä¹Ÿæ˜¯åº”ç”¨åœ¨æ¨ç†é˜¶æ®µï¼‰ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/19.png)

åœ¨FigA7ä¸Šå›¾ä¸­ï¼Œå…ˆæ˜¯åŠ æ“ä½œçš„[éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ï¼Œç„¶åæ‰§è¡Œå·ç§¯ï¼Œæ¥ç€åˆæ˜¯ä¸€ä¸ªåŠ æ“ä½œçš„[éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ã€‚åœ¨FigA7ä¸‹å›¾ä¸­ï¼Œå…ˆæ˜¯ä¹˜æ“ä½œçš„[éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ï¼Œç„¶åæ‰§è¡Œå·ç§¯ï¼Œæ¥ç€åˆæ˜¯ä¸€ä¸ªä¹˜æ“ä½œçš„[éšæ€§çŸ¥è¯†](https://shichaoxin.com/2025/07/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-You-Only-Learn-One-Representation-Unified-Network-for-Multiple-Tasks/)ã€‚

## 8.A.2.More results

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/20.png)

### 8.A.2.1.YOLOv7-mask

æˆ‘ä»¬é›†æˆäº†YOLOv7å’ŒBlendMaskç”¨äºå®ä¾‹åˆ†å‰²ã€‚æˆ‘ä»¬åªæ˜¯ç®€å•çš„å°†YOLOv7ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨MS COCOå®ä¾‹åˆ†å‰²æ•°æ®é›†ä¸Šè®­ç»ƒäº†30ä¸ªepochã€‚å®ƒå°±è¾¾åˆ°äº†SOTAçš„å®æ—¶å®ä¾‹åˆ†å‰²ç»“æœã€‚YOLOv7-maskçš„æ¨¡å‹æ¡†æ¶è§FigA8(a)ï¼Œä¸€äº›æ£€æµ‹ç»“æœè§FigA9ã€‚

>BlendMaskè®ºæ–‡ï¼šChen et al. BlendMask: Top-down meets bottom-up for instance segmentation. CVPR, 2020.ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/21.png)

### 8.A.2.2.YOLOv7-pose

æˆ‘ä»¬é›†æˆäº†YOLOv7å’ŒYOLO-Poseç”¨äºå…³é”®ç‚¹æ£€æµ‹ã€‚æˆ‘ä»¬éµå¾ªå’ŒYOLO-Poseä¸€æ ·çš„è®¾ç½®ï¼Œå°†YOLOv7-W6äººä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹åœ¨MS COCOå…³é”®ç‚¹æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œfine-tuneã€‚YOLOv7-W6-poseè¾¾åˆ°äº†SOTAçš„å®æ—¶äººä½“å§¿æ€ä¼°è®¡ç»“æœã€‚YOLOv7-W6-poseçš„æ¨¡å‹æ¡†æ¶è§FigA8(b)ï¼Œä¸€äº›æ£€æµ‹ç»“æœè§FigA10ã€‚

>YOLO-Poseè®ºæ–‡ï¼šMaji et al. YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss. CVPRW, 2022.ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/YOLOv7/22.png)

# 9.åŸæ–‡é“¾æ¥

ğŸ‘½[YOLOv7ï¼šTrainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://github.com/x-jeff/AI_Papers/blob/master/2025/YOLOv7%EF%BC%9ATrainable%20bag-of-freebies%20sets%20new%20state-of-the-art%20for%20real-time%20object%20detectors.pdf)

# 10.å‚è€ƒèµ„æ–™

1. [åœ–è§£ YOLOv7 architecture (1/2)](https://www.youtube.com/watch?v=Ot__47ItjDs)
2. [åœ–è§£ YOLOv7 loss (2/2)](https://www.youtube.com/watch?v=EhXwABGhBrw)
3. [open-mmlab/mmyolo](https://github.com/open-mmlab/mmyolo/tree/main/configs/yolov7)