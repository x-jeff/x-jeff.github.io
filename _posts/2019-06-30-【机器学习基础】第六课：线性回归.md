---
layout:     post
title:      【机器学习基础】第六课：线性回归
subtitle:   线性模型，线性回归，最小二乘法，广义线性模型，距离的定义，多变量线性回归
date:       2019-06-30
author:     x-jeff
header-img: blogimg/20190630.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.线性模型基本形式

给定由$d$个属性描述的示例$x=(x_1;x_2;...;x_d)$，那么线性模型的基本形式可写为：

$$f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b$$

一般用向量形式写成：

$$f(x)=w^Tx+b$$

其中，$w=(w_1;w_2;...;w_d)$。

把矩阵展开写，即为：

$$\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}  =\begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} \\ x_{21} & x_{22} & \cdots & x_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nd} \end{pmatrix} \cdot \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_d \end{pmatrix} + \begin{pmatrix} b \\ b \\ \vdots \\ d \end{pmatrix}$$

(矩阵维数：$n\times 1=(n\times d)\cdot (d\times 1)+(n\times 1)$)

其中，$n$为数据条数，$d$为属性个数。

# 2.线性回归

先考虑一种最简单的情形：输入属性的数目只有一个，此时有两种情况：

1. 属性值为连续型数据。
2. 属性值为离散型数据。
	* 若属性值间存在“序”关系，可通过连续化将其转化为连续值，例如二值属性“身高”的取值“高”“矮”可转化为$\{1,0\}$，三值属性“高度”的取值“高”“中”“低”可转化为$\{1,0.5,0\}$
	* 若属性值间不存在序关系，假定有$k$个属性值，则通常转化为$k$维向量，例如属性“瓜类”的取值“西瓜”“南瓜”“黄瓜”可转化为$(0,0,1),(0,1,0),(1,0,0)$（👉即**one-hot编码**）（⚠️若将无序属性连续化，则会不恰当地引入“序”关系，对后续处理如距离计算等造成误导）。

含有$m$条数据的数据集$D$可表示为$D=\{(x_i,y_i)\}_{i=1}^{m}$。

线性回归试图学得：

$$f(x_i)=wx_i+b$$

使得$f(x_i)\simeq y_i$，即$f(x_i)$去逼近$y_i$。

此时我们只要求得$w$和$b$的值即可构建出该线性回归模型。如果使用“均方误差”作为模型的性能度量（均方误差是回归任务中最常用的性能度量），则现在的任务是试图求出一组$(w,b)$可使均方误差最小化，即：

$$\begin{align} (w^*,b^*) & =\mathop{\arg\min}_{(w,b)} \sum_{i=1}^m(y_i-f(x_i))^2 \\ & = \mathop{\arg\min}_{(w,b)} \sum_{i=1}^m(y_i-wx_i-b)^2 \end{align}$$

* $w^*,b^*$表示$w$和$b$的解。
* $\arg\min$：就是使后面这个式子达到最小值时的变量的取值；$\arg\min$

