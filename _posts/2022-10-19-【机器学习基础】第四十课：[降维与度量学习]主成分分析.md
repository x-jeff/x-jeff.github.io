---
layout:     post
title:      【机器学习基础】第四十课：[降维与度量学习]主成分分析
subtitle:   主成分分析（PCA），矩阵的内积（弗罗比尼乌斯内积），矩阵的外积（克罗内克积）
date:       2022-10-19
author:     x-jeff
header-img: blogimg/20221019.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.主成分分析

>[【数学基础】第十六课：主成分分析](http://shichaoxin.com/2020/09/21/数学基础-第十六课-主成分分析/)

主成分分析（Principal Component Analysis，简称PCA）是最常用的一种降维方法。在介绍PCA之前，不妨先考虑这样一个问题：对于正交属性空间中的样本点，如何用一个超平面（直线的高维推广）对所有样本进行恰当的表达？

>“主成分分析”亦称“主分量分析”。

容易想到，若存在这样的超平面，那么它大概应具有这样的性质：

* **最近重构性：**样本点到这个超平面的距离都足够近；
* **最大可分性：**样本点在这个超平面上的投影能尽可能分开。

有趣的是，基于最近重构性和最大可分性，能分别得到主成分分析的两种等价推导，我们先从最近重构性来推导。

## 1.1.最近重构性

2

假定数据样本进行了中心化，即$\sum\_i \mathbf{x}\_i=0$；再假定投影变换后得到的新坐标系为$\\{\mathbf{w}\_1,\mathbf{w}\_2,...,\mathbf{w}\_d \\}$，其中$\mathbf{w}\_i$是标准正交基向量，$\parallel \mathbf{w}\_i \parallel\_2 = 1, \mathbf{w}\_i^T \mathbf{w}\_j = 0\  (i \neq j)$。若丢弃新坐标系中的部分坐标，即将维度降低到$d' < d$，则样本点$\mathbf{x}_i$在低维坐标系中的投影是$\mathbf{z}\_i = (z\_{i1};z\_{i2};...;z\_{id'})$，其中$z\_{ij} = \mathbf{w}\_j^T \mathbf{x}\_i$是$\mathbf{x}_i$在低维坐标系下第$j$维的坐标。若基于$\mathbf{z}_i$来重构$\mathbf{x}_i$，则会得到$\hat{\mathbf{x}}\_i = \sum\_{j=1}^{d'} z\_{ij} \mathbf{w}\_j$。

>**标准正交基：**在线性代数中，一个内积空间的正交基（orthogonal basis）是元素两两正交的基。称基中的元素为基向量。假若，一个正交基的基向量的模长都是单位长度1，则称这正交基为标准正交基或“规范正交基”（Orthonormal basis）。

对上面一段话说下自己的理解。假设投影变换后的新坐标系表示为：

$$\mathbf{W}_d = \{\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_d \}$$

其中，$\mathbf{w}_i$为$d\times 1$维的向量，比如我们常见的三维坐标系可表示为：

$$\mathbf{W}_3 = \{\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3 \} = \{ \begin{bmatrix} 1 \\ 0 \\ 0 \\ \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ \end{bmatrix} \}$$

其实$\mathbf{W}_d$可以看作一个$d\times d$的矩阵，每一列都是一个基向量。为了实现降维的目的，我们直接将矩阵$\mathbf{W}_d$去掉几列，即丢弃一些维度，使得$\mathbf{W}_d$的维度变为$d\times d'$，即下文中提到的$\mathbf{W}$。这样的话，只要求出矩阵$\mathbf{W}$，我们便能通过$\mathbf{W}^T \mathbf{x}_i$（$(d\times d')^T \times (d\times 1)$）来求得样本点$\mathbf{x}_i$降维后的值，即$\mathbf{z}_i$（$d' \times 1$）。

$z_{ij} = \mathbf{w}_j^T \mathbf{x}_i$是$\mathbf{x}_i$在低维坐标系下第$j$维的坐标。这句话也很好理解，举个例子，假设是三维坐标系，$\mathbf{x}_i = (1;2;3)$，那么就有：

$$z_{i1} = \mathbf{w}_1^T \mathbf{x}_i = \begin{bmatrix} 1 & 0 & 0 \\ \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \\ \end{bmatrix} = 1$$

所以说$z_{ij}$就是一个具体的数，也就是第$j$维的值。

关于$\hat{\mathbf{x}}\_i$的计算，因为$z\_{ij}$是一个实数，$\mathbf{w}\_j$的维度是$d\times 1$，所以得到的重构的$\hat{\mathbf{x}}\_i$的维度也是$d\times 1$。此外，有$\hat{\mathbf{x}}\_i = \sum\_{j=1}^{d'} z\_{ij} \mathbf{w}\_j = \mathbf{W} \mathbf{z}\_i$，解释一下这个等式，假设有：

$$\mathbf{W}_d = \begin{bmatrix}  1 & 0 & 0 \\ 0& 1 & 0 \\ 0& 0& 1 \\ \end{bmatrix}$$

假设降维时去掉最后一列：

$$\mathbf{W} = \begin{bmatrix}  1 & 0  \\ 0& 1  \\ 0& 0 \\ \end{bmatrix}$$

此外，有：

$$\mathbf{z}_i = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$$

则可得到：

$$\hat{\mathbf{x}}_i = \sum_{j=1}^{d'} z_{ij} \mathbf{w}_j = 2\begin{bmatrix} 1 \\ 0 \\ 0 \\  \end{bmatrix} + 3 \begin{bmatrix} 0 \\ 1 \\ 0 \\  \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 0 \\  \end{bmatrix}$$

$$\hat{\mathbf{x}}_i = \mathbf{W} \mathbf{z}_i = \begin{bmatrix}  1 & 0  \\ 0& 1  \\ 0& 0 \\ \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 0 \\  \end{bmatrix}$$

可以看到，二者是相等的。再用公式推导一下：

$$\begin{align} \sum_{j=1}^{d'} z_{ij} \mathbf{w}_j &= z_{i1}\mathbf{w}_1 + z_{i2}\mathbf{w}_2 + ... + z_{id'}\mathbf{w}_{d'} \\&= z_{i1} \begin{bmatrix} w_{11} \\ w_{21} \\ \vdots \\ w_{d1} \\ \end{bmatrix} + z_{i2}\begin{bmatrix} w_{12} \\ w_{22} \\ \vdots \\ w_{d2} \\ \end{bmatrix} +...+z_{id'} \begin{bmatrix} w_{1d'} \\ w_{2d'} \\ \vdots \\ w_{dd'} \\ \end{bmatrix} \\&= \begin{bmatrix} z_{i1} w_{11}+z_{i2} w_{12}+...+z_{id'}w_{1d'} \\ z_{i1} w_{21}+z_{i2} w_{22}+...+z_{id'}w_{2d'} \\ \vdots \\ z_{i1} w_{d1}+z_{i2} w_{d2}+...+z_{id'}w_{dd'}   \end{bmatrix} \\&= \begin{bmatrix} w_{11} & w_{12} & \cdots & w_{1d'} \\ w_{21} & w_{22} & \cdots & w_{2d'} \\ \vdots & \vdots & \ddots & \vdots \\ w_{d1} & w_{d2} & \cdots & w_{dd'} \\ \end{bmatrix} \begin{bmatrix} z_{i1} \\ z_{i2} \\ \vdots \\ z_{id'} \\ \end{bmatrix} \\&= \mathbf{W} \mathbf{z}_i \end{align}$$

在开始公式推导之前，再说两个已知的条件。第一个是$\mathbf{W}^T \mathbf{W} = \mathbf{I}$（$\mathbf{I}$为[单位矩阵](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#23单位矩阵)）。原因就是$\mathbf{w}_i$都是正交基向量，有$\parallel \mathbf{w}_i \parallel_2 = 1, \mathbf{w}_i^T \mathbf{w}_j = 0\  (i \neq j)$。第二个已知条件是$\mathbf{z}_i  = \mathbf{W}^T \mathbf{x}_i$。这个没什么需要多说的，前面已经提到过了。接下来开始公式推导。

考虑整个训练集，原样本点$\mathbf{x}_i$与基于投影重构的样本点$\hat{\mathbf{x}}_i$之间的距离为：

$$\begin{align} \sum_{i=1} ^m \left \| \sum_{j=1}^{d'} z_{ij} \mathbf{w}_j - \mathbf{x}_i \right \|_2^2 &= \sum_{i=1} ^m \left \| \mathbf{W} \mathbf{z}_i - \mathbf{x}_i \right \| _2^2 \\&= \sum_{i=1} ^m (\mathbf{W} \mathbf{z}_i - \mathbf{x}_i)^T (\mathbf{W} \mathbf{z}_i - \mathbf{x}_i) \\&= \sum_{i=1} ^m (\mathbf{z}_i^T \mathbf{W}^T \mathbf{W}\mathbf{z}_i- \mathbf{z}_i^T \mathbf{W}^T\mathbf{x}_i-\mathbf{x}_i^T \mathbf{W} \mathbf{z}_i +\mathbf{x}_i^T \mathbf{x}_i ) \\&=   \sum_{i=1} ^m (\mathbf{z}_i^T\mathbf{z}_i - 2\mathbf{z}_i^T\mathbf{W}^T \mathbf{x}_i + \mathbf{x}_i^T \mathbf{x}_i  ) \\&= \sum_{i=1} ^m \mathbf{z}_i^T\mathbf{z}_i -2\sum_{i=1} ^m  \mathbf{z}_i^T\mathbf{W}^T \mathbf{x}_i + \sum_{i=1} ^m  \mathbf{x}_i^T \mathbf{x}_i \\&= \sum_{i=1} ^m \mathbf{z}_i^T\mathbf{z}_i -2\sum_{i=1} ^m  \mathbf{z}_i^T\mathbf{W}^T \mathbf{x}_i + \text{const} \\&= \sum_{i=1} ^m \mathbf{z}_i^T\mathbf{z}_i -2\sum_{i=1} ^m  \mathbf{z}_i^T\mathbf{z}_i + \text{const} \\&= -\sum_{i=1} ^m  \mathbf{z}_i^T\mathbf{z}_i + \text{const} \\&= -\sum_{i=1} ^m \text{tr} (\mathbf{z}_i \mathbf{z}_i^T) + \text{const} \\&= -\text{tr} \left( \sum_{i=1} ^m \mathbf{z}_i \mathbf{z}_i^T \right) +  \text{const} \\&= -\text{tr} \left( \sum_{i=1}^m \mathbf{W}^T \mathbf{x}_i \mathbf{x}_i^T \mathbf{W} \right) + \text{const} \\&= -\text{tr} \left( \mathbf{W}^T \left( \sum_{i=1}^m \mathbf{x}_i \mathbf{x}_i^T \right) \mathbf{W} \right) +\text{const} \\&\propto -\text{tr} \left( \mathbf{W}^T \left( \sum_{i=1}^m \mathbf{x}_i \mathbf{x}_i^T \right) \mathbf{W} \right)  \end{align} \tag{1}$$

>$\text{const}$是一个常数。$\propto$表示正比于。

根据最近重构性，式(1)应被最小化，考虑到$\mathbf{w}_j$是标准正交基，$\sum_i \mathbf{x}_i \mathbf{x}_i^T$是[协方差矩阵](http://shichaoxin.com/2019/12/18/数学基础-第九课-协方差和相关系数/#32协方差矩阵)，有：

$$\begin{align*}
&\min \limits_{\mathbf{W}} \quad -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) \\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.& \mathbf{W}^T\mathbf{W}=\mathbf{I} \\ \end{array} \end{align*} \tag{2}$$

这就是主成分分析的优化目标。

## 1.2.最大可分性

从最大可分性出发，能得到主成分分析的另一种解释。我们知道，样本点$\mathbf{x}_i$在新空间中超平面上的投影是$\mathbf{W}^T \mathbf{x}_i$，若所有样本点的投影能尽可能分开，则应该使投影后样本点的方法最大化，如图10.4所示。

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/MachineLearningSeries/Lesson40/40x1.png)

投影后样本点的方差是$\sum_i \mathbf{W}^T \mathbf{x}_i \mathbf{x}_i^T \mathbf{W}$，于是优化目标可写为：

$$\begin{align*}
&\max \limits_{\mathbf{W}} \quad \text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) \\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.& \mathbf{W}^T\mathbf{W}=\mathbf{I} \\ \end{array} \end{align*} \tag{3}$$

显然，式(2)与式(3)等价。

对式(2)或式(3)使用[拉格朗日乘子法](http://shichaoxin.com/2019/10/17/机器学习基础-第八课-线性判别分析/#21拉格朗日乘子法)可得：

$$\mathbf{XX}^T\mathbf{W}=\lambda \mathbf{W} \tag{4}$$

以式(2)为例推导一下式(4)。在式(2)中，$\mathbf{X}=(\mathbf{x}\_1,\mathbf{x}\_2,...,\mathbf{x}\_m) \in \mathbb{R}^{d\times m},\mathbf{W}=(\mathbf{w}\_1,\mathbf{w}\_2,...,\mathbf{w}\_{d'})\in \mathbb{R}^{d\times d'},\mathbf{I}\in \mathbb{R}^{d' \times d'}$。对于带矩阵约束的优化问题，此优化目标的[拉格朗日函数](http://shichaoxin.com/2019/10/17/机器学习基础-第八课-线性判别分析/#21拉格朗日乘子法)为：

$$\begin{align} L(\mathbf{W},\Theta) &= -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) + \langle \Theta,\mathbf{W}^T \mathbf{W}-\mathbf{I} \rangle \\&= -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) + \text{tr} (\Theta^T (\mathbf{W}^T \mathbf{W}-\mathbf{I}) ) \end{align}$$

>$\langle ... \rangle$为矩阵内积，见本文第2部分。

其中，$\Theta \in \mathbb{R}^{d'\times d'}$为拉格朗日乘子矩阵，其维度恒等于约束条件的维度，且其中的每个元素均为未知的拉格朗日乘子。若此时仅考虑约束$\mathbf{w}\_i^T \mathbf{w}\_i = 1 \  (i=1,2,...,d')$，则拉格朗日乘子矩阵$\Theta$此时为[对角矩阵](http://shichaoxin.com/2020/08/12/数学基础-第十五课-矩阵的相似变换和相合变换/#3正交相似变换)（个人理解：先只考虑主对角线），令新的拉格朗日乘子矩阵为$\Lambda = \text{diag} (\lambda\_1, \lambda\_2,...,\lambda\_{d'}) \in \mathbb{R}^{d' \times d'}$，则新的拉格朗日函数为：

$$L (\mathbf{W},\Lambda) = -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) + \text{tr} (\Lambda^T (\mathbf{W}^T \mathbf{W}-\mathbf{I}) ) $$

对拉格朗日函数关于$\mathbf{W}$求导可得：

$$\begin{align} \frac{\partial L(\mathbf{W},\Lambda)}{\partial \mathbf{W}} &= \frac{\partial}{\partial \mathbf{W}} [ -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) + \text{tr} (\Lambda^T (\mathbf{W}^T \mathbf{W}-\mathbf{I}) ) ] \\&= -\frac{\partial}{\partial \mathbf{W}}  \text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W})+\frac{\partial}{\partial \mathbf{W}} \text{tr} (\Lambda^T (\mathbf{W}^T \mathbf{W}-\mathbf{I}) ) \\ \end{align}$$

由矩阵微分公式$\frac{\partial}{\partial \mathbf{X}} \text{tr} \  (\mathbf{X}^T \mathbf{B} \mathbf{X})=\mathbf{B}\mathbf{X}+\mathbf{B}^T\mathbf{X}, \frac{\partial}{\partial \mathbf{X}} \text{tr} \  (\mathbf{B} \mathbf{X}^T \mathbf{X})=\mathbf{XB}^T+\mathbf{XB}$可得：

$$\begin{align} \frac{\partial L(\mathbf{W},\Lambda)}{\partial \mathbf{W}} &= -2\mathbf{X} \mathbf{X}^T \mathbf{W} + \mathbf{W} \Lambda + \mathbf{W} \Lambda^T \\&= -2 \mathbf{XX}^T \mathbf{W} + \mathbf{W} (\Lambda+\Lambda^T) \\&= -2 \mathbf{XX}^T \mathbf{W} + 2\mathbf{W} \Lambda \end{align}$$

令$\frac{\partial L(\mathbf{W},\Lambda)}{\partial \mathbf{W}}=0$可得：

$$-2 \mathbf{XX}^T \mathbf{W} + 2\mathbf{W} \Lambda = 0$$

$$\mathbf{XX}^T \mathbf{W} = \mathbf{W} \Lambda $$

将$\mathbf{W}$和$\Lambda$展开可得：

$$\mathbf{XX}^T \mathbf{w}_i = \lambda_i \mathbf{w}_i, \quad i=1,2,...,d'$$

显然，此式为[矩阵特征值和特征向量](http://shichaoxin.com/2020/08/12/数学基础-第十五课-矩阵的相似变换和相合变换/#121矩阵的特征值和特征向量)的定义式，其中$\lambda_i,\mathbf{w}_i$分别表示矩阵$\mathbf{XX}^T$的特征值和单位特征向量。由于以上是仅考虑约束$\mathbf{w}_i^T \mathbf{w}_i=1$所求得的结果，而$\mathbf{w}_i$还需满足约束$\mathbf{w}_i^T \mathbf{w}_j=0\  (i\neq j)$。观察$\mathbf{XX}^T$的定义可知，$\mathbf{XX}^T$是一个[实对称矩阵](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#25对称矩阵和反对称矩阵)，**实对称矩阵的不同特征值所对应的特征向量之间相互正交，同一特征值的不同特征向量可以通过施密特正交化使其变得正交**，所以通过上式求得的$\mathbf{w}_i$可以同时满足约束$\mathbf{w}_i^T \mathbf{w}_i = 1, \mathbf{w}_i^T \mathbf{w}_j=0\  (i\neq j)$。

>**施密特正交化（Schmidt orthogonalization）**是求欧式空间正交基的一种方法。从欧式空间任意线性无关的向量组$\alpha_1,\alpha_2,...,\alpha_m$出发，求得正交向量组$\beta_1,\beta_2,...,\beta_m$，使由$\alpha_1,\alpha_2,...,\alpha_m$与向量组$\beta_1,\beta_2,...,\beta_m$等价，再将正交向量组中每个向量经过单位化，就得到一个标准正交向量组，这种方法称为施密特正交化。

根据[拉格朗日乘子法](http://shichaoxin.com/2019/10/17/机器学习基础-第八课-线性判别分析/#21拉格朗日乘子法)的原理可知，此时求得的结果仅是最优解的必要条件，而且$\mathbf{XX}^T$有$d$个相互正交的单位特征向量，所以还需要从这$d$个特征向量里找出$d'$个能使得目标函数达到最优值的特征向量作为最优解。将$\mathbf{XX}^T \mathbf{w}_i = \lambda_i \mathbf{w}_i$代入目标函数可得：

$$\begin{align} \min \limits_{\mathbf{W}} \  -\text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) &= \max \limits_{\mathbf{W}} \  \text{tr} (\mathbf{W}^T \mathbf{X}\mathbf{X}^T\mathbf{W}) \\&=  \max \limits_{\mathbf{W}} \  \sum_{i=1}^{d'} \mathbf{w}_i^T \mathbf{XX}^T \mathbf{w}_i \\&= \max \limits_{\mathbf{W}} \  \sum_{i=1}^{d'} \mathbf{w}_i^T \cdot \lambda_i \mathbf{w}_i \\&=  \max \limits_{\mathbf{W}} \  \sum_{i=1}^{d'} \lambda_i \mathbf{w}_i^T \mathbf{w}_i \\&= \max \limits_{\mathbf{W}} \  \sum_{i=1}^{d'} \lambda_i \end{align}$$

显然，此时只需要令$\lambda_1,\lambda_2,...,\lambda_{d'}$和$\mathbf{w}\_1,\mathbf{w}\_2,...,\mathbf{w}\_{d'}$分别为矩阵$\mathbf{XX}^T$的前$d'$个最大的特征值和单位特征向量就能使得目标函数达到最优值。

>实践中常通过对$\mathbf{X}$进行[奇异值分解](http://shichaoxin.com/2020/11/24/数学基础-第十七课-奇异值分解/)来代替协方差矩阵的特征值分解。

PCA算法描述如下图所示：

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/MachineLearningSeries/Lesson40/40x2.png)

>PCA也可看作是逐一选取方差最大方向，即先对协方差矩阵$\sum\_i \mathbf{x}\_i \mathbf{x}\_i^T$做特征值分解，取最大特征值对应的特征向量$\mathbf{w}\_1$；再对$\sum\_i \mathbf{x}\_i \mathbf{x}\_i^T - \lambda\_1 \mathbf{w}\_1 \mathbf{w}\_1^T$做特征值分解，取最大特征值对应的特征向量$\mathbf{w}\_2$。由$\mathbf{W}$各分量正交及$\sum\_{i=1}^m \mathbf{x}\_i \mathbf{x}\_i^T = \sum\_{j=1}^d  \lambda\_j \mathbf{w}\_j \mathbf{w}\_j^T$可知，上述逐一选取方差最大方向的做法与直接选取最大$d'$个特征值等价。

降维后低维空间的维数$d'$通常是由用户事先指定，或通过在$d'$值不同的低维空间中对[$k$近邻分类器](http://shichaoxin.com/2022/06/23/机器学习基础-第三十八课-降维与度量学习-k近邻学习/)（或其他开销较小的学习器）进行交叉验证来选取较好的$d'$值。对PCA，还可从重构的角度设置一个重构阈值，例如$t= 95\%$，然后选取使下式成立的最小$d'$值：

$$\frac{\sum_{i=1}^{d'}\lambda_i}{\sum_{i=1}^d \lambda_i} \geqslant t$$

PCA仅需保留$\mathbf{W}$与样本的均值向量（保存均值向量是为了通过向量减法对新样本同样进行中心化）即可通过简单的向量减法和矩阵-向量乘法将新样本投影至低维空间中。显然，低维空间与原始高维空间必有不同，因为对应于最小的$d-d'$个特征值的特征向量被舍弃了，这是降维导致的结果。但舍弃这部分信息往往是必要的：一方面，舍弃这部分信息之后能使样本的采样密度增大，这正是降维的重要动机；另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的效果。

# 2.矩阵的内积

在数学中，**弗罗比尼乌斯内积（Frobenius inner product）**是一种基于两个矩阵的二元运算，结果是一个数值。它常常被记为$\langle \mathbf{A}, \mathbf{B} \rangle_F$。这个运算是一个将矩阵视为向量的逐元素内积。参与运算的两个矩阵必须有相同的维度、行数和列数，但不局限于方阵。

## 2.1.定义

给定两个$n\times m$维复矩阵$\mathbf{A}$和$\mathbf{B}$：

$$\mathbf{A} = \begin{pmatrix} A_{11} & A_{12} & \cdots & A_{1m} \\ A_{21} & A_{22} & \cdots & A_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nm} \\ \end{pmatrix}, \mathbf{B} = \begin{pmatrix} B_{11} & B_{12} & \cdots & B_{1m} \\ B_{21} & B_{22} & \cdots & B_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ B_{n1} & B_{n2} & \cdots & B_{nm} \\ \end{pmatrix}$$

>复矩阵，指的是元素中含有复数的矩阵，实矩阵是复矩阵的特例。

弗罗比尼乌斯内积定义为如下的矩阵元素求和：

$$\langle \mathbf{A},\mathbf{B} \rangle_F = \sum_{i,j} \overline{A_{ij}} B_{ij} = \text{tr} \left( \overline{\mathbf{A^T}} \mathbf{B} \right)$$

其中上划线表示复数和复矩阵的共轭操作。若将定义详细写出，则有：

$$\begin{align} \langle \mathbf{A},\mathbf{B} \rangle_F &= \overline{A_{11}}B_{11} + \overline{A_{12}}B_{12} + \cdots + \overline{A_{1m}}B_{1m} \\&\quad  + \overline{A_{21}}B_{21} + \overline{A_{22}}B_{22} + \cdots + \overline{A_{2m}} B_{2m} \\&\quad \vdots \\&\quad + \overline{A_{n1}}B_{n1} + \overline{A_{n2}}B_{n2} + \cdots + \overline{A_{nm}}B_{nm}  \end{align}$$

此计算与[点积](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#64数量积)十分相似，所以是一个内积的范例。

## 2.2.性质

弗罗比尼乌斯内积是半双线性形式。给定复矩阵$\mathbf{A},\mathbf{B},\mathbf{C},\mathbf{D}$以及复数$a$和$b$，我们有：

$$\langle a \mathbf{A}, b \mathbf{B} \rangle_F = \overline{a}b \langle \mathbf{A},\mathbf{B} \rangle_F$$

$$\langle \mathbf{A}+\mathbf{C},\mathbf{B}+\mathbf{D} \rangle_F = \langle \mathbf{A},\mathbf{B} \rangle_F + \langle \mathbf{A},\mathbf{D} \rangle_F+\langle \mathbf{C},\mathbf{B} \rangle_F+\langle \mathbf{C},\mathbf{D} \rangle_F$$

并且，交换复矩阵的次序所得到的是原来结果的共轭矩阵：

$$\langle \mathbf{B},\mathbf{A} \rangle_F = \overline{ \langle \mathbf{A},\mathbf{B} \rangle_F }$$

对于相同的矩阵，有：

$$\langle \mathbf{A},\mathbf{A} \rangle_F \geqslant 0$$

## 2.3.样例

### 2.3.1.实矩阵

给定实矩阵：

$$\mathbf{A} = \begin{pmatrix} 2 & 0 & 6 \\ 1 & -1 & 2 \\ \end{pmatrix}, \mathbf{B} = \begin{pmatrix} 8 & -3 & 2 \\ 4 & 1 & -5 \\ \end{pmatrix}$$

则：

$$\begin{align} \langle \mathbf{A},\mathbf{B} \rangle_F &= 2\cdot 8 + 0 \cdot (-3) + 6\cdot 2 + 1\cdot 4 + (-1) \cdot 1 + 2\cdot (-5) \\&= 16+12+4-1-10 \\&= 21 \end{align}$$

### 2.3.2.复矩阵

给定复矩阵：

$$\mathbf{A} = \begin{pmatrix} 1+i & -2i \\ 3 & -5 \\ \end{pmatrix}, \mathbf{B} = \begin{pmatrix} -2 & 3i \\ 4-3i & 6 \end{pmatrix}$$

那么它们的共轭（非转置）矩阵为：

$$\overline{\mathbf{A}} = \begin{pmatrix} 1-i & +2i \\ 3 & -5 \\ \end{pmatrix}, \overline{\mathbf{B}} = \begin{pmatrix} -2 & -3i \\ 4+3i & 6 \end{pmatrix}$$

因此，

$$\begin{align} \langle \mathbf{A},\mathbf{B} \rangle_F &= (1-i)\cdot (-2) + (+2i)\cdot 3i+3\cdot (4-3i)+(-5)\cdot 6 \\&= (-2+2i)+(-6)+12-9i+(-30) \\&= -26-7i \end{align}$$

但注意：

$$\begin{align} \langle \mathbf{B},\mathbf{A} \rangle_F &= (-2)\cdot (1+i) + (-3i)\cdot (-2i) + (4+3i) \cdot 3 + 6\cdot(-5) \\&= -26+7i \end{align}$$

$\mathbf{A}$、$\mathbf{B}$与其本身的弗罗比尼乌斯内积分别为：

$$\langle \mathbf{A},\mathbf{A} \rangle_F=2+4+9+25=40$$

$$\langle \mathbf{B},\mathbf{B} \rangle_F = 4+9+25+36=74$$

## 2.4.弗罗比尼乌斯范数

从弗罗比尼乌斯内积我们可以诱导出弗罗比尼乌斯范数：

$$\left \| \mathbf{A} \right \|_F = \sqrt{ \langle \mathbf{A},\mathbf{A} \rangle_F }$$

>这就是[矩阵的$F$范数](http://shichaoxin.com/2020/02/01/深度学习基础-第十一课-正则化/#22矩阵的f范数)。

# 3.矩阵的外积

数学上，**克罗内克积（Kronecker product）**是两个任意大小的矩阵间的运算，表示为$\otimes$。简单地说，就是将前一个矩阵的每个元素乘上后一个完整的矩阵。克罗内克积是外积从向量到矩阵的推广，也是张量积在标准基下的矩阵表示。

## 3.1.定义

如果$\mathbf{A}$是一个$m\times n$的矩阵，而$\mathbf{B}$是一个$p\times q$的矩阵，克罗内克积$\mathbf{A} \otimes \mathbf{B}$则是一个$mp \times nq$的分块矩阵：

$$\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix} a_{11}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ \vdots & \ddots & \vdots \\ a_{m1}\mathbf{B} & \cdots & a_{mn}\mathbf{B} \\ \end{bmatrix}$$

更具体地可表示为：

$$\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix} a_{11}b_{11} & a_{11}b_{12} & \cdots & a_{11}b_{1q} & \cdots & \cdots &   a_{1n}b_{11} & a_{1n}b_{12} & \cdots & a_{1n}b_{1q} \\ a_{11}b_{21} & a_{11}b_{22} & \cdots & a_{11}b_{2q} & \cdots & \cdots &   a_{1n}b_{21} & a_{1n}b_{22} & \cdots & a_{1n}b_{2q} \\ \vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\ a_{11}b_{p1} & a_{11}b_{p2} & \cdots & a_{11}b_{pq} & \cdots & \cdots & a_{1n}b_{p1} & a_{1n}b_{p2} & \cdots & a_{1n}b_{pq} \\ \vdots& \vdots & & \vdots & \ddots & & \vdots & \vdots & & \vdots \\ \vdots& \vdots & & \vdots &  & \ddots & \vdots & \vdots & & \vdots \\ a_{m1}b_{11} & a_{m1}b_{12} & \cdots & a_{m1}b_{1q} & \cdots & \cdots &   a_{mn}b_{11} & a_{mn}b_{12} & \cdots & a_{mn}b_{1q} \\ a_{m1}b_{21} & a_{m1}b_{22} & \cdots & a_{m1}b_{2q} & \cdots & \cdots &   a_{mn}b_{21} & a_{mn}b_{22} & \cdots & a_{mn}b_{2q} \\ \vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\ a_{m1}b_{p1} & a_{m1}b_{p2} & \cdots & a_{m1}b_{pq} & \cdots & \cdots & a_{mn}b_{p1} & a_{mn}b_{p2} & \cdots & a_{mn}b_{pq} \end{bmatrix}$$

我们可以更紧凑地写为：

$$(\mathbf{A} \otimes \mathbf{B})_{p(r-1)+v,q(s-1)+w} = a_{rs}b_{vw}$$

## 3.2.例子

$$\begin{bmatrix} 1 & 2 \\3 & 1 \end{bmatrix} \otimes \begin{bmatrix} 0&3 \\ 2&1 \end{bmatrix} = \begin{bmatrix} 1\cdot 0 & 1\cdot 3 & 2\cdot 0 & 2\cdot 3 \\ 1\cdot 2 & 1\cdot 1 & 2\cdot 2 & 2\cdot 1 \\ 3\cdot 0 & 3\cdot 3 & 1\cdot 0 & 1\cdot 3 \\ 3\cdot 2 & 3\cdot 1 & 1\cdot 2 & 1\cdot 1 \end{bmatrix}= \begin{bmatrix} 0&3&0&6 \\ 2&1&4&2 \\ 0&9&0&3 \\ 6&3&2&1 \\ \end{bmatrix}$$

## 3.3.特性

👉**双线性和结合律**

克罗内克积是张量积的特殊形式，因此满足双线性与结合律：

$$\mathbf{A} \otimes (\mathbf{B}+\mathbf{C}) = \mathbf{A} \otimes \mathbf{B} + \mathbf{A} \otimes \mathbf{C}  \quad  \text{(if B and C have the same size)}$$

$$(\mathbf{A} + \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes \mathbf{C} + \mathbf{B} \otimes \mathbf{C} \quad \text{(if A and B have the same size)}$$

$$(k \mathbf{A})\otimes \mathbf{B}=\mathbf{A} \otimes (k\mathbf{B}) = k(\mathbf{A} \otimes \mathbf{B})$$

$$(\mathbf{A} \otimes \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes (\mathbf{B} \otimes \mathbf{C})$$

其中，$\mathbf{A},\mathbf{B}$和$\mathbf{C}$是矩阵，而$k$是常量。

克罗内克积不符合交换律：通常，$\mathbf{A} \otimes \mathbf{B}$不同于$\mathbf{B} \otimes \mathbf{A}$。

$\mathbf{A} \otimes \mathbf{B}$和$\mathbf{B} \otimes \mathbf{A}$是排列等价的，也就是说，存在排列矩阵$\mathbf{P}$和$\mathbf{Q}$，使得：

$$\mathbf{A} \otimes \mathbf{B} =\mathbf{P}(  \mathbf{B} \otimes \mathbf{A}) \mathbf{Q}$$

如果$\mathbf{A}$和$\mathbf{B}$是方块矩阵，则$\mathbf{A} \otimes \mathbf{B}$和$\mathbf{B} \otimes \mathbf{A}$甚至是排列相似的，也就是说，我们可以取$\mathbf{P}=\mathbf{Q}^T$。

👉**混合乘积性质**

如果$\mathbf{A}$、$\mathbf{B}$、$\mathbf{C}$和$\mathbf{D}$是四个矩阵，且矩阵乘积$\mathbf{AC}$和$\mathbf{BD}$存在，那么：

$$(\mathbf{A}\otimes \mathbf{B})(\mathbf{C}\otimes \mathbf{D})=\mathbf{AC} \otimes \mathbf{BD}$$

这个性质称为“混合乘积性质”，因为它混合了通常的矩阵乘积和克罗内克积。于是可以推出，$\mathbf{A}\otimes \mathbf{B}$是可逆的当且仅当$\mathbf{A}$和$\mathbf{B}$是可逆的，其逆矩阵为：

$$(\mathbf{A}\otimes \mathbf{B})^{-1} = \mathbf{A}^{-1}\otimes \mathbf{B}^{-1}$$

👉**克罗内克和**

如果$\mathbf{A}$是$n\times n$矩阵，$\mathbf{B}$是$m\times m$矩阵，$\mathbf{I}_k$表示$k\times k$单位矩阵，那么我们可以定义克罗内克和$\oplus$为：

$$\mathbf{A} \oplus \mathbf{B} = \mathbf{A} \otimes \mathbf{I}_m + \mathbf{I}_n \otimes \mathbf{B}$$

👉**谱**

假设$\mathbf{A}$和$\mathbf{B}$分别是大小为$n$和$q$的方块矩阵。设$\lambda_1,...,\lambda_n$为$\mathbf{A}$的[特征值](http://shichaoxin.com/2020/08/12/数学基础-第十五课-矩阵的相似变换和相合变换/#121矩阵的特征值和特征向量)，$\mu_1,...,\mu_q$为$\mathbf{B}$的[特征值](http://shichaoxin.com/2020/08/12/数学基础-第十五课-矩阵的相似变换和相合变换/#121矩阵的特征值和特征向量)。那么$\mathbf{A}\otimes \mathbf{B}$的[特征值](http://shichaoxin.com/2020/08/12/数学基础-第十五课-矩阵的相似变换和相合变换/#121矩阵的特征值和特征向量)为：

$$\lambda_i \mu_j , \quad i=1,...,n ;j=1,...,q$$

于是可以推出，两个矩形的克罗内克积的[迹](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#11矩阵的迹trace)和[行列式](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#32行列式)为：

$$\text{tr} (  \mathbf{A}\otimes \mathbf{B}) = \text{tr} \  \mathbf{A}\   \text{tr} \  \mathbf{B}$$

$$\text{det} (  \mathbf{A}\otimes \mathbf{B}) = (\text{det} \ \mathbf{A})^q (\text{det}\  \mathbf{B} )^n$$

👉**奇异值**

如果$\mathbf{A}$和$\mathbf{B}$是长方矩阵，那么我们可以考虑它们的奇异值。假设$\mathbf{A}$有$r_{\mathbf{A}}$个非零的奇异值，它们是：

$$\sigma_{\mathbf{A},i} , \quad i=1,...,r_{\mathbf{A}}$$

类似地，设$\mathbf{B}$的非零奇异值为：

$$\sigma_{\mathbf{B},i} , \quad i=1,...,r_{\mathbf{B}}$$

那么克罗内克积$\mathbf{A} \otimes \mathbf{B}$有$r_{\mathbf{A}} r_{\mathbf{B}}$个非零奇异值，它们是：

$$\sigma_{\mathbf{A},i} \sigma_{\mathbf{B},j}, \quad i=1,...,r_{\mathbf{A}}; j=1,...,r_{\mathbf{B}}$$

由于一个[矩阵的秩](http://shichaoxin.com/2019/08/27/数学基础-第七课-矩阵与向量/#12矩阵的秩rank)等于非零奇异值的数目，因此我们有：

$$\text{rank} (\mathbf{A} \otimes \mathbf{B}) = \text{rank} \  \mathbf{A} \  \text{rank} \  \mathbf{B}$$

👉**与抽象张量积的关系**

矩阵的克罗内克积对应于线性映射的抽象张量积。特别地，如果向量空间$V$、$W$、$X$和$Y$分别具有基$\\{v_1,...,v_m \\}$、$\\{ w_1,...,w_n\\}$、$\\{x_1,...,x_d \\}$和$\\{ y_1,...,y_e \\}$，且矩阵$A$和矩阵$B$分别在恰当的基中表示线性变换$S:V\to X$和$T:W\to Y$，那么矩阵$A \otimes B$表示两个映射的张量积$S \otimes T:V\otimes W \to X \otimes Y$，关于$V \otimes W$的基$\\{ v_1 \otimes w_1, v_1\otimes w_2,...,v_2 \otimes w_1,...,v_m \otimes w_n \\}$和$X \otimes Y$的类似基。

👉**与图的乘积的关系**

两个图的邻接矩阵的克罗内克积是它们的张量积图的邻接矩阵。两个图的邻接矩阵的克罗内克和，则是它们的笛卡儿积图的邻接矩阵。

👉**转置**

克罗内克积转置运算符合分配律：

$$(\mathbf{A} \otimes \mathbf{B})^T = \mathbf{A}^T \otimes \mathbf{B}^T$$

# 4.参考资料

1. [南瓜书PumpkinBook](https://datawhalechina.github.io/pumpkin-book/#/chapter10/chapter10?id=_1014)
2. [标准正交基（百度百科）](https://baike.baidu.com/item/标准正交基/4729576?fr=aladdin)
3. [弗罗比尼乌斯内积（wiki百科）](https://zh.wikipedia.org/wiki/弗罗比尼乌斯内积)
4. [施密特正交化（百度百科）](https://baike.baidu.com/item/施密特正交化?fromModule=lemma_search-box)