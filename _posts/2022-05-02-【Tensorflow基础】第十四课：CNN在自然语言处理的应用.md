---
layout:     post
title:      ã€TensorflowåŸºç¡€ã€‘ç¬¬åå››è¯¾ï¼šCNNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨
subtitle:   tf.app.flagsï¼Œtf.app.runï¼Œtf.flagsï¼Œre.subï¼ŒVocabularyProcessorï¼Œnp.random.permutationï¼Œtf.ConfigProtoï¼Œcompute_gradientsï¼Œapply_gradientsï¼Œtf.nn.zero_fractionï¼Œos.path.abspathï¼Œos.path.curdirï¼Œdatetime.datetime.now().isoformat()ï¼Œyieldï¼Œtf.train.global_step
date:       2022-05-02
author:     x-jeff
header-img: blogimg/20220502.jpg
catalog: true
tags:
    - Tensorflow Series
---
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.CNNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨

CNNé€šå¸¸åº”ç”¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚ä½†è¿‘å‡ å¹´CNNä¹Ÿå¼€å§‹åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¹¶å–å¾—äº†ä¸€äº›å¼•äººæ³¨ç›®çš„æˆç»©ã€‚

CNNåº”ç”¨äºNLPä»»åŠ¡ï¼Œå¤„ç†çš„å¾€å¾€æ˜¯ä»¥çŸ©é˜µå½¢å¼è¡¨è¾¾çš„å¥å­æˆ–æ–‡æœ¬ã€‚çŸ©é˜µä¸­çš„æ¯ä¸€è¡Œå¯¹åº”äºä¸€ä¸ªåˆ†è¯å…ƒç´ ï¼Œä¸€èˆ¬æ˜¯ä¸€ä¸ªå•è¯ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ã€‚å‡è®¾æˆ‘ä»¬ä¸€å…±æœ‰10ä¸ªè¯ï¼Œæ¯ä¸ªè¯éƒ½ç”¨128ç»´çš„å‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ª$10 \times 128$ç»´çš„çŸ©é˜µã€‚æ¯”å¦‚ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/TensorflowSeries/Lesson14/14x1.png)

# 2.ä»£ç å®ç°

ä»£ç åŸºäº[https://github.com/dennybritz/cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)ç¨ä½œä¿®æ”¹ã€‚ä»»åŠ¡æè¿°ï¼šå¯¹ç”µå½±è¯„è®ºè¿›è¡ŒäºŒåˆ†ç±»ï¼ˆå¥½è¯„æˆ–è€…å·®è¯„ï¼‰ã€‚

ğŸ‘‰å¯¼å…¥å¿…è¦çš„åŒ…ï¼š

```python
import tensorflow as tf
import numpy as np
import os
import time
import datetime
import data_helpers
from text_cnn import TextCNN
from tensorflow.contrib import learn
```

ğŸ‘‰å®šä¹‰ä¸€äº›æ¨¡å‹å‚æ•°ï¼š

```python
# Data loading params
## éªŒè¯é›†å æ¯”
tf.flags.DEFINE_float("dev_sample_percentage", .1, "Percentage of the training data to use for validation")
## æ­£æ ·æœ¬è·¯å¾„
tf.flags.DEFINE_string("positive_data_file", "./data/rt-polaritydata/rt-polarity.pos",
                       "Data source for the positive data.")
## è´Ÿæ ·æœ¬è·¯å¾„                    
tf.flags.DEFINE_string("negative_data_file", "./data/rt-polaritydata/rt-polarity.neg",
                       "Data source for the negative data.")
                       
# Model Hyperparameters
## è¯å‘é‡é•¿åº¦
tf.flags.DEFINE_integer("embedding_dim", 128, "Dimensionality of character embedding (default: 128)")
## å·ç§¯æ ¸å¤§å°
tf.flags.DEFINE_string("filter_sizes", "3,4,5", "Comma-separated filter sizes (default: '3,4,5')")
## æ¯ä¸€ç§å·ç§¯æ ¸çš„ä¸ªæ•°
tf.flags.DEFINE_integer("num_filters", 128, "Number of filters per filter size (default: 128)")
## dropoutå‚æ•°
tf.flags.DEFINE_float("dropout_keep_prob", 0.5, "Dropout keep probability (default: 0.5)")
## L2æ­£åˆ™åŒ–å‚æ•°
tf.flags.DEFINE_float("l2_reg_lambda", 0.0, "L2 regularization lambda (default: 0.0)")
                       
# Training parameters
## batch size
tf.flags.DEFINE_integer("batch_size", 64, "Batch Size (default: 64)")
## epochæ•°
tf.flags.DEFINE_integer("num_epochs", 200, "Number of training epochs (default: 200)")
## æ¯å¤šå°‘stepæµ‹è¯•ä¸€æ¬¡
tf.flags.DEFINE_integer("evaluate_every", 100, "Evaluate model on dev set after this many steps (default: 100)")
## æ¯å¤šå°‘stepä¿å­˜ä¸€æ¬¡æ¨¡å‹
tf.flags.DEFINE_integer("checkpoint_every", 100, "Save model after this many steps (default: 100)")
## æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ¨¡å‹
tf.flags.DEFINE_integer("num_checkpoints", 5, "Number of checkpoints to store (default: 5)")

# Misc Parameters
## tensorflowä¼šè‡ªåŠ¨é€‰æ‹©ä¸€ä¸ªå­˜åœ¨å¹¶ä¸”æ”¯æŒçš„è®¾å¤‡æ¥è¿è¡Œoperation
tf.flags.DEFINE_boolean("allow_soft_placement", True, "Allow device soft device placement")
## è·å–ä½ çš„operationså’Œtensorè¢«æŒ‡æ´¾åˆ°å“ªä¸ªè®¾å¤‡ä¸Šè¿è¡Œ
tf.flags.DEFINE_boolean("log_device_placement", False, "Log placement of ops on devices") 

# flagsè§£æ
FLAGS = tf.flags.FLAGS
FLAGS.flag_values_dict()

# æ‰“å°æ‰€æœ‰å‚æ•°
print("\nParameters:")
for attr, value in sorted(FLAGS.flag_values_dict().items()):
    print("{}={}".format(attr.upper(), value))
print("")               
```

```
Parameters:
ALLOW_SOFT_PLACEMENT=True
ALSOLOGTOSTDERR=False
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
LOG_DIR=
LOGTOSTDERR=False
NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg
NUM_CHECKPOINTS=5
NUM_EPOCHS=200
NUM_FILTERS=128
ONLY_CHECK_ARGS=False
OP_CONVERSION_FALLBACK_TO_WHILE_LOOP=False
PDB_POST_MORTEM=False
POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos
PROFILE_FILE=None
RUN_WITH_PDB=False
RUN_WITH_PROFILING=False
SHOWPREFIXFORINFO=True
STDERRTHRESHOLD=fatal
TEST_RANDOM_SEED=301
TEST_RANDOMIZE_ORDERING_SEED=
TEST_SRCDIR=
TEST_TMPDIR=/var/folders/qg/0r2bywpn6s16dsr8j9xyjnm80000gn/T/absl_testing
USE_CPROFILE_FOR_PROFILING=True
V=-1
VERBOSITY=-1
XML_OUTPUT_FILE=
```

`tf.app.flags`ä¸»è¦ç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°çš„è§£æå·¥ä½œï¼Œæ”¯æŒæ¥å—å‘½ä»¤è¡Œä¼ é€’å‚æ•°ã€‚è·Ÿå®ƒé…åˆçš„è¿˜æœ‰ä¸€ä¸ª`tf.app.run`å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œç¨‹åºä¸­mainå‡½æ•°å¹¶è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚æ¯”å¦‚ï¼š

```python
##test_use.py
import tensorflow as tf

##ç¬¬ä¸€ä¸ªæ˜¯å‚æ•°åç§°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯é»˜è®¤å€¼ï¼Œç¬¬ä¸‰ä¸ªæ˜¯å‚æ•°æè¿°
tf.app.flags.DEFINE_string('str_name', 'def_v_1', "descrip1")
tf.app.flags.DEFINE_integer('int_name', 10, "descript2")
tf.app.flags.DEFINE_boolean('bool_name', False, "descript3")
FLAGS = tf.app.flags.FLAGS


##å¿…é¡»å¸¦å‚æ•°ï¼Œå¦åˆ™ï¼š'TypeError: main() takes no arguments (1 given)';   ##mainçš„å‚æ•°åéšæ„å®šä¹‰ï¼Œæ— è¦æ±‚
def main(_):
    print(FLAGS.str_name)
    print(FLAGS.int_name)
    print(FLAGS.bool_name)


if __name__ == '__main__':
    tf.app.run()  # tf.app.run()çš„ä½œç”¨ï¼šå…ˆå¤„ç†flagè§£æï¼Œç„¶åæ‰§è¡Œmainå‡½æ•°ï¼Œ
```

è¾“å‡ºä¸ºï¼š

```
def_v_1
10
False
```

å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œä¿®æ”¹é»˜è®¤å€¼ï¼Œæ¯”å¦‚ï¼š

```
$ python test_use.py --str_name="def_v_2"
```

è¿è¡Œç»“æœä¸ºï¼š

```
def_v_2
10
False
```

åœ¨è€ç‰ˆæœ¬1.0+çš„tensorflowä¸­ä½¿ç”¨`tf.app.flags`æ¥å®šä¹‰å‚æ•°ï¼Œæ–°ç‰ˆæœ¬2.0+ç”¨`tf.flags`æ¥å®šä¹‰å‚æ•°ã€‚

ğŸ‘‰è¯»å…¥æ•°æ®é›†ï¼š

```python
def clean_str(string):
    """
    Tokenization/string cleaning for all datasets except for SST.
    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    """
    # ä¸æ˜¯ç‰¹å®šå­—ç¬¦éƒ½å˜æˆç©ºæ ¼
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    # åŠ ç©ºæ ¼
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    # åŒ¹é…2ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦å˜æˆä¸€ä¸ª" "ç©ºæ ¼
    string = re.sub(r"\s{2,}", " ", string)
    # å»æ‰å¥å­é¦–å°¾çš„ç©ºç™½ç¬¦ï¼Œå†è½¬å°å†™
    return string.strip().lower()


def load_data_and_labels(positive_data_file, negative_data_file):
    """
    Loads MR polarity data from files, splits the data into words and generates labels.
    Returns split sentences and labels.
    """
    # Load data from files
    positive_examples = list(open(positive_data_file, "r", encoding="utf-8").readlines())
    positive_examples = [s.strip() for s in positive_examples]
    negative_examples = list(open(negative_data_file, "r", encoding="utf-8").readlines())
    negative_examples = [s.strip() for s in negative_examples]
    # Split by words
    x_text = positive_examples + negative_examples
    x_text = [clean_str(sent) for sent in x_text]
    # Generate labels
    positive_labels = [[0, 1] for _ in positive_examples]
    negative_labels = [[1, 0] for _ in negative_examples]
    y = np.concatenate([positive_labels, negative_labels], 0)
    return [x_text, y]

# Load data
print("Loading data...")
x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)
```

`re.sub`ç”¨äºæ›¿æ¢å­—ç¬¦ä¸²ä¸­çš„åŒ¹é…é¡¹ï¼š

```python
re.sub(pattern, repl, string, count=0, flags=0)
```

å‚æ•°è¯¦è§£ï¼ˆå‰ä¸‰ä¸ªä¸ºå¿…é€‰å‚æ•°ï¼Œåä¸¤ä¸ªä¸ºå¯é€‰å‚æ•°ï¼‰ï¼š

* `pattern`ï¼šæ­£åˆ™ä¸­çš„æ¨¡å¼å­—ç¬¦ä¸²ã€‚
* `repl`ï¼šæ›¿æ¢çš„å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä¸ºä¸€ä¸ªå‡½æ•°ã€‚
* `string`ï¼šè¦è¢«æŸ¥æ‰¾æ›¿æ¢çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
* `count`ï¼šæ¨¡å¼åŒ¹é…åæ›¿æ¢çš„æœ€å¤§æ¬¡æ•°ï¼Œé»˜è®¤0è¡¨ç¤ºæ›¿æ¢æ‰€æœ‰çš„åŒ¹é…ã€‚
* `flags`ï¼šç¼–è¯‘æ—¶ç”¨çš„åŒ¹é…æ¨¡å¼ï¼Œæ•°å­—å½¢å¼ã€‚

```python
#!/usr/bin/python3
import re

phone = "2004-959-559 # è¿™æ˜¯ä¸€ä¸ªç”µè¯å·ç "

# åˆ é™¤æ³¨é‡Š
num = re.sub(r'#.*$', "", phone)
print("ç”µè¯å·ç  : ", num)

# ç§»é™¤éæ•°å­—çš„å†…å®¹
num = re.sub(r'\D', "", phone)
print("ç”µè¯å·ç  : ", num)
```

```
ç”µè¯å·ç  :  2004-959-559 
ç”µè¯å·ç  :  2004959559
```

ğŸ‘‰å»ºç«‹å­—å…¸ï¼š

```python
# Build vocabulary
max_document_length = max([len(x.split(" ")) for x in x_text])
vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
x = np.array(list(vocab_processor.fit_transform(x_text)))
```

Tensorflowæä¾›äº†`VocabularyProcessor`å‡½æ•°ç”¨äºæ„å»ºè¯å…¸ï¼Œå¾—åˆ°çš„æ•°ç»„`x`ä¸­çš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå¥å­ï¼Œæ•°å­—å¯¹åº”å•è¯åœ¨è¯å…¸ä¸­çš„ç´¢å¼•ï¼Œ`x`çš„åˆ—æ•°é€šå¸¸è®¾ä¸ºæœ€é•¿å¥å­çš„å•è¯æ•°ï¼Œå•è¯æ•°ä¸è¶³çš„å¥å­ç”¨0è¡¥é½ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/TensorflowSeries/Lesson14/14x2.png)

ğŸ‘‰å°†æ•°æ®æ‰“ä¹±ï¼š

```python
# Randomly shuffle data
np.random.seed(10)
shuffle_indices = np.random.permutation(np.arange(len(y)))
x_shuffled = x[shuffle_indices]
y_shuffled = y[shuffle_indices]
```

`np.random.permutation`ç”¨äºéšæœºæ’åºï¼š

```
>>> np.random.permutation(10)
array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random
    
>>> np.random.permutation([1, 4, 9, 12, 15])
array([15,  1,  9,  4, 12]) # random
    
>>> arr = np.arange(9).reshape((3, 3))
>>> np.random.permutation(arr)
array([[6, 7, 8], # random
       [0, 1, 2],
       [3, 4, 5]])
```

ğŸ‘‰åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
# Split train/test set
dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))
x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
```

ğŸ‘‰ä¼ å…¥å‚æ•°ï¼š

```python
with tf.Graph().as_default():
    session_conf = tf.ConfigProto(
        allow_soft_placement=FLAGS.allow_soft_placement,
        log_device_placement=FLAGS.log_device_placement)
    sess = tf.Session(config=session_conf)
    with sess.as_default():
        cnn = TextCNN(
            sequence_length=x_train.shape[1],
            num_classes=y_train.shape[1],
            vocab_size=len(vocab_processor.vocabulary_),
            embedding_size=FLAGS.embedding_dim,
            filter_sizes=list(map(int, FLAGS.filter_sizes.split(","))),
            num_filters=FLAGS.num_filters,
            l2_reg_lambda=FLAGS.l2_reg_lambda)
```

`tf.ConfigProto`åœ¨åˆ›å»ºä¼šè¯çš„æ—¶å€™è¿›è¡Œå‚æ•°é…ç½®ï¼Œæ¯”å¦‚GPUã€CPUã€æ˜¾å­˜ç­‰ã€‚`TextCNN`å®šä¹‰äº†ç¬¬1éƒ¨åˆ†ä¸­æ‰€ç¤ºçš„ç½‘ç»œç»“æ„ï¼Œè¯¦è§åšæ–‡æœ«å°¾æ‰€é™„çš„ä»£ç é“¾æ¥ä¸­çš„`text_cnn.py`ï¼Œå¾ˆç®€å•çš„å®ç°ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚

ğŸ‘‰å®šä¹‰è®­ç»ƒï¼š

```python
# Define Training procedure
global_step = tf.Variable(0, name="global_step", trainable=False)
optimizer = tf.train.AdamOptimizer(1e-3)
grads_and_vars = optimizer.compute_gradients(cnn.loss)
train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)
```

é€šå¸¸æ‰€ç”¨çš„`minimize()`å†…éƒ¨å…¶å®ä¹Ÿæ˜¯åˆ†ä¸¤éƒ¨åˆ†ï¼šç¬¬ä¸€æ­¥ï¼Œ`compute_gradients`æ ¹æ®lossç›®æ ‡å‡½æ•°è®¡ç®—æ¢¯åº¦ï¼›ç¬¬äºŒæ­¥ï¼Œ`apply_gradients`ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ¥æ›´æ–°å¯¹åº”çš„Variableã€‚ä¹‹æ‰€ä»¥åˆ†å¼€ï¼Œæ˜¯å› ä¸ºæœ‰æ—¶å€™éœ€è¦å¯¹è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦åšä¸€å®šçš„ä¿®æ­£ï¼Œä»¥é˜²[æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±](http://shichaoxin.com/2020/02/07/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬åä¸‰è¯¾-æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸/)ã€‚

ğŸ‘‰å°†æ¢¯åº¦çš„å˜åŒ–è®°å½•åˆ°tensorboardä¸­ï¼š

```python
# Keep track of gradient values and sparsity (optional)
grad_summaries = []
# g : gradient
# v : variable
for g, v in grads_and_vars:
    if g is not None:
        grad_hist_summary = tf.summary.histogram("{}/grad/hist".format(v.name), g)
        sparsity_summary = tf.summary.scalar("{}/grad/sparsity".format(v.name), tf.nn.zero_fraction(g))
        grad_summaries.append(grad_hist_summary)
        grad_summaries.append(sparsity_summary)
grad_summaries_merged = tf.summary.merge(grad_summaries)
```

>TensorBoardçš„ä½¿ç”¨ï¼š[ã€TensorflowåŸºç¡€ã€‘ç¬¬å…­è¯¾ï¼šTensorBoardçš„ä½¿ç”¨](http://shichaoxin.com/2020/07/29/TensorflowåŸºç¡€-ç¬¬å…­è¯¾-TensorBoardçš„ä½¿ç”¨/)ã€‚

`tf.nn.zero_fraction`çš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„tensorä¸­0å…ƒç´ åœ¨æ‰€æœ‰å…ƒç´ ä¸­æ‰€å çš„æ¯”ä¾‹è®¡ç®—å¹¶è¿”å›ï¼Œå³è¿”å›è¾“å…¥tensorçš„0å…ƒç´ çš„ä¸ªæ•°ä¸è¾“å…¥tensorçš„æ‰€æœ‰å…ƒç´ çš„ä¸ªæ•°çš„æ¯”å€¼ã€‚

ğŸ‘‰å®šä¹‰è¾“å‡ºè·¯å¾„ï¼š

```python
# Output directory for models and summaries
timestamp = str(int(time.time()))
out_dir = os.path.abspath(os.path.join(os.path.curdir, "runs", timestamp))
```

`os.path.abspath`ç”¨äºè·å–æŒ‡å®šæ–‡ä»¶æˆ–ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚`os.path.curdir`è¿”å›'.'ï¼Œè¡¨ç¤ºå½“å‰è·¯å¾„ã€‚

ğŸ‘‰æ·»åŠ æ›´å¤šä¿¡æ¯åˆ°summaryï¼š

```python
# Summaries for loss and accuracy
loss_summary = tf.summary.scalar("loss", cnn.loss)
acc_summary = tf.summary.scalar("accuracy", cnn.accuracy)

# Train Summaries
train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])
train_summary_dir = os.path.join(out_dir, "summaries", "train")
train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

# Dev summaries
dev_summary_op = tf.summary.merge([loss_summary, acc_summary])
dev_summary_dir = os.path.join(out_dir, "summaries", "dev")
dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)
```

ğŸ‘‰æ¨¡å‹ä¿å­˜å’Œæˆ‘ä»¬æ„å»ºçš„å­—å…¸ï¼š

```python
# Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
checkpoint_prefix = os.path.join(checkpoint_dir, "model")
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)
saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)

# Write vocabulary
vocab_processor.save(os.path.join(out_dir, "vocab"))
```

ğŸ‘‰åˆå§‹åŒ–Variableï¼š

```python
# Initialize all variables
sess.run(tf.global_variables_initializer())
```

ğŸ‘‰å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ­¥éª¤ï¼š

```python
def train_step(x_batch, y_batch):
    """
    A single training step
    """
    feed_dict = {
        cnn.input_x: x_batch,
        cnn.input_y: y_batch,
        cnn.dropout_keep_prob: FLAGS.dropout_keep_prob
    }
    _, step, summaries, loss, accuracy = sess.run(
        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],
        feed_dict)
    time_str = datetime.datetime.now().isoformat()
    print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))
    train_summary_writer.add_summary(summaries, step)

def dev_step(x_batch, y_batch, writer=None):
    """
    Evaluates model on a dev set
    """
    feed_dict = {
        cnn.input_x: x_batch,
        cnn.input_y: y_batch,
        cnn.dropout_keep_prob: 1.0
    }
    step, summaries, loss, accuracy = sess.run(
        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],
        feed_dict)
    time_str = datetime.datetime.now().isoformat()
    print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))
    if writer:
        writer.add_summary(summaries, step)
```

`datetime.datetime.now().isoformat()`ï¼š

```python
import datetime
datetime.datetime.now() #datetime.datetime(2022, 5, 24, 21, 13, 0, 907223)
datetime.datetime.now().isoformat() #è¿”å›å­—ç¬¦ä¸²ï¼š'2022-05-24T21:13:11.881698'
```

ğŸ‘‰äº§ç”Ÿbatchï¼š

```python
batches = data_helpers.batch_iter(
    list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)
```

`data_helpers.batch_iter`çš„å®šä¹‰å¦‚ä¸‹ï¼š

```python
def batch_iter(data, batch_size, num_epochs, shuffle=True):
    """
    Generates a batch iterator for a dataset.
    """
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1
    print("num_batches_per_epoch:",num_batches_per_epoch)
    for epoch in range(num_epochs):
        # Shuffle the data at each epoch
        if shuffle:
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_index:end_index]
```

`yield`çš„ç”¨æ³•è§æœ¬æ–‡ç¬¬3éƒ¨åˆ†ã€‚

ğŸ‘‰è®­ç»ƒéƒ¨åˆ†ä»£ç ï¼š

```python
# Training loop. For each batch...
for batch in batches:
    x_batch, y_batch = zip(*batch)
    train_step(x_batch, y_batch)
    current_step = tf.train.global_step(sess, global_step)
    if current_step % FLAGS.evaluate_every == 0:
        print("\nEvaluation:")
        dev_step(x_dev, y_dev, writer=dev_summary_writer)
        print("")
    if current_step % FLAGS.checkpoint_every == 0:
        path = saver.save(sess, checkpoint_prefix, global_step=current_step)
        print("Saved model checkpoint to {}\n".format(path))
```

`tf.train.global_step`ï¼ˆç›¸å½“äºbatchï¼‰æ¯æ‰§è¡Œä¸€æ¬¡ï¼Œ`global_step`å°±ä¼šåŠ 1ã€‚

# 3.`yield`

é¦–å…ˆä»‹ç»ä¸€ä¸‹**ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰**ï¼Œå…¶æä¾›ä¸€ç§å¯ä»¥è¾¹å¾ªç¯è¾¹è®¡ç®—çš„æœºåˆ¶ã€‚ç”Ÿæˆå™¨æ˜¯è§£å†³ä½¿ç”¨åºåˆ—å­˜å‚¨å¤§é‡æ•°æ®æ—¶ï¼Œå†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®å­˜å‚¨æ•°æ®çš„æŸäº›è§„å¾‹ï¼Œæ¼”ç®—ä¸ºç®—æ³•ï¼Œåœ¨å¾ªç¯è¿‡ç¨‹ä¸­é€šè¿‡è®¡ç®—å¾—åˆ°ï¼Œè¿™æ ·å¯ä»¥ä¸ç”¨åˆ›å»ºå®Œæ•´åºåˆ—ï¼Œä»è€Œå¤§å¤§èŠ‚çœå ç”¨ç©ºé—´ã€‚`yield`æ˜¯å®ç°ç”Ÿæˆå™¨æ–¹æ³•ä¹‹ä¸€ï¼Œå½“å‡½æ•°ä½¿ç”¨`yield`æ–¹æ³•ï¼Œåˆ™è¯¥å‡½æ•°å°±æˆä¸ºäº†ä¸€ä¸ªç”Ÿæˆå™¨ã€‚è°ƒç”¨è¯¥å‡½æ•°ï¼Œå°±ç­‰äºåˆ›å»ºäº†ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ã€‚æ¥ä¸‹æ¥é€šè¿‡å‡ ä¸ªä¾‹å­æ¥è¿›ä¸€æ­¥äº†è§£`yield`ã€‚

```python
def foo():
    print("starting...")
    while True:
        res = yield 4
        print("res:",res)
g = foo()
print(next(g))
print("*"*20)
print(next(g))
```

è¾“å‡ºä¸ºï¼š

```
starting...
4
********************
res: None
4
```

ä»£ç æ‰§è¡Œé¡ºåºè§£é‡Šï¼š

1. ç¨‹åºå¼€å§‹æ‰§è¡Œä»¥åï¼Œå› ä¸º`foo`å‡½æ•°ä¸­æœ‰`yield`å…³é”®å­—ï¼Œæ‰€ä»¥`foo`å‡½æ•°å¹¶ä¸ä¼šçœŸçš„æ‰§è¡Œï¼Œè€Œæ˜¯å…ˆå¾—åˆ°ä¸€ä¸ªç”Ÿæˆå™¨`g`ï¼ˆç›¸å½“äºä¸€ä¸ªå¯¹è±¡ï¼‰ã€‚
2. ç›´åˆ°è°ƒç”¨`next`æ–¹æ³•ï¼Œ`foo`å‡½æ•°æ­£å¼å¼€å§‹æ‰§è¡Œï¼Œå…ˆæ‰§è¡Œ`foo`å‡½æ•°ä¸­çš„`print`æ–¹æ³•ï¼Œç„¶åè¿›å…¥`while`å¾ªç¯ã€‚
3. ç¨‹åºé‡åˆ°`yield`å…³é”®å­—ï¼Œç„¶åæŠŠ`yield`æƒ³è±¡æˆreturnï¼Œreturnäº†ä¸€ä¸ª4ä¹‹åï¼Œç¨‹åºåœæ­¢ï¼Œå¹¶æ²¡æœ‰æ‰§è¡Œèµ‹å€¼ç»™`res`æ“ä½œï¼Œæ­¤æ—¶`next(g)`è¯­å¥æ‰§è¡Œå®Œæˆï¼Œæ‰€ä»¥è¾“å‡ºçš„å‰ä¸¤è¡Œæ˜¯æ‰§è¡Œ`print(next(g))`çš„ç»“æœã€‚
4. ç¨‹åºæ‰§è¡Œ`print("*"*20)`ã€‚
5. å¼€å§‹æ‰§è¡Œä¸‹é¢çš„`print(next(g))`ï¼Œè¿™ä¸ªæ—¶å€™å’Œä¸Šé¢é‚£ä¸ªå·®ä¸å¤šï¼Œä¸è¿‡ä¸åŒçš„æ˜¯ï¼Œè¿™ä¸ªæ—¶å€™æ˜¯ä»åˆšæ‰é‚£ä¸ª`next`ç¨‹åºåœæ­¢çš„åœ°æ–¹å¼€å§‹æ‰§è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯è¦æ‰§è¡Œ`res`çš„èµ‹å€¼æ“ä½œï¼Œè¿™æ—¶å€™è¦æ³¨æ„ï¼Œè¿™ä¸ªæ—¶å€™èµ‹å€¼æ“ä½œçš„å³è¾¹æ˜¯æ²¡æœ‰å€¼çš„ï¼ˆå› ä¸ºåˆšæ‰é‚£ä¸ªæ˜¯returnå‡ºå»äº†ï¼Œå¹¶æ²¡æœ‰ç»™èµ‹å€¼æ“ä½œçš„å·¦è¾¹ä¼ å‚æ•°ï¼‰ï¼Œæ‰€ä»¥è¿™ä¸ªæ—¶å€™`res`èµ‹å€¼æ˜¯Noneï¼Œæ‰€ä»¥æ¥ç€ä¸‹é¢çš„è¾“å‡ºå°±æ˜¯`res: None`ã€‚
6. ç¨‹åºä¼šç»§ç»­åœ¨`while`é‡Œæ‰§è¡Œï¼Œåˆä¸€æ¬¡ç¢°åˆ°`yield`ï¼Œè¿™ä¸ªæ—¶å€™åŒæ ·returnå‡º4ï¼Œç„¶åç¨‹åºåœæ­¢ï¼Œ`print`å‡½æ•°è¾“å‡ºçš„4å°±æ˜¯è¿™æ¬¡returnå‡ºçš„4ã€‚

å†çœ‹å¦å¤–ä¸€ä¸ªä¾‹å­ï¼š

```python
def foo():
    print("starting...")
    while True:
        res = yield 4
        print("res:",res)
g = foo()
print(next(g))
print("*"*20)
print(g.send(7))
```

è¾“å‡ºä¸ºï¼š

```
starting...
4
********************
res: 7
4
```

å‰4æ­¥å’Œä¸Šä¸€ä¸ªä¾‹å­æ˜¯ä¸€æ ·çš„ã€‚ç¬¬5æ­¥ï¼šç¨‹åºæ‰§è¡Œ`g.send(7)`ï¼Œç¨‹åºä¼šä»`yield`å…³é”®å­—é‚£ä¸€è¡Œç»§ç»­å‘ä¸‹è¿è¡Œï¼Œ`send`ä¼šæŠŠ7è¿™ä¸ªå€¼èµ‹ç»™`res`å˜é‡ã€‚ç¬¬6æ­¥ï¼šç”±äº`send`æ–¹æ³•ä¸­åŒ…å«`next()`æ–¹æ³•ï¼Œæ‰€ä»¥ç¨‹åºä¼šç»§ç»­å‘ä¸‹è¿è¡Œ`print`ï¼Œç„¶åå†æ¬¡è¿›å…¥`while`å¾ªç¯ã€‚ç¬¬7æ­¥ï¼šç¨‹åºæ‰§è¡Œå†æ¬¡é‡åˆ°`yield`å…³é”®å­—ï¼Œ`yield`ä¼šè¿”å›åé¢çš„å€¼ï¼Œç„¶åç¨‹åºå†æ¬¡æš‚åœï¼Œç›´åˆ°å†æ¬¡è°ƒç”¨`next`æ–¹æ³•æˆ–`send`æ–¹æ³•ã€‚

æœ€åé€šè¿‡ä¸€ä¸ªä¾‹å­è§£é‡Šä¸‹ä½¿ç”¨ç”Ÿæˆå™¨çš„ä¸€ä¸ªåŸå› ã€‚ä¾‹å¦‚ï¼š

```python
for n in range(1000):
	print(n)
```

æ­¤æ—¶ï¼Œ`range(1000)`é»˜è®¤ç”Ÿæˆä¸€ä¸ªå«æœ‰1000ä¸ªæ•°çš„listï¼Œæ‰€ä»¥ä¼šå¾ˆå å†…å­˜ã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨`yield`ï¼š

```python
def foo(num):
    print("starting...")
    while num<1000:
        num=num+1
        yield num
for n in foo(0):
    print(n)
```

æ­¤æ—¶ï¼Œ`foo(0)`ä¼šä¸€ä¸ªæ•°ä¸€ä¸ªæ•°çš„è¿”å›ï¼ŒèŠ‚çœäº†å†…å­˜ï¼ˆä¸ªäººæ„Ÿè§‰æœ‰ç‚¹åƒC++ä¸­çš„[static](http://shichaoxin.com/2021/12/04/C++åŸºç¡€-ç¬¬ä¸‰åå››è¯¾-å‡½æ•°åŸºç¡€/#22å±€éƒ¨é™æ€å¯¹è±¡)ï¼‰ã€‚

# 4.ä»£ç åœ°å€

1. [CNNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨](https://github.com/x-jeff/Tensorflow_Code_Demo/tree/master/Demo13)

# 5.å‚è€ƒèµ„æ–™

1. [tf.app.flags()å’Œtf.flags()çš„ç”¨æ³•åŠåŒºåˆ«](https://blog.csdn.net/pearl8899/article/details/108061781)
2. [Tensorflowä½¿ç”¨flagså®šä¹‰å‘½ä»¤è¡Œå‚æ•°è¯¦è§£](https://blog.csdn.net/qq_36653505/article/details/81124533)
3. [Python3 æ­£åˆ™è¡¨è¾¾å¼ï¼ˆèœé¸Ÿæ•™ç¨‹ï¼‰](https://www.runoob.com/python3/python3-reg-expressions.html#flags)
4. [ä»¥ç»ˆä¸ºå§‹ï¼šcompute\_gradients å’Œ apply\_gradients](https://zhuanlan.zhihu.com/p/343628982)
5. [Tensorflow-tf.nn.zero_fraction()è¯¦è§£](https://blog.csdn.net/fegang2002/article/details/83539768)
6. [pythonä¸­yieldçš„ç”¨æ³•è¯¦è§£â€”â€”æœ€ç®€å•ï¼Œæœ€æ¸…æ™°çš„è§£é‡Š](https://blog.csdn.net/mieleizhi0522/article/details/82142856/)
7. [Python ç”Ÿæˆå™¨yield](https://baijiahao.baidu.com/s?id=1725787762423255527&wfr=spider&for=pc)