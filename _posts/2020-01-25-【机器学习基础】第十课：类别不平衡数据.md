---
layout:     post
title:      【机器学习基础】第十课：类别不平衡数据
subtitle:   不平衡数据，阈值移动，欠采样，过采样
date:       2020-01-25
author:     x-jeff
header-img: blogimg/20200125.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.类别不平衡问题的困扰

⚠️几乎大部分的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。

如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。例如有998个反例，但正例只有2个，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度；然而这样的学习器往往没有价值，因为它不能预测出任何正例。

# 2.类别不平衡问题的解决方法

**类别不平衡**就是指分类任务中不同类别的训练样例数目差别很大的情况。

在现实的分类学习任务中，我们经常会遇到类别不平衡，例如在通过拆分法解决多分类问题时，即使原始问题中不同类别的训练样例数目相当，在使用OvR、MvM策略后产生的二分类任务仍可能出现类别不平衡现象。

>但是对于OvR、MvM来说，由于对每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需专门处理。

## 2.1.阈值移动(threshold-moving)

对于类别平衡的二分类问题，假设新的测试样本预测为正例的概率为y，当y>0.5时判别为正例，否则为反例，即：

若$\frac{y}{1-y}>1$，则预测为正例。

其中，几率$\frac{y}{1-y}$则反映了正例可能性与反例可能性之比值。

类似的，当训练集中正、反例的数目不同时，令$m^+$表示正例数目，$m^-$表示反例数目。假设训练集是真实样本总体的**无偏采样**，则有：

若$\frac{y}{1-y}>\frac{m^+}{m^-}$，则预测为正例。

>⚠️**无偏采样**意味着真实样本总体的类别比例在训练集中得以保持。

这种解决策略也称为**再缩放(rescaling)**或者**再平衡(rebalance)**。

## 2.2.欠采样(undersampling)

**欠采样**也称**下采样(downsampling)**，即去除一些多数类别中的样本使得两个类别数目接近，然后再进行学习。

‼️欠采样算法如果随机丢弃多数类别中的样本，可能会丢失一些重要信息。

👉代表性算法：**EasyEnsemble**。利用集成学习机制，将多数类别中的样本划分为若干个集合供不同学习器使用。

## 2.3.过采样(oversampling)

**过采样**也称**上采样(upsampling)**，即增加一些少数类别中的样本使得两个类别数目接近，然后再进行学习。

‼️过采样算法不能简单地对初始少数类别中的样本进行重复采样，否则会造成严重的过拟合。

👉代表性算法：**SMOTE**。通过对训练集少数类别中的样本进行插值来产生额外的样本。