---
layout:     post
title:      【深度学习基础】第二十课：学习率衰减
subtitle:   什么是学习率衰减，为什么要进行学习率衰减，怎么进行学习率衰减
date:       2020-03-23
author:     x-jeff
header-img: blogimg/20200323.jpg
catalog: true
tags:
    - Deep Learning Series
---
>【深度学习基础】系列博客为学习Coursera上吴恩达深度学习课程所做的课程笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.学习率衰减

加快学习算法的一个办法就是随时间慢慢减小学习率，我们称之为**学习率衰减**。

# 2.为什么要使用学习率衰减

我们通过一个例子来解释。

如果我们不使用学习率衰减，那么算法会在最小值附近大幅度的摆动，而不能收敛。如下图蓝线所示：

![](https://github.com/x-jeff/BlogImage/raw/master/DeepLearningSeries/Lesson20/20x1.png)

如果我们使用了学习率衰减，算法则会在最小值附近的一小块区域内摆动。如下图绿线所示：

![](https://github.com/x-jeff/BlogImage/raw/master/DeepLearningSeries/Lesson20/20x2.png)

# 3.学习率衰减的方式

学习率衰减有多种方式。

## 3.1.方式一

$$\alpha = \frac{1}{1+decay-rate * epoch-num} \alpha_0$$

其中，

* `decay-rate`为衰减率。
* `epoch-num`为已经进行epoch的次数。
* $\alpha_0$为初始学习率。
* $\alpha$为更新后的学习率。

例如，$\alpha_0=0.2,decay-rate=1$时：

|Epoch|$\alpha$|
|:-:|:-:|
|1|0.1|
|2|0.067|
|3|0.05|
|4|0.04|
|...|...|

使用时涉及调整两个超参数`decay-rate`和$\alpha_0$的值。

## 3.2.方式二

指数衰减：

$$\alpha=0.95^{epoch-num} \cdot \alpha_0$$

符号解释同3.1部分。学习率呈指数下降。

## 3.3.方式三

$$\alpha=\frac{k}{\sqrt{epoch-num}} \cdot \alpha_0$$

其中，k为一个常数，其余符号解释同3.1部分。此方式又增加了一个超参数k。

该方式有时也会写为：

$$\alpha=\frac{k}{\sqrt{mini-batch-num}}\cdot \alpha_0$$

其中，`mini-batch-num`为mini-batch的数量。

## 3.4.方式四

离散下降的学习率，也就是各个步骤有其自己特定的学习率。学习率呈离散下降：

![](https://github.com/x-jeff/BlogImage/raw/master/DeepLearningSeries/Lesson20/20x3.png)

## 3.5.方式五

除了上述方式外，也可以手动衰减，即人为的调整学习率。