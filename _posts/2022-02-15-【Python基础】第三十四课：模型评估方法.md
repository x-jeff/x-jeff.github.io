---
layout:     post
title:      ã€PythonåŸºç¡€ã€‘ç¬¬ä¸‰åå››è¯¾ï¼šæ¨¡å‹è¯„ä¼°æ–¹æ³•
subtitle:   ç•™å‡ºæ³•ï¼Œtrain_test_splitï¼Œäº¤å‰éªŒè¯æ³•ï¼ŒKFoldï¼Œcross_val_scoreï¼Œç•™ä¸€æ³•ï¼ŒLeaveOneOut
date:       2022-02-15
author:     x-jeff
header-img: blogimg/20220215.jpg
catalog: true
tags:
    - Python Series
---
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.ç•™å‡ºæ³•

>â€œç•™å‡ºæ³•â€è¯¦è§£è¯·è§ï¼š[é“¾æ¥](http://shichaoxin.com/2018/11/27/æœºå™¨å­¦ä¹ åŸºç¡€-ç¬¬äºŒè¯¾-æ¨¡å‹è¯„ä¼°æ–¹æ³•/#21ç•™å‡ºæ³•)ã€‚

ğŸ‘‰å¼•ç”¨æ•°æ®ä¸å»ºç«‹æ¨¡å‹ï¼š

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data
y = iris.target
```

ğŸ‘‰å»ºç«‹è®­ç»ƒä¸æµ‹è¯•æ•°æ®é›†ï¼š

```python
from sklearn.model_selection import train_test_split

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=123)
clf = DecisionTreeClassifier()
clf.fit(train_X, train_y)
```

ğŸ‘‰äº§ç”Ÿå‡†ç¡®åº¦ï¼š

```python
from sklearn.metrics import accuracy_score

predicted = clf.predict(test_X)
accuracy_score(test_y, predicted)
```

ğŸ‘‰å»ºç«‹æ··æ·†çŸ©é˜µï¼š

```python
from sklearn.metrics import confusion_matrix

m = confusion_matrix(test_y, predicted)
```

# 2.äº¤å‰éªŒè¯æ³•

>â€œäº¤å‰éªŒè¯æ³•â€è¯¦è§£è¯·è§ï¼š[é“¾æ¥](http://shichaoxin.com/2018/11/27/æœºå™¨å­¦ä¹ åŸºç¡€-ç¬¬äºŒè¯¾-æ¨¡å‹è¯„ä¼°æ–¹æ³•/#22äº¤å‰éªŒè¯æ³•)ã€‚

```python
from sklearn.model_selection import KFold

kf = KFold(n_splits=10)
for train, test in kf.split(X):
    train_X, test_X, train_y, test_y = X[train], X[test], y[train], y[test]
    clf = DecisionTreeClassifier()
    clf.fit(train_X, train_y)
    predicted = clf.predict(test_X)
    print(accuracy_score(test_y, predicted))
```

```
1.0
1.0
1.0
0.9333333333333333
0.9333333333333333
0.8666666666666667
1.0
0.8666666666666667
0.8
1.0
```

å¦ä¸€ç§æ–¹æ³•ï¼š

```python
from sklearn.model_selection import cross_val_score

acc = cross_val_score(clf, X=iris.data, y=iris.target, cv=10)
print(acc)
print(acc.mean())
```

```
[1.         0.93333333 1.         0.93333333 0.93333333 0.86666667
 0.93333333 0.93333333 1.         1.        ]
0.9533333333333334
```

# 3.ç•™ä¸€æ³•

>â€œç•™ä¸€æ³•â€è¯¦è§£è¯·è§ï¼š[é“¾æ¥](http://shichaoxin.com/2018/11/27/æœºå™¨å­¦ä¹ åŸºç¡€-ç¬¬äºŒè¯¾-æ¨¡å‹è¯„ä¼°æ–¹æ³•/#22äº¤å‰éªŒè¯æ³•)ã€‚

```python
from sklearn.model_selection import LeaveOneOut

res = []
loo = LeaveOneOut()
for train, test in loo.split(X):
    train_X, test_X, train_y, test_y = X[train], X[test], y[train], y[test]
    clf = DecisionTreeClassifier()
    clf.fit(train_X, train_y)
    predicted = clf.predict(test_X)
    res.extend((predicted == test_y).tolist())
print(sum(res)) #143
```

# 4.ä»£ç åœ°å€

1. [æ¨¡å‹è¯„ä¼°æ–¹æ³•](https://github.com/x-jeff/Python_Code_Demo/tree/master/Demo34)