---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘BERTï¼šPre-training of Deep Bidirectional Transformers for Language Understanding
subtitle:   BERT
date:       2024-08-12
author:     x-jeff
header-img: blogimg/20210721.jpg
catalog: true
tags:
    - Natural Language Processing
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒå·²ç»è¢«è¯å®å¯¹æå‡è®¸å¤šNLPä»»åŠ¡æ˜¯æœ‰æ•ˆçš„ã€‚

å°†é¢„è®­ç»ƒè¯­è¨€è¡¨å¾åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡é€šå¸¸æœ‰ä¸¤ç§ç­–ç•¥ï¼šfeature-basedå’Œfine-tuningã€‚feature-basedç­–ç•¥çš„ä»£è¡¨æ–¹æ³•æ˜¯ELMoï¼Œå…¶é’ˆå¯¹æ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œæ„é€ ä¸€ä¸ªä¸è¿™ä¸ªä»»åŠ¡ç›¸å…³çš„ç¥ç»ç½‘ç»œï¼ˆå®é™…ä½¿ç”¨çš„æ˜¯[RNNæ¡†æ¶](http://shichaoxin.com/2020/11/22/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬å››åè¯¾-å¾ªç¯ç¥ç»ç½‘ç»œ/)ï¼‰ï¼Œç„¶åå°†é¢„è®­ç»ƒå¥½çš„è¡¨å¾ä½œä¸ºé¢å¤–çš„ç‰¹å¾å’ŒåŸæœ‰è¾“å…¥ä¸€èµ·å–‚ç»™æ¨¡å‹ã€‚fine-tuningç­–ç•¥çš„ä»£è¡¨æ–¹æ³•æ˜¯[GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)ï¼Œå…¶å°†é¢„è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œä¸”ä¸éœ€è¦åšè¿‡å¤šçš„ä¿®æ”¹ï¼Œé¢„è®­ç»ƒå¥½çš„å‚æ•°ä¼šåœ¨ä¸‹æ¸¸æ•°æ®ä¸Šè¿›è¡Œfine tuneã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µéƒ½æ˜¯ä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å‡½æ•°ï¼Œä¸”éƒ½æ˜¯å•å‘çš„è¯­è¨€æ¨¡å‹ï¼ˆä¸ªäººæ³¨è§£ï¼šè¯­è¨€æ¨¡å‹é€šå¸¸å°±æ˜¯å•å‘çš„ï¼Œæ¯”å¦‚ç»™å®šä¸€äº›è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼‰ã€‚

>ELMoï¼šMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.
>
>ä¸ªäººæ³¨è§£ï¼šBERTå’ŒELMoéƒ½æ˜¯åŠ¨ç”»ç‰‡èŠéº»è¡—é‡Œçš„äººç‰©åï¼Œè¿™ä¹Ÿå¼€å¯äº†NLPèŠéº»è¡—ç³»åˆ—ï¼Œåç­‰åç»­ä¼šä¸ä¼šæœ‰æ–°çš„èŠéº»è¡—äººç‰©å‡ºç°ğŸ˜‚ã€‚
>
>![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/1.png)

æˆ‘ä»¬è®¤ä¸ºå½“å‰çš„æŠ€æœ¯é™åˆ¶äº†é¢„è®­ç»ƒè¡¨å¾çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºfine-tuningçš„æ–¹æ³•ã€‚ä¸»è¦çš„å±€é™æ€§åœ¨äºæ ‡å‡†çš„è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ï¼Œè¿™é™åˆ¶äº†åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹äºå¯ä½¿ç”¨æ¡†æ¶çš„é€‰æ‹©ã€‚æ¯”å¦‚ï¼Œåœ¨[OpenAIçš„GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ç§ä»å·¦åˆ°å³çš„æ¡†æ¶ï¼Œå³æ¯ä¸ªtokenåªèƒ½å…³æ³¨åˆ°å‰é¢çš„tokenã€‚ä½†è¿™å¯¹äºsentence-levelçš„ä»»åŠ¡æ¥è¯´æ˜¯æ¬¡ä¼˜çš„ï¼Œæ¯”å¦‚æ ¹æ®ä¸€ä¸ªå¥å­åˆ¤æ–­æƒ…ç»ªï¼Œæ— è®ºæ˜¯ä»å·¦åˆ°å³åˆ†æè¿™ä¸ªå¥å­ï¼Œè¿˜æ˜¯ä»å³åˆ°å·¦åˆ†æè¿™ä¸ªå¥å­ï¼Œå¾—åˆ°çš„ç»“æœåº”è¯¥éƒ½æ˜¯ä¸€æ ·çš„ã€‚ç”šè‡³å¯¹äºä¸€äº›token-levelçš„ä»»åŠ¡ä¹Ÿä¸æ˜¯æœ€ä¼˜çš„ï¼Œæ¯”å¦‚Q&Aä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å®Œæ•´ä¸ªé—®é¢˜å†å»é€‰ç­”æ¡ˆï¼Œå¹¶ä¸éœ€è¦ä¸€ä¸ªæ¥ä¸€ä¸ªçš„é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºå¦‚æœæŠŠä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯éƒ½æ”¾è¿›æ¥çš„è¯ï¼Œæ˜¯å¯ä»¥æå‡è¿™äº›ä»»åŠ¡çš„æ€§èƒ½çš„ã€‚

æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®Œå–„äº†åŸºäºfine-tuningçš„æ–¹æ³•ï¼Œæå‡ºäº†BERTï¼š**B**idirectional **E**ncoder **R**epresentations from **T**ransformersã€‚BERTé€šè¿‡ä½¿ç”¨ä¸€ä¸ªå¸¦æ©ç çš„è¯­è¨€æ¨¡å‹ï¼ˆmasked language modelï¼ŒMLMï¼‰ç¼“è§£äº†è¯­è¨€æ¨¡å‹çš„å•å‘é™åˆ¶ã€‚MLMéšæœºå±è”½è¾“å…¥ä¸­çš„ä¸€äº›tokenï¼Œç›®çš„æ˜¯ä»…æ ¹æ®å…¶ä¸Šä¸‹æ–‡é¢„æµ‹å‡ºè¢«å±è”½çš„tokenï¼ˆä¸ªäººæ³¨è§£ï¼šç±»ä¼¼å®Œå½¢å¡«ç©ºï¼‰ã€‚ä¸ä»å·¦åˆ°å³è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸åŒï¼ŒMLMèƒ½å¤Ÿèåˆå·¦å³ä¸Šä¸‹æ–‡ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥é¢„è®­ç»ƒä¸€ä¸ªæ·±çš„åŒå‘[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)ã€‚é™¤äº†MLMå¤–ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†å¦å¤–ä¸€ä¸ªä»»åŠ¡ï¼Œå«åšâ€œä¸‹ä¸€ä¸ªå¥å­çš„é¢„æµ‹â€ï¼ˆâ€œnext sentence predictionâ€ï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç»™å®šä¸¤ä¸ªå¥å­ï¼Œè®©æ¨¡å‹å»åˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­åœ¨åŸæ–‡ä¸­æ˜¯ä¸æ˜¯ç›¸é‚»çš„ï¼Œè¿™èƒ½ä½¿æ¨¡å‹å­¦ä¹ åˆ°ä¸€äº›å¥å­å±‚é¢çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦æœ‰ä»¥ä¸‹3ç‚¹ï¼š

1. æˆ‘ä»¬è¯æ˜äº†åŒå‘é¢„è®­ç»ƒå¯¹è¯­è¨€è¡¨å¾çš„é‡è¦æ€§ã€‚
2. æˆ‘ä»¬è¯æ˜äº†ï¼Œä¸€ä¸ªå¥½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°±ä¸éœ€è¦å†å¯¹ç‰¹å®šçš„ä»»åŠ¡åšä¸€äº›ç‰¹å®šçš„æ¨¡å‹æ”¹åŠ¨äº†ã€‚åœ¨åŸºäºfine-tuningçš„æ–¹æ³•ä¸­ï¼ŒBERTæ˜¯ç¬¬ä¸€ä¸ªåœ¨ä¸€ç³»åˆ—NLPä»»åŠ¡ï¼ˆåŒ…æ‹¬sentence-levelå’Œtoken-levelï¼‰ä¸Šè¾¾åˆ°SOTAçš„ã€‚
3. å¼€æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼š[https://github.com/google-research/bert](https://github.com/google-research/bert)ã€‚

>ä¸ªäººæ³¨è§£ï¼šä½œè€…ä¸»è¦ä»‹ç»äº†ä¸¤ä¸ªä¹‹å‰çš„ç ”ç©¶ï¼Œä¸€ä¸ªæ˜¯ELMoï¼Œå…¶æ˜¯åŒå‘+[RNNæ¡†æ¶](http://shichaoxin.com/2020/11/22/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬å››åè¯¾-å¾ªç¯ç¥ç»ç½‘ç»œ/)ï¼Œå¦ä¸€ä¸ªæ˜¯[GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)ï¼Œå…¶æ˜¯å•å‘+[Transformeræ¡†æ¶](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)ã€‚è€ŒBERTå°±æ˜¯ç»“åˆäº†ä¸Šè¿°ä¸¤ç§æ€æƒ³ï¼Œæ˜¯åŒå‘+[Transformeræ¡†æ¶](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)ã€‚

# 2.Related Work

ä¸å†è¯¦è¿°ã€‚

# 3.BERT

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/2.png)

BERTæœ‰ä¸¤ä¸ªæ­¥éª¤ï¼šé¢„è®­ç»ƒå’Œfine-tuningã€‚é¢„è®­ç»ƒæ˜¯åœ¨ä¸€ä¸ªæ²¡æœ‰æ ‡ç­¾çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚fine-tuningæ˜¯åœ¨ä¸‹æ¸¸æœ‰æ ‡ç­¾çš„æ•°æ®ä¸Šè¿›è¡Œçš„ã€‚æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡éƒ½æœ‰ç‰¹å®šçš„fine-tunedæ¨¡å‹ï¼Œå³ä½¿å®ƒä»¬éƒ½æ˜¯ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå‚æ•°åˆå§‹åŒ–çš„ã€‚

BERTçš„ä¸€ä¸ªæ˜¾è‘—ç‰¹å¾å°±æ˜¯ä¸åŒä»»åŠ¡ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹æ¡†æ¶ã€‚é¢„è®­ç»ƒæ¨¡å‹æ¡†æ¶å’Œæœ€ç»ˆä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹æ¡†æ¶ä¹‹é—´çš„å·®å¼‚å¾ˆå°ã€‚

ğŸ‘‰**Model Architecture**

BERTçš„æ¨¡å‹æ¡†æ¶æ˜¯ä¸€ä¸ªå¤šå±‚åŒå‘Transformerç¼–ç å™¨ã€‚

æˆ‘ä»¬å°†å±‚æ•°ï¼ˆå³Transformer blocksï¼‰è®°ä¸º$L$ï¼Œhidden sizeï¼ˆå³éšè—å±‚å¤§å°ï¼‰è®°ä¸º$H$ï¼Œè‡ªæ³¨æ„åŠ›å¤´çš„æ•°é‡è®°ä¸º$A$ã€‚æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼š$\text{BERT}_{\text{BASE}}$ï¼ˆ$L=12,H=768,A=12$ï¼Œæ€»å‚æ•°é‡ä¸º110Mï¼‰å’Œ$\text{BERT}_{\text{LARGE}}$ï¼ˆ$L=24,H=1024,A=16$ï¼Œæ€»å‚æ•°é‡ä¸º340Mï¼‰ã€‚

$\text{BERT}_{\text{BASE}}$å’Œ[GPT1](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/#1gpt1)çš„å‚æ•°é‡ç›¸å½“ã€‚

ğŸ‘‰**Input/Output Representations**

ä¸ºäº†ä½¿BERTå¯ä»¥å¤„ç†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒBERTçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œå…¶å³å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå¥å­å¯¹ï¼ˆæ¯”å¦‚`<Question,Answer>`ï¼‰ã€‚è¿™é‡Œçš„å¥å­æŒ‡çš„æ˜¯ä¸€æ®µè¿ç»­çš„æ–‡å­—ï¼Œå¹¶ä¸ä¸€å®šçœŸçš„åªæ˜¯ä¸€å¥è¯ã€‚

æˆ‘ä»¬ç”¨çš„åˆ‡è¯æ–¹æ³•æ˜¯WordPieceã€‚å‡è®¾æˆ‘ä»¬æŒ‰ç…§ç©ºæ ¼åˆ‡è¯çš„è¯ï¼Œä¸€ä¸ªè¯ä½œä¸ºä¸€ä¸ªtokenï¼Œæˆ‘ä»¬çš„æ•°æ®é‡æ¯”è¾ƒå¤§ï¼Œä»è€Œå¯¼è‡´è¯å…¸å¤§å°ä¹Ÿç‰¹åˆ«å¤§ï¼Œå¯èƒ½ä¼šè¾¾åˆ°ç™¾ä¸‡çº§åˆ«ï¼Œå› æ­¤ï¼ŒæŒ‰ç…§WordPieceçš„å¤„ç†æ–¹æ³•ï¼Œå¦‚æœä¸€ä¸ªè¯å‡ºç°çš„æ¦‚ç‡ä¸å¤§çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒåˆ‡å¼€ï¼Œçœ‹å®ƒçš„ä¸€ä¸ªå­åºåˆ—ï¼ˆå¯èƒ½æ˜¯ä¸€ä¸ªè¯æ ¹ï¼‰ï¼Œè‹¥å­åºåˆ—å‡ºç°çš„æ¦‚ç‡æ¯”è¾ƒå¤§çš„è¯ï¼Œæˆ‘ä»¬å°±åªä¿ç•™è¿™ä¸ªå­åºåˆ—å°±å¯ä»¥äº†ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥æŠŠä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒé•¿çš„è¯ï¼Œåˆ‡æˆå¤šä¸ªç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µæ˜¯ç»å¸¸å‡ºç°çš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”¨ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„è¯å…¸ï¼ˆæœ¬æ–‡ä¸­ï¼Œæ˜¯ä¸€ä¸ª30,000 tokençš„è¯å…¸ï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªè¾ƒå¤§çš„æ–‡æœ¬äº†ã€‚æ¯ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ªtokenæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„åˆ†ç±»tokenï¼š`[CLS]`ï¼ˆå³classificationï¼‰ã€‚`[CLS]`æœ€åå¯¹åº”çš„è¾“å‡ºä»£è¡¨çš„æ˜¯æ•´ä¸ªåºåˆ—çš„ä¸€ä¸ªä¿¡æ¯ã€‚å¦‚æœæ˜¯å¥å­å¯¹ä½œä¸ºä¸€ä¸ªåºåˆ—ï¼Œåˆ™éœ€è¦å¯¹è¿™ä¸¤ä¸ªå¥å­è¿›è¡ŒåŒºåˆ†ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•ã€‚ç¬¬ä¸€ä¸ªæ–¹æ³•æ˜¯åœ¨å¥å­åé¢æ”¾ä¸€ä¸ªç‰¹æ®Šçš„è¯ï¼š`[SEP]`ï¼ˆå³separateï¼‰ã€‚ç¬¬äºŒä¸ªæ–¹æ³•æ˜¯å­¦ä¹ ä¸€ä¸ªåµŒå…¥å±‚æ¥è¡¨ç¤ºè¿™ä¸ªå¥å­æ˜¯å¥å­$A$è¿˜æ˜¯å¥å­$B$ã€‚å¦‚Fig1æ‰€ç¤ºã€‚æˆ‘ä»¬å°†input embeddingè®°ä¸º$E$ï¼Œ`[CLS]`å¯¹åº”çš„æœ€ç»ˆéšè—å‘é‡ï¼ˆfinal hidden vectorï¼‰ä¸º$C \in \mathbb{R}^H$ï¼Œç¬¬$i$ä¸ªè¾“å…¥tokençš„æœ€ç»ˆéšè—å‘é‡ä¸º$T_i \in \mathbb{R}^H$ã€‚

>WordPieceï¼šYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Googleâ€™s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/3.png)

å¦‚Fig2æ‰€ç¤ºï¼Œå°†è¯è½¬åŒ–ä¸ºBERTçš„input embeddingåŒ…å«3éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯è¯æœ¬èº«çš„embeddingï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯è¯åœ¨å“ªä¸ªå¥å­çš„embeddingï¼Œç¬¬ä¸‰éƒ¨åˆ†æ˜¯ä½ç½®çš„embeddingï¼ˆåœ¨[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)ä¸­ï¼Œä½ç½®ä¿¡æ¯æ˜¯æ‰‹åŠ¨æ„é€ å‡ºæ¥çš„ä¸€ä¸ªçŸ©é˜µï¼Œè€Œåœ¨BERTä¸­ï¼Œç¬¬äºŒéƒ¨åˆ†å’Œç¬¬ä¸‰éƒ¨åˆ†éƒ½æ˜¯é€šè¿‡å­¦ä¹ å¾—æ¥çš„ï¼‰ã€‚

## 3.1.Pre-training BERT

BERTçš„é¢„è®­ç»ƒä½¿ç”¨äº†2ç§éç›‘ç£ä»»åŠ¡ã€‚

ğŸ‘‰**Task #1: Masked LM**

ä¸ºäº†è®­ç»ƒæ·±å±‚åŒå‘è¡¨å¾ï¼Œæˆ‘ä»¬éšæœºmaskæ‰äº†è¾“å…¥ä¸­çš„ä¸€äº›è¯ï¼Œç„¶åæ¥é¢„æµ‹è¿™äº›è¢«maskçš„è¯ã€‚æˆ‘ä»¬å°†è¿™ä¸€è¿‡ç¨‹ç§°ä¸ºâ€œmasked LMâ€ï¼ˆMLMï¼‰ã€‚åºåˆ—ä¸­çš„æ¯ä¸ªè¯ï¼ˆé™¤äº†ç‰¹æ®Šçš„tokenï¼Œæ¯”å¦‚`[CLS]`å’Œ`[SEP]`ï¼‰éƒ½æœ‰15%çš„æ¦‚ç‡è¢«maskæ‰ã€‚

è¢«maskæ‰çš„è¯ä¼šä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„tokenï¼š`[MASK]`æ¥ä»£æ›¿ã€‚è¿™ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨fine-tuneçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸ç”¨maskï¼Œæ‰€ä»¥å¯¹äºé¢„è®­ç»ƒå’Œfine-tuneï¼Œå–‚ç»™æ¨¡å‹çš„æ•°æ®å¯èƒ½ä¼šç¨ç¨æœ‰äº›ä¸ä¸€æ ·ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå¯¹äºè¿™15%è¢«é€‰ä¸­çš„è¯ï¼š1ï¼‰80%è¢«çœŸæ­£çš„maskæ‰ï¼Œæ›¿æ¢ä¸º`[MASK]`ï¼›2ï¼‰10%è¢«æ›¿æ¢ä¸ºä¸€ä¸ªéšæœºçš„tokenï¼›3ï¼‰10%ä¸åšä»»ä½•å˜åŒ–ã€‚ç›¸å…³çš„æ¶ˆèå®éªŒè§é™„å½•C.2ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/4.png)

ğŸ‘‰**Task #2: Next Sentence Prediction (NSP)**

å¾ˆå¤šé‡è¦çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ¯”å¦‚QAå’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œéƒ½æ˜¯åŸºäºç†è§£ä¸¤å¥è¯ä¹‹é—´çš„å…³ç³»ï¼Œè€Œè¯­è¨€å»ºæ¨¡å¹¶ä¸èƒ½ç›´æ¥æ•æ‰åˆ°è¿™ç§å…³ç³»ã€‚ä¸ºäº†è®­ç»ƒä¸€ä¸ªå¯ä»¥ç†è§£å¥å­å…³ç³»çš„æ¨¡å‹ï¼Œæˆ‘ä»¬é¢„è®­ç»ƒäº†ä¸€ä¸ªäºŒå€¼åŒ–çš„ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆnext sentence predictionï¼ŒNSPï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å¯ä»¥ä»ä»»ä½•è¯­æ–™åº“ä¸­è½»æ¾ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªé¢„è®­ç»ƒæ ·æœ¬ï¼Œæœ‰å¥å­$A$å’Œå¥å­$B$ï¼Œæœ‰50%çš„æ¦‚ç‡å¥å­$B$åœ¨åŸå§‹è¯­æ–™åº“ä¸­å°±æ˜¯è·Ÿåœ¨å¥å­$A$åé¢çš„ï¼ˆæ ‡ç­¾ä¸º`IsNext`ï¼‰ï¼Œè¿˜æœ‰50%çš„æ¦‚ç‡å¥å­$B$æ˜¯éšæœºé€‰çš„ï¼ŒåŸæœ¬å°±ä¸åœ¨å¥å­$A$åé¢ï¼ˆæ ‡ç­¾ä¸º`NotNext`ï¼‰ã€‚åœ¨Fig1ä¸­ï¼Œ$C$å°±æ˜¯ç”¨æ¥é¢„æµ‹è¿™ä¸ªæ ‡ç­¾çš„ã€‚å°½ç®¡è¿™ä¸ªç­–ç•¥å¾ˆç®€å•ï¼Œä½†å´æœ‰æ•ˆã€‚æœ€ç»ˆæ¨¡å‹åœ¨NSPä¸Šè¾¾åˆ°äº†97%-98%çš„å‡†ç¡®ç‡ã€‚å¦‚æœä¸è¿›è¡Œfine-tuneï¼Œå‘é‡$C$å°±ä¸æ˜¯ä¸€ä¸ªæœ‰æ„ä¹‰çš„å¥å­è¡¨å¾ï¼Œå› ä¸ºå®ƒæ˜¯ç”¨NSPè®­ç»ƒçš„ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/5.png)

ğŸ‘‰**Pre-training data**

é¢„è®­ç»ƒä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†ï¼šBooksCorpusï¼ˆåŒ…å«800Mä¸ªè¯ï¼‰å’ŒEnglish Wikipediaï¼ˆåŒ…å«2,500Mä¸ªè¯ï¼‰ã€‚

## 3.2.Fine-tuning BERT

è¿™é‡Œé€šè¿‡å‡ ä¸ªBERT fine-tuneçš„ä¾‹å­æ¥ç†è§£è¿™ä¸€è¿‡ç¨‹ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¬¬ä¸€ä¸ªä¾‹å­çš„ä¸‹æ¸¸ä»»åŠ¡æ˜¯å¥å­åˆ†ç±»ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«ã€‚æ¯”å¦‚è¾“å…¥å¥å­â€œThis is goodâ€ï¼Œè¾“å‡ºè¿™ä¸ªå¥å­æ‰€è¡¨è¾¾çš„æƒ…ç»ªæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/6.png)

ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºæ˜¯åŒæ ·é•¿åº¦çš„å¦å¤–ä¸€ä¸ªåºåˆ—ã€‚æ¯”å¦‚è¾“å…¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œè¾“å‡ºæ˜¯å¯¹å¥å­ä¸­æ¯ä¸ªè¯è¯æ€§çš„åˆ†ç±»ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/7.png)

ç¬¬ä¸‰ä¸ªä¾‹å­ä¸­ï¼Œè¾“å…¥æ˜¯ä¸¤ä¸ªå¥å­ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«ã€‚æ¯”å¦‚NLIï¼ˆNatural Language Inferenceï¼‰ä»»åŠ¡ï¼Œä¸»è¦ç”¨äºåˆ¤æ–­ä¸¤å¥è¯ä¹‹é—´çš„é€»è¾‘å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€å¯¹å¥å­ï¼Œç§°ä¸ºå‰æï¼ˆpremiseï¼‰å’Œå‡è®¾ï¼ˆhypothesisï¼‰ï¼ŒNLIçš„ä»»åŠ¡æ˜¯ç¡®å®šå‡è®¾ç›¸å¯¹äºå‰æçš„å…³ç³»ï¼Œå¯ä»¥æ˜¯ä»¥ä¸‹ä¸‰ç§ä¹‹ä¸€ï¼š1ï¼‰è•´å«ï¼ˆentailmentï¼‰ï¼šå‡è®¾èƒ½å¤Ÿä»å‰æä¸­æ¨å¯¼å‡ºæ¥ï¼›2ï¼‰çŸ›ç›¾ï¼ˆcontradictionï¼‰ï¼šå‡è®¾ä¸å‰æç›¸çŸ›ç›¾ï¼›3ï¼‰ä¸­ç«‹ï¼ˆneutralï¼‰ï¼šå‡è®¾ä¸å‰ææ—¢ä¸è•´å«ä¹Ÿä¸çŸ›ç›¾ï¼Œå¯èƒ½æä¾›äº†ä¸å‰ææ— å…³çš„æ–°ä¿¡æ¯ã€‚NLIä»»åŠ¡ç¤ºæ„ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/8.png)

fine-tuneç¤ºæ„ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/9.png)

ä¸é¢„è®­ç»ƒæ¯”ï¼Œfine-tuneç›¸å¯¹ä¾¿å®œä¸€äº›ã€‚æ‰€æœ‰çš„ç»“æœåªéœ€è¦ç”¨TPUè·‘ä¸€ä¸ªå°æ—¶ï¼Œæˆ–è€…ä½¿ç”¨GPUè·‘å‡ ä¸ªå°æ—¶ä¹Ÿå¯ä»¥ã€‚

# 4.Experiments

æœ¬éƒ¨åˆ†å±•ç¤ºäº†BERTåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šçš„fine-tuneç»“æœã€‚

## 4.1.GLUE

GLUEï¼ˆGeneral Language Understanding Evaluationï¼‰ benchmarkæ˜¯å¤šä¸ªNLPä»»åŠ¡çš„é›†åˆã€‚GLUEæ•°æ®é›†çš„è¯¦ç»†ä»‹ç»è§é™„å½•B.1ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/10.png)

## 4.2.SQuAD v1.1

SQuAD v1.1ï¼ˆStanford Question Answering Datasetï¼‰æ˜¯ä¸€ä¸ªQAæ•°æ®é›†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™é‡Œçš„ç­”æ¡ˆæ˜¯é—®é¢˜æ–‡æœ¬ä¸­çš„æŸä¸ªå­åºåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬åªè¦è¾“å‡ºè¿™ä¸ªå­åºåˆ—å¼€å§‹çš„åºå·så’Œç»“æŸçš„åºå·eå°±å¯ä»¥äº†ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/11.png)

BERT fine-tuneçš„è¿‡ç¨‹å¯è¡¨ç¤ºä¸ºï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/12.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/13.png)

fine-tuneåçš„ç»“æœï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/14.png)

## 4.3.SQuAD v2.0

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/15.png)

## 4.4.SWAG

SWAGï¼ˆSituations With Adversarial Generationsï¼‰æ•°æ®é›†åŒ…å«113kä¸ªå¥å­å¯¹ï¼Œè¯¥æ•°æ®é›†ç”¨äºåˆ¤æ–­å¥å­ä¹‹é—´çš„å…³ç³»ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/16.png)

# 5.Ablation Studies

## 5.1.Effect of Pre-training Tasks

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/17.png)

## 5.2.Effect of Model Size

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/18.png)

## 5.3.Feature-based Approach with BERT

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/19.png)

# 6.Conclusion

æœ€è¿‘ä¸€äº›å®éªŒè¡¨æ˜ä½¿ç”¨éç›‘ç£çš„é¢„è®­ç»ƒæ˜¯éå¸¸å¥½çš„ã€‚è¿™ä½¿å¾—è®­ç»ƒæ ·æœ¬ä¸å¤šçš„ä»»åŠ¡ä¹Ÿå¯ä»¥ä½¿ç”¨æ·±å±‚å•å‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å°†å…¶è¿›ä¸€æ­¥æ¨å¹¿åˆ°æ·±å±‚åŒå‘æ¡†æ¶ï¼Œä½¿åŒæ ·çš„é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„NLPä»»åŠ¡ã€‚

# 7.Appendix

ä¸»è¦åˆ†ä¸º3éƒ¨åˆ†ï¼š

* BERTçš„é¢å¤–å®ç°ç»†èŠ‚è§é™„å½•Aã€‚
* é¢å¤–çš„å®éªŒç»†èŠ‚è§é™„å½•Bã€‚
* é¢å¤–çš„æ¶ˆèå®éªŒè§é™„å½•Cã€‚

## 7.A.Additional Details for BERT

### 7.A.1.Illustration of the Pre-training Tasks

ğŸ‘‰**Masked LM and the Masking Procedure**

å‡è®¾æœ‰æœªæ ‡æ³¨çš„å¥å­â€œmy dog is hairyâ€ï¼Œæˆ‘ä»¬é€‰æ‹©ç¬¬4ä¸ªè¯hairyè¿›è¡Œmaskï¼Œåˆ™ï¼š

* æœ‰80%çš„æ¦‚ç‡è¢«çœŸçš„maskï¼Œæ›¿æ¢ä¸º`[MASK]`ï¼šâ€œmy dog is [MASK]â€ã€‚
* æœ‰10%çš„æ¦‚ç‡è¢«æ›¿æ¢ä¸ºä»»æ„ä¸€ä¸ªè¯ï¼šâ€œmy dog is appleâ€ã€‚
* æœ‰10%çš„æ¦‚ç‡ä¸åšä»»ä½•æ”¹å˜ï¼šâ€œmy dog is hairyâ€ã€‚

ğŸ‘‰**Next Sentence Prediction**

NSPä»»åŠ¡ç¤ºä¾‹ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/20.png)

ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼Œå•è¯`flightless`è¢«WordPieceåˆ†æˆäº†ä¸¤ä¸ªè¯ï¼š`flight`å’Œ`less`ï¼Œ`##less`è¡¨ç¤ºåŸæœ¬æ˜¯å’Œä¸Šä¸€ä¸ªè¯ç»„åˆåœ¨ä¸€èµ·çš„ã€‚

### 7.A.2.Pre-training Procedure

ä¸ºäº†ç”Ÿæˆæ¯ä¸ªè®­ç»ƒè¾“å…¥åºåˆ—ï¼Œæˆ‘ä»¬ä»è¯­æ–™åº“ä¸­é‡‡æ ·å¾—åˆ°ä¸¤æ®µæ–‡å­—ï¼Œè§†ä¸ºæˆ‘ä»¬å®šä¹‰çš„å¥å­ï¼Œå³å¥å­Aå’Œå¥å­Bã€‚å¯¹äºNSPä»»åŠ¡ï¼Œæœ‰50%çš„æ¦‚ç‡å¥å­BçœŸçš„æ˜¯åœ¨å¥å­Aåé¢ï¼Œè¿˜æœ‰50%çš„æ¦‚ç‡å¥å­Bæ˜¯éšæœºé€‰æ‹©çš„ã€‚å¥å­Aå’Œå¥å­Bçš„ç»„åˆé•¿åº¦å°äºç­‰äº512ä¸ªtokenã€‚å¯¹äºMLMä»»åŠ¡ï¼Œmask rateä¸º15%ã€‚

è®­ç»ƒç”¨çš„batch sizeä¸º256ä¸ªåºåˆ—ï¼ˆ256ä¸ªåºåˆ—\*512ä¸ªtoken=128,000 tokens/batchï¼‰ï¼Œå…±è®­ç»ƒäº†1,000,000æ­¥ï¼Œè¿‘ä¼¼äºåœ¨3.3 billionçš„è¯­æ–™åº“ä¸Šè®­ç»ƒäº†40ä¸ªepochã€‚ä½¿ç”¨Adamï¼Œå­¦ä¹ ç‡ä¸º$1e-4$ï¼Œ$\beta_1=0.9$ï¼Œ$\beta_2=0.999$ï¼ŒL2 weight decay=0.01ï¼Œå‰10,000æ­¥ç”¨äºå­¦ä¹ ç‡warm upï¼Œå­¦ä¹ ç‡ä½¿ç”¨çº¿æ€§è¡°å‡ã€‚æ‰€æœ‰å±‚çš„dropoutæ¦‚ç‡éƒ½æ˜¯0.1ã€‚ä½¿ç”¨[GELU](http://shichaoxin.com/2022/04/09/è®ºæ–‡é˜…è¯»-GAUSSIAN-ERROR-LINEAR-UNITS-(GELUS)/)æ¿€æ´»å‡½æ•°ã€‚è®­ç»ƒlossæ˜¯å¹³å‡MLMä¼¼ç„¶å’Œå¹³å‡NSPä¼¼ç„¶ä¹‹å’Œã€‚

è®­ç»ƒ$\text{BERT}_{\text{BASE}}$ä½¿ç”¨äº†4å—TPUï¼ˆå…±16ä¸ªTPUèŠ¯ç‰‡ï¼‰ã€‚è®­ç»ƒ$\text{BERT}_{\text{LARGE}}$ä½¿ç”¨äº†16å—TPUï¼ˆå…±64ä¸ªTPUèŠ¯ç‰‡ï¼‰ã€‚æ¯ä¸ªæ¨¡å‹éƒ½é¢„è®­ç»ƒäº†4å¤©æ—¶é—´ã€‚

ä¸ºäº†åŠ é€Ÿé¢„è®­ç»ƒï¼Œå‰90%æ­¥ä½¿ç”¨é•¿åº¦ä¸º128çš„åºåˆ—ï¼Œå10%æ­¥ä½¿ç”¨é•¿åº¦ä¸º512çš„åºåˆ—ã€‚

### 7.A.3.Fine-tuning Procedure

å¯¹äºfine-tuneï¼Œé™¤äº†batch sizeã€å­¦ä¹ ç‡å’Œè®­ç»ƒçš„epochæ•°ï¼Œå‰©ä½™è¶…å‚æ•°å’Œé¢„è®­ç»ƒæ˜¯ä¸€æ ·çš„ã€‚æœ€ä¼˜çš„è¶…å‚æ•°æ˜¯å’Œä»»åŠ¡ç›¸å…³çš„ï¼Œä½†æˆ‘ä»¬å‘ç°å¦‚ä¸‹ä¸€äº›è¶…å‚æ•°å–å€¼å¯¹æ‰€æœ‰ä»»åŠ¡æ¥è¯´æ•ˆæœéƒ½è¿˜å¯ä»¥ï¼š

* **Batch size**ï¼š16ã€32ã€‚
* **å­¦ä¹ ç‡ï¼ˆAdamï¼‰**ï¼š$5e-5$ã€$3e-5$ã€$2e-5$ã€‚
* **epochæ•°é‡**ï¼š2ã€3ã€4ã€‚

æˆ‘ä»¬è¿˜å‘ç°ï¼Œå¤§å‹æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼Œ100k+å¸¦æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼‰å¯¹è¶…å‚æ•°é€‰æ‹©çš„æ•æ„Ÿåº¦è¿œä½äºå°å‹æ•°æ®é›†ã€‚fine-tuneé€šå¸¸éå¸¸å¿«ã€‚

### 7.A.4.Comparison of BERT, ELMo ,and OpenAI GPT

å¯¹äºæœ€è¿‘æµè¡Œçš„è¡¨å¾å­¦ä¹ æ¨¡å‹ï¼šELMoã€[OpenAI GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)å’ŒBERTï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…¶å·®å¼‚ã€‚æ¨¡å‹æ¡†æ¶çš„æ¯”è¾ƒè§Fig3ã€‚BERTå’Œ[OpenAI GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)æ˜¯fine-tuningæ–¹æ³•ï¼Œè€ŒELMoæ˜¯feature-basedæ–¹æ³•ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/21.png)

ä¸BERTé¢„è®­ç»ƒæ–¹æ³•æœ€ç›¸ä¼¼çš„æ˜¯[OpenAI GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)ï¼Œå®ƒåœ¨å¤§å‹æ–‡æœ¬åº“ä¸Šè®­ç»ƒä»å·¦åˆ°å³çš„Transformer LMã€‚BERTå’Œ[GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)çš„è®­ç»ƒæ–¹å¼æœ‰ä»¥ä¸‹ä¸€äº›å·®å¼‚ï¼š

* [GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)åœ¨BooksCorpusï¼ˆ800Mä¸ªè¯ï¼‰ä¸Šè®­ç»ƒï¼›BERTåœ¨BooksCorpusï¼ˆ800Mä¸ªè¯ï¼‰å’ŒWikipediaï¼ˆ2,500Mä¸ªè¯ï¼‰ä¸Šè®­ç»ƒã€‚
* [GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)åªåœ¨fine-tuneé˜¶æ®µå¼•å…¥`[SEP]`å’Œ`[CLS]`ï¼›è€ŒBERTæ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µã€‚
* [GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)è®­ç»ƒäº†1Mæ­¥ï¼Œbatch sizeä¸º32,000ä¸ªè¯ï¼›BERTä¹Ÿæ˜¯è®­ç»ƒäº†1Mæ­¥ï¼Œä½†batch sizeä¸º128,000ä¸ªè¯ã€‚
* [GPT](http://shichaoxin.com/2024/03/20/LLM-ä¸€æ–‡è¯»æ‡‚ChatGPTèƒŒåçš„æŠ€æœ¯/)åœ¨æ‰€æœ‰fine-tuneå®éªŒä¸­éƒ½ä½¿ç”¨åŒæ ·çš„å­¦ä¹ ç‡$5e-5$ï¼›è€ŒBERTæ ¹æ®fine-tuneçš„ä»»åŠ¡é€‰æ‹©ç‰¹å®šçš„å­¦ä¹ ç‡ã€‚

ç¬¬5.1éƒ¨åˆ†çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTå¤§éƒ¨åˆ†çš„æ”¹è¿›æ¥è‡ªä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡ä»¥åŠå…¶åŒå‘æ€§ã€‚

### 7.A.5.Illustrations of Fine-tuning on Different Tasks

åœ¨ä¸åŒä»»åŠ¡ä¸Šfine-tune BERTçš„ç¤ºæ„è§Fig4ã€‚ä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹æ˜¯åŸºäºBERTæ·»åŠ ä¸€ä¸ªé¢å¤–çš„è¾“å‡ºå±‚è€Œå½¢æˆçš„ã€‚(a)å’Œ(b)æ˜¯sequence-levelçš„ä»»åŠ¡ï¼Œè€Œ(c)å’Œ(d)æ˜¯token-levelçš„ä»»åŠ¡ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/22.png)

## 7.B.Detailed Experimental Setup

### 7.B.1.Detailed Descriptions for the GLUE Benchmark Experiments.

GLUE benchmarkåŒ…å«ä»¥ä¸‹æ•°æ®é›†ï¼š

1. **MNLI**ï¼šMulti-Genre Natural Language Inferenceã€‚
2. **QQP**ï¼šQuora Question Pairsã€‚
3. **QNLI**ï¼šQuestion Natural Language Inferenceã€‚
4. **SST-2**ï¼šStanford Sentiment Treebankã€‚
5. **CoLA**ï¼šCorpus of Linguistic Acceptabilityã€‚
6. **STS-B**ï¼šSemantic Textual Similarity Benchmarkã€‚
7. **MRPC**ï¼šMicrosoft Research Paraphrase Corpusã€‚
8. **RTE**ï¼šRecognizing Textual Entailmentã€‚
9. **WNLI**ï¼šWinograd NLIã€‚

## 7.C.Additional Ablation Studies

### 7.C.1.Effect of Number of Training Steps

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/23.png)

### 7.C.2.Ablation for Different Masking Procedures

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/BERT/24.png)

# 8.åŸæ–‡é“¾æ¥

ğŸ‘½[BERTï¼šPre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/x-jeff/AI_Papers/blob/master/2024/BERTï¼šPre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf)

# 9.å‚è€ƒèµ„æ–™

1. [BERT è®ºæ–‡é€æ®µç²¾è¯»ã€è®ºæ–‡ç²¾è¯»ã€‘](https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.880.my_history.page.click&vd_source=896374db59ca8f208a0bb9f453a24c25)
2. [ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘è‡ªç£å°å¼å­¸ç¿’ (Self-supervised Learning) (äºŒ) â€“ BERTç°¡ä»‹](https://www.youtube.com/watch?v=gh0hewYkjgo)