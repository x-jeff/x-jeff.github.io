---
layout:     post
title:      ã€TensorflowåŸºç¡€ã€‘ç¬¬åä¸‰è¯¾ï¼šWord2Vec
subtitle:   os.path.existsï¼Œurllib.request.urlretrieveï¼Œos.statï¼Œzipfile.ZipFileï¼ŒZipFile.namelistï¼Œtf.compat.as_strï¼Œcollections.dequeï¼Œrandom.randintï¼Œnumpy.random.choiceï¼Œtf.nn.embedding_lookupï¼Œtf.nn.nce_lossï¼Œxrangeï¼Œargsortï¼ŒTSNEé™ç»´å¯è§†åŒ–
date:       2022-04-07
author:     x-jeff
header-img: blogimg/20220407.jpg
catalog: true
tags:
    - Tensorflow Series
---
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Word2Vec

Word2Vecç›¸å…³çŸ¥è¯†è¯·è§ï¼š[ã€æ·±åº¦å­¦ä¹ åŸºç¡€ã€‘ç¬¬å››åäº”è¯¾ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥](http://shichaoxin.com/2021/01/17/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬å››åäº”è¯¾-è‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥/)ã€‚

# 2.ä»£ç å®ç°

ğŸ‘‰è½½å…¥åŒ…ï¼š

```python
import collections
import math
import os
import random
import zipfile

import numpy as np
from six.moves import urllib
from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf
```

ğŸ‘‰ä¸‹è½½æ•°æ®é›†ï¼š

```python
# Step 1: Download the data.
url = 'http://mattmahoney.net/dc/'

# ä¸‹è½½æ•°æ®é›†
def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
        filename, _ = urllib.request.urlretrieve(url + filename, filename)
    # è·å–æ–‡ä»¶ç›¸å…³å±æ€§
    statinfo = os.stat(filename)
    # æ¯”å¯¹æ–‡ä»¶çš„å¤§å°æ˜¯å¦æ­£ç¡®
    if statinfo.st_size == expected_bytes:
        print('Found and verified', filename)
    else:
        print(statinfo.st_size)
        raise Exception(
            'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

filename = maybe_download('text8.zip', 31344016)
```

`os.path.exists()`ç”¨äºåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›trueï¼›å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è¿”å›falseã€‚

`urllib.request.urlretrieve(url,filename)`ç”¨äºå°†urlï¼ˆå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„ä¹Ÿå¯ä»¥æ˜¯ç½‘ç»œé“¾æ¥ï¼‰è¡¨ç¤ºçš„å¯¹è±¡å¤åˆ¶åˆ°filenameï¼ˆä¿å­˜åˆ°æœ¬åœ°çš„è·¯å¾„ï¼‰ã€‚

`os.stat(path)`ç”¨äºåœ¨ç»™å®šçš„è·¯å¾„ä¸Šæ‰§è¡Œä¸€ä¸ªç³»ç»Ÿstatçš„è°ƒç”¨ã€‚è¿”å›å€¼ï¼š

* `st_mode`ï¼šinodeä¿æŠ¤æ¨¡å¼ã€‚
* `st_ino`ï¼šinodeèŠ‚ç‚¹å·ã€‚
* `st_dev`ï¼šinodeé©»ç•™çš„è®¾å¤‡ã€‚
* `st_nlink`ï¼šinodeçš„é“¾æ¥æ•°ã€‚
* `st_uid`ï¼šæ‰€æœ‰è€…çš„ç”¨æˆ·IDã€‚
* `st_gid`ï¼šæ‰€æœ‰è€…çš„ç»„IDã€‚
* `st_size`ï¼šæ™®é€šæ–‡ä»¶ä»¥å­—èŠ‚ä¸ºå•ä½çš„å¤§å°ï¼›åŒ…å«ç­‰å¾…æŸäº›ç‰¹æ®Šæ–‡ä»¶çš„æ•°æ®ã€‚
* `st_atime`ï¼šä¸Šæ¬¡è®¿é—®çš„æ—¶é—´ã€‚
* `st_mtime`ï¼šæœ€åä¸€æ¬¡ä¿®æ”¹çš„æ—¶é—´ã€‚
* `st_ctime`ï¼šç”±æ“ä½œç³»ç»ŸæŠ¥å‘Šçš„â€œctimeâ€ã€‚åœ¨æŸäº›ç³»ç»Ÿä¸Šï¼ˆå¦‚Unixï¼‰æ˜¯æœ€æ–°çš„å…ƒæ•°æ®æ›´æ”¹çš„æ—¶é—´ï¼Œåœ¨å…¶å®ƒç³»ç»Ÿä¸Šï¼ˆå¦‚Windowsï¼‰æ˜¯åˆ›å»ºæ—¶é—´ï¼ˆè¯¦ç»†ä¿¡æ¯å‚è§å¹³å°çš„æ–‡æ¡£ï¼‰ã€‚

ğŸ‘‰è¯»å–æ•°æ®ï¼š

```python
# Read the data into a list of strings.
def read_data(filename):
    """Extract the first file enclosed in a zip file as a list of words"""
    with zipfile.ZipFile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

# å•è¯è¡¨
words = read_data(filename)
```

`zipfile.ZipFile(file,mode)`ï¼šå¦‚æœmode='r'ï¼Œåˆ™ä¸ºè¯»å–å‹ç¼©æ–‡ä»¶fileä¸­çš„å†…å®¹ï¼›å¦‚æœmode='w'ï¼Œåˆ™ä¸ºå‘å‹ç¼©æ–‡ä»¶fileä¸­å†™å…¥å†…å®¹ã€‚

`ZipFile.namelist()`ï¼šè·å–å‹ç¼©æ–‡ä»¶å†…æ‰€æœ‰æ–‡ä»¶çš„åç§°åˆ—è¡¨ã€‚

`tf.compat.as_str`ï¼šå°†ç›®æ ‡è½¬åŒ–ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚

ğŸ‘‰åˆ›å»ºä¸€ä¸ªå•è¯è¡¨ï¼ˆå…±50000ä¸ªæœ€å¸¸è§çš„å•è¯ï¼ŒåŒ…å«`UNK`ï¼‰ï¼š

```python
# Step 2: Build the dictionary and replace rare words with UNK token.
# åªç•™50000ä¸ªå•è¯ï¼Œå…¶ä»–çš„è¯éƒ½å½’ä¸ºUNK
vocabulary_size = 50000

def build_dataset(words, vocabulary_size):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    # ç”Ÿæˆ dictionaryï¼Œè¯å¯¹åº”ç¼–å·, word:id(0-49999)
    # è¯é¢‘è¶Šé«˜ç¼–å·è¶Šå°
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    # dataæŠŠæ•°æ®é›†çš„è¯éƒ½ç¼–å·
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary['UNK']
            unk_count += 1
        data.append(index)
    # è®°å½•UNKè¯çš„æ•°é‡
    count[0][1] = unk_count
    # ç¼–å·å¯¹åº”è¯çš„å­—å…¸
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary

# data æ•°æ®é›†ï¼Œç¼–å·å½¢å¼
# count å‰50000ä¸ªå‡ºç°æ¬¡æ•°æœ€å¤šçš„è¯
# dictionary è¯å¯¹åº”ç¼–å·
# reverse_dictionary ç¼–å·å¯¹åº”è¯
data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)
del words  # Hint to reduce memory.
```

ğŸ‘‰äº§ç”Ÿbatchï¼š

```python
data_index = 0

# Step 3: Function to generate a training batch for the skip-gram model.
def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window

    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)

    span = 2 * skip_window + 1  # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)

    # å¾ªç¯3æ¬¡
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    # è·å–batchå’Œlabels
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [skip_window]
        # å¾ªç¯2æ¬¡ï¼Œä¸€ä¸ªç›®æ ‡å•è¯å¯¹åº”ä¸¤ä¸ªä¸Šä¸‹æ–‡å•è¯
        for j in range(num_skips):
            while target in targets_to_avoid:
                # å¯èƒ½å…ˆæ‹¿åˆ°å‰é¢çš„å•è¯ä¹Ÿå¯èƒ½å…ˆæ‹¿åˆ°åé¢çš„å•è¯
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    # Backtrack a little bit to avoid skipping words in the end of a batch
    # å›æº¯3ä¸ªè¯ã€‚å› ä¸ºæ‰§è¡Œå®Œä¸€ä¸ªbatchçš„æ“ä½œä¹‹åï¼Œdata_indexä¼šå¾€å³å¤šåç§»spanä¸ªä½ç½®
    data_index = (data_index + len(data) - span) % len(data)
    return batch, labels

# æ‰“å°sample data
batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
```

ä¸¾ä¸ªä¾‹å­è§£é‡Šä¸€ä¸‹ï¼Œæ•°æ®é›†ä¸­å‰6ä¸ªå•è¯åœ¨å•è¯è¡¨ä¸­çš„ç´¢å¼•è§ä¸‹ï¼š

![](https://github.com/x-jeff/BlogImage/raw/master/TensorflowSeries/Lesson13/13x1.png)

å¦‚æœä»¥ç¬¬1ä¸ªå•è¯ï¼ˆ3081ï¼‰ä¸ºä¸­å¿ƒï¼Œåˆ™å…¶ä¸Šä¸‹æ–‡ä¸ºç¬¬0ä¸ªå•è¯ï¼ˆ5234ï¼‰å’Œç¬¬2ä¸ªå•è¯ï¼ˆ12ï¼‰ï¼›å¦‚æœä»¥ç¬¬2ä¸ªå•è¯ï¼ˆ12ï¼‰ä¸ºä¸­å¿ƒï¼Œåˆ™å…¶ä¸Šä¸‹æ–‡ä¸ºç¬¬1ä¸ªå•è¯ï¼ˆ3081ï¼‰å’Œç¬¬3ä¸ªå•è¯ï¼ˆ6ï¼‰ï¼›å‰©ä½™ä»¥æ­¤ç±»æ¨ï¼Œåˆ™æ­¤æ—¶`generate_batch`å‡½æ•°è¿”å›çš„batchä¸ºï¼š

```
[3081 3081 12 12 6 6 195 195]
```

è¿”å›çš„labelsä¸ºï¼š

```
[
[5234]
[12]
[3081]
[6]
[12]
[195]
[6]
[2]
]
```

å¯¹åº”å…³ç³»ä¸ºï¼š

```
3081 originated -> 5234 anarchism
3081 originated -> 12 as
12 as -> 3081 originated
12 as -> 6 a
6 a -> 12 as
6 a -> 195 term
195 term -> 6 a
195 term -> 2 of
```

`collections.deque`ç”¨äºäº§ç”Ÿä¸€ä¸ªåŒå‘é˜Ÿåˆ—ï¼Œå¯ä»¥ä»ä¸¤ç«¯appendã€extendæˆ–popï¼š

```python
import collections
d = collections.deque([])
d.append('a') # åœ¨æœ€å³è¾¹æ·»åŠ ä¸€ä¸ªå…ƒç´ ï¼Œæ­¤æ—¶ d=deque('a')
d.appendleft('b') # åœ¨æœ€å·¦è¾¹æ·»åŠ ä¸€ä¸ªå…ƒç´ ï¼Œæ­¤æ—¶ d=deque(['b', 'a'])
d.extend(['c','d']) # åœ¨æœ€å³è¾¹æ·»åŠ æ‰€æœ‰å…ƒç´ ï¼Œæ­¤æ—¶ d=deque(['b', 'a', 'c', 'd'])
d.extendleft(['e','f']) # åœ¨æœ€å·¦è¾¹æ·»åŠ æ‰€æœ‰å…ƒç´ ï¼Œæ­¤æ—¶ d=deque(['f', 'e', 'b', 'a', 'c', 'd'])
d.pop() # å°†æœ€å³è¾¹çš„å…ƒç´ å–å‡ºï¼Œè¿”å› 'd'ï¼Œæ­¤æ—¶ d=deque(['f', 'e', 'b', 'a', 'c'])
d.popleft() # å°†æœ€å·¦è¾¹çš„å…ƒç´ å–å‡ºï¼Œè¿”å› 'f'ï¼Œæ­¤æ—¶ d=deque(['e', 'b', 'a', 'c'])
d.rotate(-2) # å‘å·¦æ—‹è½¬ä¸¤ä¸ªä½ç½®ï¼ˆæ­£æ•°åˆ™å‘å³æ—‹è½¬ï¼‰ï¼Œæ­¤æ—¶ d=deque(['a', 'c', 'e', 'b'])
d.count('a') # é˜Ÿåˆ—ä¸­'a'çš„ä¸ªæ•°ï¼Œè¿”å› 1
d.remove('c') # ä»é˜Ÿåˆ—ä¸­å°†'c'åˆ é™¤ï¼Œæ­¤æ—¶ d=deque(['a', 'e', 'b'])
d.reverse() # å°†é˜Ÿåˆ—å€’åºï¼Œæ­¤æ—¶ d=deque(['b', 'e', 'a'])
f=d.copy()
print(f)#deque(['b', 'e', 'a'])
f.clear()
print(f)#deque([])
 
#å¯ä»¥æŒ‡å®šé˜Ÿåˆ—çš„é•¿åº¦ï¼Œå¦‚æœæ·»åŠ çš„å…ƒç´ è¶…è¿‡æŒ‡å®šé•¿åº¦ï¼Œåˆ™åŸå…ƒç´ ä¼šè¢«æŒ¤å‡ºã€‚
e=collections.deque(maxlen=5)
e.extend([1,2,3,4,5])
e.append("a")
print(e)
#deque([2, 3, 4, 5, 'a'], maxlen=5)
e.appendleft("b")
print(e)
#deque(['b', 2, 3, 4, 5], maxlen=5)
e.extendleft(["c","d"])
print(e)
#deque(['d', 'c', 'b', 2, 3], maxlen=5)
```

`random.randint(a,b)`ï¼šå‚æ•°aå’Œå‚æ•°bå¿…é¡»æ˜¯æ•´æ•°ï¼Œè¯¥å‡½æ•°è¿”å›å‚æ•°aå’Œå‚æ•°bä¹‹é—´çš„ä»»æ„æ•´æ•°ï¼ˆ$[a,b]$ï¼‰ã€‚

ğŸ‘‰å»ºç«‹[skip-gramæ¨¡å‹](http://shichaoxin.com/2021/01/17/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬å››åäº”è¯¾-è‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥/#41skip-gramæ¨¡å‹)ï¼š

```python
# Step 4: Build and train a skip-gram model.
batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1  # How many words to consider left and right.
num_skips = 2  # How many times to reuse an input to generate a label.

# We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent.
valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
# ä»0-100æŠ½å–16ä¸ªæ•´æ•°ï¼Œæ— æ”¾å›æŠ½æ ·
valid_examples = np.random.choice(valid_window, valid_size, replace=False)
# è´Ÿé‡‡æ ·æ ·æœ¬æ•°
num_sampled = 64  # Number of negative examples to sample.

graph = tf.Graph()
with graph.as_default():
    # Input data.
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # è¯å‘é‡
    # Look up embeddings for inputs.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    # embedding_lookup(params,ids)å…¶å®å°±æ˜¯æŒ‰ç…§idsé¡ºåºè¿”å›paramsä¸­çš„ç¬¬idsè¡Œ
    # æ¯”å¦‚è¯´ï¼Œids=[1,7,4],å°±æ˜¯è¿”å›paramsä¸­ç¬¬1,7,4è¡Œã€‚è¿”å›ç»“æœä¸ºç”±paramsçš„1,7,4è¡Œç»„æˆçš„tensor
    # æå–è¦è®­ç»ƒçš„è¯
    embed = tf.nn.embedding_lookup(embeddings, train_inputs)

    # Construct the variables for the noise-contrastive estimation(NCE) loss
    nce_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Compute the average NCE loss for the batch.
    # tf.nce_loss automatically draws a new sample of the negative labels each
    # time we evaluate the loss.
    loss = tf.reduce_mean(
        tf.nn.nce_loss(weights=nce_weights,
                       biases=nce_biases,
                       labels=train_labels,
                       inputs=embed,
                       num_sampled=num_sampled,
                       num_classes=vocabulary_size))

    # Construct the SGD optimizer using a learning rate of 1.0.
    optimizer = tf.train.GradientDescentOptimizer(1).minimize(loss)

    # Compute the cosine similarity between minibatch examples and all embeddings.
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    # æŠ½å–ä¸€äº›å¸¸ç”¨è¯æ¥æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦
    # valid_embeddingsç»´åº¦[16,128]
    valid_embeddings = tf.nn.embedding_lookup(
        normalized_embeddings, valid_dataset)
    # valid_size == 16
    # [16,128] * [128,50000] = [16,50000]
    # 16ä¸ªè¯åˆ†åˆ«ä¸50000ä¸ªå•è¯ä¸­çš„æ¯ä¸€ä¸ªè®¡ç®—å†…ç§¯
    similarity = tf.matmul(
        valid_embeddings, normalized_embeddings, transpose_b=True)

    # Add variable initializer.
    init = tf.global_variables_initializer()
```

`numpy.random.choice(a, size=None, replace=True, p=None)`ç”¨äºç”Ÿæˆéšæœºæ ·æœ¬ï¼Œå‚æ•°è¯¦è§£ï¼š

1. `a`ï¼šä¸€ç»´æ•°ç»„æˆ–è€…ä¸€ä¸ªintå‹æ•´æ•°ã€‚å¦‚æœ`a`ä¸ºæ•°ç»„ï¼Œåˆ™ä»æ•°ç»„ä¸­çš„å…ƒç´ è¿›è¡Œéšæœºé‡‡æ ·ï¼›å¦‚æœ`a`ä¸ºintå‹æ•´æ•°ï¼Œåˆ™é‡‡æ ·èŒƒå›´ä¸º`np.arange(a)`ã€‚
2. `size`ï¼šéšæœºé‡‡æ ·çš„æ ·æœ¬çš„æ•°é‡ã€‚
3. `replace`ï¼šTrueè¡¨ç¤ºæœ‰æ”¾å›é‡‡æ ·ï¼›Falseè¡¨ç¤ºæ— æ”¾å›é‡‡æ ·ã€‚
4. `p`ï¼šä¸æ•°ç»„`a`å¯¹åº”ï¼Œä¸º`a`ä¸­æ¯ä¸ªå…ƒç´ è¢«é‡‡æ ·çš„æ¦‚ç‡ã€‚

```python
tf.random_uniform(shape,
                  minval=0,
                  maxval=None,
                  dtype=dtypes.float32,
                  seed=None,
                  name=None)
```

ç”¨äºäº§ç”Ÿ[minval,maxval)èŒƒå›´å†…æœä»å‡åŒ€åˆ†å¸ƒçš„å€¼ã€‚

`tf.nn.embedding_lookup`çš„ä½œç”¨è§ä¸Šè¿°ä»£ç æ³¨é‡Šã€‚

`tf.nn.nce_loss`ï¼šå¦‚æœä½¿ç”¨softmaxå‡½æ•°ï¼Œåˆ™ç±»åˆ«æ•°å¤ªå¤šï¼Œå¯¼è‡´è®¡ç®—é‡å¤ªå¤§ï¼Œæ‰€ä»¥è¿™é‡Œä½¿ç”¨NCE lossï¼ˆåŸæ–‡ï¼šNoise-contrastive estimation: A new estimation principle for unnormalized statistical modelsï¼‰ï¼Œå°†å¤šåˆ†ç±»é—®é¢˜è½¬åŒ–æˆäºŒåˆ†ç±»ã€‚

ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

$$similarity = \cos (\theta) = \frac{A \cdot B}{\parallel A \parallel \parallel B \parallel }$$

>å‚ç…§[å†…ç§¯](http://shichaoxin.com/2019/08/27/æ•°å­¦åŸºç¡€-ç¬¬ä¸ƒè¯¾-çŸ©é˜µä¸å‘é‡/#64æ•°é‡ç§¯)çš„è®¡ç®—ã€‚

ğŸ‘‰è®­ç»ƒæ¨¡å‹ï¼š

```python
# Step 5: Begin training.
num_steps = 100001
final_embeddings = []

with tf.Session(graph=graph) as session:
    # We must initialize all variables before we use them.
    init.run()
    print("Initialized")

    average_loss = 0
    for step in xrange(num_steps):
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„targetï¼Œä»¥åŠå¯¹åº”çš„labelsï¼Œéƒ½æ˜¯ç¼–å·å½¢å¼çš„
        batch_inputs, batch_labels = generate_batch(
            batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

        # We perform one update step by evaluating the optimizer op (including it
        # in the list of returned values for session.run()
        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
        average_loss += loss_val

        # è®¡ç®—è®­ç»ƒ2000æ¬¡çš„å¹³å‡loss
        if step % 2000 == 0:
            if step > 0:
                average_loss /= 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
            print("Average loss at step ", step, ": ", average_loss)
            average_loss = 0

        # Note that this is expensive (~20% slowdown if computed every 500 steps)
        if step % 20000 == 0:
            sim = similarity.eval()
            # è®¡ç®—éªŒè¯é›†çš„ä½™å¼¦ç›¸ä¼¼åº¦æœ€é«˜çš„è¯
            for i in xrange(valid_size):
                # æ ¹æ®idæ‹¿åˆ°å¯¹åº”å•è¯
                valid_word = reverse_dictionary[valid_examples[i]]
                top_k = 8  # number of nearest neighbors
                # ä»å¤§åˆ°å°æ’åºï¼Œæ’é™¤è‡ªå·±æœ¬èº«ï¼Œå–å‰top_kä¸ªå€¼
                nearest = (-sim[i, :]).argsort()[1:top_k + 1]
                log_str = "Nearest to %s:" % valid_word
                for k in xrange(top_k):
                    close_word = reverse_dictionary[nearest[k]]
                    log_str = "%s %s," % (log_str, close_word)
                print(log_str)
    # è®­ç»ƒç»“æŸå¾—åˆ°çš„è¯å‘é‡
    final_embeddings = normalized_embeddings.eval()
```

`xrange`å’Œ`range`ç”¨æ³•å®Œå…¨ç›¸åŒï¼Œæ‰€ä¸åŒçš„æ˜¯ç”Ÿæˆçš„ä¸æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œè€Œæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ã€‚`xrange`å·²åœ¨python3ä¸­è¢«å–æ¶ˆï¼Œå’Œ`range`å‡½æ•°åˆå¹¶ä¸º`range`ã€‚

`argsort`å°†å…ƒç´ ä»å°åˆ°å¤§æ’åºå¹¶è¿”å›å…¶å¯¹åº”çš„ç´¢å¼•ï¼š

```python
import numpy as np
a=np.array([[3,2,5],[6,3,9]])
print(a[0,:]) #array([3, 2, 5])
print(-a[0,:]) #array([-3, -2, -5])
#æœ€å°å€¼ä¸º-5ï¼Œå¯¹åº”ç´¢å¼•2
#ç¬¬äºŒå°çš„å€¼ä¸º-3ï¼Œå¯¹åº”ç´¢å¼•0
#æœ€å¤§å€¼ä¸º-2ï¼Œå¯¹åº”ç´¢å¼•1
print((-a[0,:]).argsort()) #array([2, 0, 1])
```

ğŸ‘‰ä½¿ç”¨TSNEè¿›è¡Œé™ç»´å¯è§†åŒ–ï¼š

![](https://github.com/x-jeff/BlogImage/raw/master/TensorflowSeries/Lesson13/13x2.png)

# 3.ä»£ç åœ°å€

1. [Word2Vec](https://github.com/x-jeff/Tensorflow_Code_Demo/tree/master/Demo12)

# 4.å‚è€ƒèµ„æ–™

1. [Python os.stat() æ–¹æ³•ï¼ˆèœé¸Ÿæ•™ç¨‹ï¼‰](https://www.runoob.com/python/os-stat.html)
2. [collections.deque()](https://blog.csdn.net/weixin_44056331/article/details/90261974)