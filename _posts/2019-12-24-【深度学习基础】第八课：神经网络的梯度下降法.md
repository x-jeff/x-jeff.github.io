---
layout:     post
title:      【深度学习基础】第八课：神经网络的梯度下降法
subtitle:   神经网络的梯度下降法，网络参数的随机初始化
date:       2019-12-24
author:     x-jeff
header-img: blogimg/20191224.jpg
catalog: true
tags:
    - Deep Learning Series
---
>【深度学习基础】系列博客为学习Coursera上吴恩达深度学习课程所做的课程笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.神经网络的梯度下降法

在[【深度学习基础】第四课：正向传播与反向传播](http://shichaoxin.com/2019/11/09/深度学习基础-第四课-正向传播与反向传播/)一文中我们了解了反向传播的原理，学习了梯度下降法在logistic回归中的应用。其实，logistic回归模型就可以看作是一个没有隐藏层的神经网络结构。那么，梯度下降法在一个带有隐藏层的浅层神经网络中是怎么应用的呢？这便是本文所要讨论的内容。

我们以[【深度学习基础】第六课：浅层神经网络](http://shichaoxin.com/2019/12/03/深度学习基础-第六课-浅层神经网络/)中所用的双层神经网络为例：

![](https://github.com/x-jeff/BlogImage/raw/master/DeepLearningSeries/Lesson6/6x1.png)

假设所有激活函数均为sigmoid函数，所要解决的问题为二分类问题。loss function和cost function依然采用[交叉熵损失函数](http://shichaoxin.com/2019/09/04/深度学习基础-第二课-softmax分类器和交叉熵损失函数/)。

## 1.1.单样本的情况

首先，我们先考虑只有一个样本的情况。

在反向传播之前，我们先复习下正向传播的过程。假设样本的维数为m，即有m个特征。则我们的输入为：

$$x=a^{[0]}=\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \\ \end{bmatrix} _{m \times 1}$$

第一层（即隐藏层）的参数$w^{[1]}$和$b^{[1]}$分别为：

$$w^{[1]}=\begin{bmatrix} \cdots & {w^{[1]}_1}^T & \cdots \\ \cdots & {w^{[1]}_2}^T & \cdots \\ \cdots & {w^{[1]}_3}^T & \cdots  \\ \cdots & {w^{[1]}_4}^T & \cdots \\ \end{bmatrix}_{4\times m}$$

$$b^{[1]}=\begin{bmatrix} b^{[1]}_1 \\  b^{[1]}_2 \\ b^{[1]}_3 \\ b^{[1]}_4 \\ \end{bmatrix}_{4\times 1}$$

据此可得到第一层的输出$a^{[1]}$为：

$$a^{[1]}=\begin{bmatrix} a^{[1]}_1 \\  a^{[1]}_2 \\ a^{[1]}_3 \\ a^{[1]}_4 \\ \end{bmatrix}_{4\times 1}=\sigma (z^{[1]})=\sigma (\begin{bmatrix} z^{[1]}_1 \\ z^{[1]}_2 \\ z^{[1]}_3 \\ z^{[1]}_4 \\ \end{bmatrix}_{4\times 1} )$$

第二层的参数$w^{[2]}$和$b^{[2]}$为：

$$w^{[2]}=\begin{bmatrix} \cdots & {w^{[2]}_1}^T & \cdots \\ \end{bmatrix}_{1\times 4}$$

$$b^{[2]}=\begin{bmatrix} b^{[2]}_1 \end{bmatrix}_{1\times 1}$$

第二层的输出$a^{[2]}$为：

$$a^{[2]}=\begin{bmatrix} a^{[2]}_1 \end{bmatrix}_{1\times 1}=\sigma (z^{[2]})=\sigma (\begin{bmatrix} z^{[2]}_1 \end{bmatrix}_{1\times 1})$$

借用[【深度学习基础】第四课：正向传播与反向传播](http://shichaoxin.com/2019/11/09/深度学习基础-第四课-正向传播与反向传播/)中计算的导数的结果，我们通过反向传播可以很快得到：

1. $da^{[2]}=-\frac{y}{a^{[2]}}+\frac{1-y}{1-a^{[2]}}$；维数为$1\times 1$。
2. $dz^{[2]}=a^{[2]}-y$；维数为$1\times 1$。
3. $dw^{[2]}=dz^{[2]}{a^{[1]}}^T$；维数为$1\times 4=(1\times 1)(1\times 4)$。
4. $db^{[2]}=dz^{[2]}$；维数为$1\times 1$。
5. $da^{[1]}=dz^{[2]}{w^{[2]}}^T$；维数为$4\times 1$。
6. $dz^{[1]}=da^{[1]} * g^{[1]'}(z^{[1]})$；维数为$4\times 1$。`*`表示$da^{[1]}$中的每个元素都乘上$g^{[1]'}(z^{[1]})$。其中，$g^{[1]'}(z^{[1]})=a^{[1]}*(1-a^{[1]})$。
7. $dw^{[1]}=dz^{[1]}{a^{[0]}}^T$；维度为$4\times m=(4\times 1)(1\times m)$。
8. $db^{[1]}=dz^{[1]}$；维度为$4\times 1$。

## 1.2.多个样本的情况

n个样本的情况和单样本基本类似：

1. $dZ^{[2]}=A^{[2]}-Y$；维度为$1\times n$。
2. $dW^{[2]}=\frac{1}{n} dZ^{[2]}{A^{[1]}}^T$；维数为$1\times 4=(1\times n)(n \times 4)$。
3. $db^{[2]}=\frac{1}{n} np.sum(dZ^{[2]},axis=1,keepdims=True)$；维度为$1\times 1$。
4. $dZ^{[1]}=dA^{[1]}*g^{[1]'}(Z^{[1]})$；维数为$4\times n$。
5. $dW^{[1]}=\frac{1}{n} dZ^{[1]} {A^{[0]}}^T$；维数为$4\times m=(4\times n)(n \times m)$。
6. $db^{[1]}=\frac{1}{n} np.sum(dZ^{[1]},axis=1,keepdims=True)$；维数为$4\times 1$。

# 2.网络参数的随机初始化

‼️如果将神经网络的各参数数组全部初始化为0，会使得梯度下降算法完全无效。

接下来我们一起来看一下为什么会出现这种现象。

![](https://github.com/x-jeff/BlogImage/raw/master/DeepLearningSeries/Lesson8/8x1.png)

对于上述网络，假设有：

$$W^{[1]}=\begin{bmatrix} 0 & 0 \\ 0 & 0 \\ \end{bmatrix};b^{[1]}=\begin{bmatrix} 0 \\ 0 \\  \end{bmatrix};W^{[2]}=\begin{bmatrix} 0 & 0 \\  \end{bmatrix};b^{[2]}=\begin{bmatrix} 0 \end{bmatrix}$$

如果各个神经元的激活函数类型都一样，那么给网络输入任何样本，得到的$a^{[1]}_1$、$a^{[1]}_2$和$a^{[2]}_1$都是相等的。并且在反向传播过程中，参数矩阵中各个元素总是相同的。即每个神经元都在做一模一样的运算，这样就使得神经网络失去了应有的意义。

那么解决的办法就是：**随机初始化**网络参数。例如产生高斯分布随机变量：

$$W^{[1]}=np.random.randn((2,2))*0.01$$

一般还会在末尾乘上一个很小的数字，例如0.01， 将权重初始化成很小的随机数。

因为如果权重初始值很大，对于sigmoid和tanh函数，计算得到的z值就会很大或很小，这样就会使梯度变得很小，梯度下降法的收敛速度会非常慢。 但是如果我们的神经网络中没有sigmoid或tanh函数，那么可能影响就不会很大。

在训练浅层神经网络时，0.01通常是一个比较合适的值。但是当训练深层神经网络时，一般会选择一个其他的常数，这个在今后的博客中会有介绍。

但是参数$b$可以初始化为0，这个是没有影响的：

$$b^{[1]}=np.zero((2,1))$$

# 3.代码地址

1. [浅层神经网络](https://github.com/x-jeff/DeepLearning_Code_Demo/tree/master/Demo2)