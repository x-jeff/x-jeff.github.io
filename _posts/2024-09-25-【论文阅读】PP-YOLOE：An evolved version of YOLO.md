---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘PP-YOLOEï¼šAn evolved version of YOLO
subtitle:   PP-YOLOE
date:       2024-09-25
author:     x-jeff
header-img: blogimg/20191112.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

>æºç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼š[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)ã€‚

å•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨å› åœ¨é€Ÿåº¦å’Œç²¾åº¦ä¸Šçš„è‰¯å¥½æƒè¡¡ï¼Œä¸€ç›´å—åˆ°æ¬¢è¿ã€‚å•é˜¶æ®µæ£€æµ‹å™¨ä¸­ï¼Œæœ€è‘—åçš„å°±æ˜¯YOLOç³»åˆ—ã€‚

å—åˆ°[YOLOX](https://shichaoxin.com/2024/01/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-YOLOX-Exceeding-YOLO-Series-in-2021/)çš„å¯å‘ï¼Œæˆ‘ä»¬å¯¹PP-YOLOv2è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæå‡ºäº†PP-YOLOEï¼ˆEè¡¨ç¤ºevolved versionï¼‰ã€‚PP-YOLOEä¸ºäº†æé«˜åœ¨ä¸åŒç¡¬ä»¶ä¸Šçš„é€šç”¨æ€§ï¼Œä¸å†ä½¿ç”¨[å¯å˜å½¢å·ç§¯](https://shichaoxin.com/2024/07/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deformable-Convolutional-Networks/)å’Œ[Matrix NMS](https://shichaoxin.com/2024/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PP-YOLO-An-Effective-and-Efficient-Implementation-of-Object-Detector/#32selection-of-tricks)ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/1.png)

# 2.Method

## 2.1.A Brief Review of PP-YOLOv2

è§[PP-YOLOv2](https://shichaoxin.com/2024/08/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PP-YOLOv2-A-Practical-Object-Detector/)ã€‚

## 2.2.Improvement of PP-YOLOE

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/2.png)

ğŸ‘‰**Anchor-free.**

[PP-YOLOv2](https://shichaoxin.com/2024/08/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PP-YOLOv2-A-Practical-Object-Detector/)ä»…ä¸ºæ¯ä¸ªGTç›®æ ‡åˆ†é…ä¸€ä¸ªanchor boxã€‚ç„¶è€Œï¼Œanchoræœºåˆ¶å¼•å…¥äº†å¾ˆå¤šè¶…å‚æ•°ï¼Œå¹¶ä¸”ä¾èµ–æ‰‹å·¥è®¾è®¡ï¼Œæ— æ³•å¾ˆå¥½çš„æ¨å¹¿åˆ°å…¶ä»–æ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†anchor-freeæœºåˆ¶ã€‚anchor-freeæœºåˆ¶éµå¾ª[FCOS](https://shichaoxin.com/2024/08/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)çš„æ€è·¯ï¼Œæ¯ä¸ªåƒç´ ç‚¹è§†ä¸ºä¸€ä¸ªanchor pointï¼Œå¹¶ä¸”ä¹Ÿä¸ºæ¯ä¸ªheadéƒ½è®¾ç½®äº†ä¸Šä¸‹é™ï¼ˆè¯¦è§ï¼š[FCOSè®ºæ–‡ç¬¬3.2éƒ¨åˆ†](https://shichaoxin.com/2024/08/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-FCOS-Fully-Convolutional-One-Stage-Object-Detection/#32multi-level-prediction-with-fpn-for-fcos)ï¼‰ï¼Œå°†GT bboxåˆ†é…åˆ°å¯¹åº”çš„feature mapä¸Šã€‚è·ç¦»GT bboxä¸­å¿ƒç‚¹æœ€è¿‘çš„åƒç´ ç‚¹è¢«è§†ä¸ºæ­£æ ·æœ¬ã€‚éµå¾ªYOLOç³»åˆ—ï¼Œé¢„æµ‹4ç»´å‘é‡$(x,y,w,h)$ç”¨äºå›å½’ã€‚å°½ç®¡æ ¹æ®[PP-YOLOv2](https://shichaoxin.com/2024/08/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PP-YOLOv2-A-Practical-Object-Detector/)çš„anchor sizeå¾ˆä»”ç»†çš„è®¾ç½®äº†ä¸Šä¸‹é™ï¼Œä½†anchor-basedæ–¹æ³•å’Œanchor-freeæ–¹æ³•ä¹‹é—´çš„åˆ†é…ç»“æœä»å­˜åœ¨ä¸€äº›å¾®å°çš„ä¸ä¸€è‡´ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç²¾åº¦è½»å¾®ä¸‹é™ã€‚

ğŸ‘‰**Backbone and Neck.**

æ®‹å·®è¿æ¥ï¼ˆæ¯”å¦‚[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ã€[ResNeXt](https://shichaoxin.com/2023/12/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/)ï¼‰å’Œå¯†é›†è¿æ¥ï¼ˆæ¯”å¦‚[DenseNet](https://shichaoxin.com/2023/11/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Densely-Connected-Convolutional-Networks/)ï¼‰åœ¨ç°ä»£å·ç§¯ç¥ç»ç½‘ç»œä¸­å·²ç»è¢«å¹¿æ³›ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†æ–°çš„RepResBlockç”¨äºbackboneå’Œneckä¸­ï¼Œå…¶ç»“åˆäº†æ®‹å·®è¿æ¥å’Œå¯†é›†è¿æ¥ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/3.png)

RepResBlockæ¥æºäºTreeBlockï¼Œè®­ç»ƒé˜¶æ®µæ‰€ç”¨çš„RepResBlockè§Fig3(b)ï¼Œæ¨ç†é˜¶æ®µæ‰€ç”¨çš„RepResBlockè§Fig3(c)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç®€åŒ–äº†åŸå§‹çš„TreeBlockï¼ˆè§Fig3(a)ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†concatæ“ä½œæ›¿æ¢ä¸ºäº†æŒ‰å…ƒç´ ç›¸åŠ çš„æ“ä½œï¼ˆè§Fig3(b)ï¼‰ï¼Œå› ä¸ºè¿™ä¸¤ç§æ“ä½œåœ¨æŸç§ç¨‹åº¦ä¸Šæœ‰ä¸€å®šçš„è¿‘ä¼¼æ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å°†RepResBlocké‡æ–°æ„å»ºä¸ºåŸºæœ¬çš„æ®‹å·®å—ï¼ˆè§Fig3(c)ï¼‰ã€‚

>TreeBlockï¼šLu Rao. Treenet: A lightweight one-shot aggregation convolutional network. arXiv preprint arXiv:2109.12342, 2021.ã€‚

PP-YOLOE-lçš„æ•´ä½“æ¡†æ¶å¯å‚è€ƒä¸‹å›¾ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/4.png)

>ä¸ªäººæ³¨è§£ï¼š$\oplus$è¡¨ç¤ºæŒ‰å…ƒç´ ç›¸åŠ ï¼Œ$\otimes$è¡¨ç¤ºæŒ‰å…ƒç´ ç›¸ä¹˜ã€‚

æˆ‘ä»¬ä½¿ç”¨RepResBlockæ¥æ„å»ºbackboneå’Œneckã€‚æˆ‘ä»¬å°†æˆ‘ä»¬æ„å»ºçš„backboneç§°ä¸ºCSPRepResNetï¼Œå…¶å¼€å§‹é¦–å…ˆæ˜¯3ä¸ªå·ç§¯å±‚ï¼Œç„¶åæ˜¯ç”±RepResBlockï¼ˆè§Fig3(d)ï¼‰æ„å»ºçš„4ä¸ªstageã€‚

>ä¸ªäººæ³¨è§£ï¼šCSPéƒ¨åˆ†å¯å‚è§[CSPNet](https://shichaoxin.com/2023/12/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN/)ã€‚

ESEï¼ˆEffective Squeeze and Extractionï¼‰ç”¨äºæ–½åŠ é€šé“æ³¨æ„åŠ›ï¼Œå…¶ç»“æ„è§ä¸‹ï¼ˆå–è‡ªè®ºæ–‡â€œCenterMask : Real-Time Anchor-Free Instance Segmentationâ€ï¼‰ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/5.png)

ä¸Šå›¾ä¸­ï¼Œ(a)æ˜¯OSAï¼ˆOne-Shot Aggregationï¼‰æ¨¡å—ï¼Œ(b)æ˜¯åœ¨OSAæ¨¡å—åŸºç¡€ä¸Šæ·»åŠ äº†æ®‹å·®è¿æ¥ï¼Œ(c)åœ¨(b)çš„åŸºç¡€ä¸Šæ·»åŠ äº†eSEï¼ˆeffective Squeeze-and-Excitationï¼‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆå³(c)ä¸­å³ä¸‹è§’è“è‰²éƒ¨åˆ†ï¼Œè¿™ä¸€éƒ¨åˆ†ä¹Ÿå°±æ˜¯PP-YOLOEæ‰€ç”¨çš„ESEï¼‰ã€‚

ç±»ä¼¼[YOLOv5](https://shichaoxin.com/2024/01/14/YOLO%E7%B3%BB%E5%88%97-YOLOv5/)ï¼Œæˆ‘ä»¬ä½¿ç”¨width multiplier $\alpha$å’Œdepth multiplier $\beta$æ¥æ§åˆ¶æ¨¡å‹çš„å¤§å°ã€‚æˆ‘ä»¬è®¾ç½®backboneçš„åŸºç¡€widthä¸º$[64,128,256,512,1024]$ï¼ˆä¸ªäººæ³¨è§£ï¼šä»PP-YOLOE-lçš„æ¡†æ¶å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨backboneä¸­ï¼Œstem layeråˆ°stage layer4çš„è¾“å‡ºé€šé“æ•°åˆ†åˆ«ä¸º$[64,128,256,512,1024]$ï¼Œå³ç›¸å½“äºæ­¤æ—¶$\alpha=1$ï¼Œå¦‚æœæ˜¯PP-YOLOE-sæ¨¡å‹ï¼Œæœ‰$\alpha=0.5$ï¼Œé‚£ä¹ˆstem layeråˆ°stage layer4çš„è¾“å‡ºé€šé“æ•°åº”è¯¥åˆ†åˆ«ä¸º$[32,64,128,256,512]$ï¼‰ã€‚ä¸è€ƒè™‘stemï¼Œè®¾ç½®backboneçš„åŸºç¡€depthä¸º$[3,6,6,3]$ï¼ˆä¸ªäººæ³¨è§£ï¼šåˆ†åˆ«å¯¹åº”backboneä¸­stage layer1åˆ°stage layer4çš„æ•°é‡ï¼Œå¦‚æœæ˜¯PP-YOLOE-sï¼Œæœ‰$\beta=0.33$ï¼Œåˆ™stage layer1åˆ°stage layer4ä¸­çš„å±‚æ•°åº”è¯¥åˆ†åˆ«ä¸º$[1,2,2,1]$ï¼‰ã€‚ç±»ä¼¼çš„ï¼Œåœ¨neckä¸­ï¼Œæˆ‘ä»¬è®¾ç½®åŸºç¡€widthä¸º$[192,384,768]$ï¼ŒåŸºç¡€depthä¸º$3$ï¼ˆä¸ªäººæ³¨è§£ï¼šæŒ‡çš„æ˜¯neckä¸­çš„CSPResLayerï¼‰ã€‚ä¸åŒå¤§å°æ¨¡å‹çš„$\alpha,\beta$è®¾ç½®è§è¡¨1ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/6.png)

>ä¸ªäººæ³¨è§£ï¼šwidth multiplier $\alpha$å’Œæ¡†æ¶å›¾RepVGGBlockä¸­çš„alphaä¸æ˜¯ä¸€å›äº‹ã€‚

ğŸ‘‰**Task Alignment Learning (TAL).**

æˆ‘ä»¬ä½¿ç”¨äº†[TAL](http://shichaoxin.com/2024/08/29/è®ºæ–‡é˜…è¯»-TOOD-Task-aligned-One-stage-Object-Detection/)ä¸­çš„åŠ¨æ€æ ‡ç­¾åˆ†é…ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/9.png)

ğŸ‘‰**Efficient Task-aligned Head (ET-head).**

æˆ‘ä»¬åŸºäº[TOOD](http://shichaoxin.com/2024/08/29/è®ºæ–‡é˜…è¯»-TOOD-Task-aligned-One-stage-Object-Detection/)ä¸­çš„T-headï¼Œæå‡ºäº†ç®€åŒ–çš„ET-headï¼Œå¦‚Fig2æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†T-headä¸­çš„layer attentionæ›¿æ¢ä¸ºäº†ESEã€‚

>ä¸ªäººæ³¨è§£ï¼šå’ŒFig2å¯¹æ¯”å‘ç°ï¼Œä¸Šé¢è¯¦ç»†æ¡†æ¶å›¾ä¸­çš„PPYOLOESELayeråº”è¯¥ç”»çš„æœ‰é—®é¢˜ï¼Œæ­£ç¡®çš„åº”è¯¥å¦‚ä¸‹ï¼š
>
>![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/7.png)

ä½¿ç”¨çš„æŸå¤±å‡½æ•°ä¸ºï¼š

$$Loss = \frac{\alpha \cdot loss_{VFL} + \beta \cdot loss_{GIoU} + \gamma \cdot loss_{DFL}}{\sum_i^{N_{pos}}\hat{t}}$$

æŸå¤±å‡½æ•°çš„å½¢å¼å€Ÿé‰´PP-Picodetï¼ˆåœ¨PP-Picodetä¸­ï¼Œ$\alpha=1,\beta=2,\gamma=0.25$ï¼‰ã€‚$\hat{t}$è¡¨ç¤ºå½’ä¸€åŒ–ç›®æ ‡åˆ†æ•°ï¼Œè¯¦è§[TOOD](http://shichaoxin.com/2024/08/29/è®ºæ–‡é˜…è¯»-TOOD-Task-aligned-One-stage-Object-Detection/#322task-aligned-loss)ã€‚

>* PP-Picodetï¼šGuanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng Cui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong Wang, Yuning Du, Baohua Lai, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, and Yanjun Ma. Pp-picodet: A better real-time object detector on mobile devices. CoRR, abs/2111.00902, 2021.ã€‚
>* VFLï¼š[ã€è®ºæ–‡é˜…è¯»ã€‘VarifocalNetï¼šAn IoU-aware Dense Object Detector](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-VarifocalNet-An-IoU-aware-Dense-Object-Detector/)ã€‚
>* DFLï¼š[ã€è®ºæ–‡é˜…è¯»ã€‘Generalized Focal Lossï¼šLearning Qualified and Distributed Bounding Boxes for Dense Object Detection](http://shichaoxin.com/2024/09/04/è®ºæ–‡é˜…è¯»-Generalized-Focal-Loss-Learning-Qualified-and-Distributed-Bounding-Boxes-for-Dense-Object-Detection/)ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/8.png)

# 3.Experiment

æ‰€æœ‰å®éªŒçš„è®­ç»ƒéƒ½åœ¨MS COCO-2017è®­ç»ƒé›†ä¸Šè¿›è¡Œï¼Œå…±118kå¼ å›¾åƒï¼Œ80ä¸ªç±»åˆ«ã€‚å¯¹äºæ¶ˆèè¯•éªŒï¼Œæˆ‘ä»¬åŸºäºMS COCO-2017éªŒè¯é›†ï¼ˆå…±5000kå¼ å›¾åƒï¼‰ï¼Œä½¿ç”¨single-scaleå’Œæ ‡å‡†COCO APè¯„ä»·æŒ‡æ ‡ã€‚åœ¨MS COCO-2017 test-devä¸Šæ±‡æŠ¥äº†æœ€ç»ˆç»“æœã€‚

## 3.1.Implementation details

ä½¿ç”¨SGDï¼Œmomentum=0.9ï¼Œweight decay=$5e-4$ã€‚ä½¿ç”¨[cosine learning rate schedule](https://shichaoxin.com/2024/07/10/è®ºæ–‡é˜…è¯»-Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/#51cosine-learning-rate-decay)ï¼Œä¸€å…±300ä¸ªepochï¼Œ5ä¸ªepochç”¨äºwarmupï¼ŒåŸºç¡€å­¦ä¹ ç‡ä¸º0.01ã€‚æ€»çš„batch sizeä¸º64ï¼Œåœ¨8å—32G V100 GPUä¸Šï¼Œéµå¾ªlinear scaling ruleæ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚ä½¿ç”¨[EMA](https://shichaoxin.com/2020/02/25/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬åå…­è¯¾-æŒ‡æ•°åŠ æƒå¹³å‡/)ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¾decay=0.9998ã€‚æˆ‘ä»¬åªä½¿ç”¨äº†ä¸€äº›åŸºç¡€çš„æ•°æ®æ‰©å±•ï¼ŒåŒ…æ‹¬éšæœºè£å‰ªã€éšæœºæ°´å¹³ç¿»è½¬ã€color distortionå’Œå¤šå°ºåº¦ã€‚è¾“å…¥å›¾åƒçš„å¤§å°åœ¨320åˆ°768èŒƒå›´å†…å‡åŒ€é‡‡æ ·32ä¸ªã€‚

>linear scaling ruleå‡ºè‡ªè®ºæ–‡ï¼šPriya Goyal, Piotr DollÂ´ ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.ã€‚

## 3.2.Comparsion with Other SOTA Detectors

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/PPYOLOE/10.png)

# 4.Conclusion

ä¸å†èµ˜è¿°ã€‚

# 5.åŸæ–‡é“¾æ¥

ğŸ‘½[PP-YOLOEï¼šAn evolved version of YOLO](https://github.com/x-jeff/AI_Papers/blob/master/2024/PP-YOLOEï¼šAn%20evolved%20version%20of%20YOLO.pdf)

# 6.å‚è€ƒèµ„æ–™

1. [A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS](https://arxiv.org/html/2304.00501v6/#S1)