---
layout:     post
title:      【深度学习基础】第十九课：Adam优化算法
subtitle:   Adam优化算法
date:       2020-03-19
author:     x-jeff
header-img: blogimg/20200319.jpg
catalog: true
tags:
    - Deep Learning Series
---
>【深度学习基础】系列博客为学习Coursera上吴恩达深度学习课程所做的课程笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.Adam优化算法

Adam优化算法（Adaptive Moment Estimation）和[RMSprop](http://shichaoxin.com/2020/03/13/深度学习基础-第十八课-RMSprop/)、[Momentum梯度下降法](http://shichaoxin.com/2020/03/05/深度学习基础-第十七课-Momentum梯度下降法/)是被广泛应用并且经受了大量考验的优化算法，适用于不同的深度学习结构。

Adam优化算法实际上就是将RMSprop和Momentum结合在一起。

接下来看下Adam优化算法的详细步骤：

* 初始化：$V_{dw}=0,S_{dw}=0,V_{db}=0,S_{db}=0$
* On iteration t:
	* Compute $dw,db$ using current mini-batch
	* $$V_{dw}=\beta _1 V_{dw}+(1-\beta_1)dw;V_{db}=\beta _1 V_{db}+(1-\beta_1)db$$
	* $$S_{dw}=\beta_2 S_{dw}+(1-\beta_2)(dw)^2;S_{db}=\beta_2 S_{db}+(1-\beta_2)(db)^2$$
	* $$V^{corrected}_{dw}=V_{dw}/(1-\beta _1^t);V^{corrected}_{db}=V_{db}/(1-\beta_1 ^t)$$
	* $$S^{corrected}_{dw}=S_{dw}/(1-\beta _2^t);S^{corrected}_{db}=S_{db}/(1-\beta_2 ^t)$$
	* $$w:=w-\alpha \frac{V^{corrected}_{dw}}{\sqrt{S^{corrected}_{dw}}+\epsilon};b:=b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}}+\epsilon}$$

Adam算法涉及很多超参数：

1. $\alpha$：学习率，需要自行尝试得到合适的值。
2. $\beta_1$：常用的值是0.9。
3. $\beta_2$：Adam算法的发明者推荐使用0.999。
4. $\epsilon$：Adam算法的发明者建议为$10^{-8}$。

其中超参数$\beta_1,\beta_2,\epsilon$通常使用推荐值即可，没有调整的必要，对结果影响不大。

后续提出的AdamW则可用来解决Adam优化器中L2正则化失效的问题。Adam算法参数更新的计算公式如下（和上面是一样的，只是换了种表示方式）：

$$t=t+1$$

$$moment\_1\_out = \beta _1 * moment\_1 + (1-\beta _1) * grad$$

$$moment\_2\_out = \beta_2 * moment\_2 + (1-\beta_2) * grad * grad$$

$$learning\_rate = learning\_rate * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$$

$$param\_out = param - learning\_rate * \frac{moment\_1}{\sqrt{moment\_2} + \epsilon}$$

AdamW算法参数更新的计算公式如下：

$$t =t +1$$

$$moment\_1\_out = \beta_1 * moment\_1 + (1 - \beta_1) * grad$$

$$moment\_2\_out = \beta_2 * moment\_2 + (1-\beta_2) * grad * grad$$

$$learning\_rate = learning\_rate * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}$$

$$param\_out = param - learning\_rate * ( \frac{moment\_1}{\sqrt{moment\_2} + \epsilon} + \lambda * param )$$

# 2.参考资料

1. [Adam](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Adam_cn.html#cn-api-paddle-optimizer-adam)
2. [AdamW](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/AdamW_cn.html)