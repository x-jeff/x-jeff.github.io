---
layout:     post
title:      【深度学习基础】第十九课：Adam优化算法
subtitle:   Adam优化算法
date:       2020-03-19
author:     x-jeff
header-img: blogimg/20200319.jpg
catalog: true
tags:
    - Deep Learning Series
---
>【深度学习基础】系列博客为学习Coursera上吴恩达深度学习课程所做的课程笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.Adam优化算法

Adam优化算法（Adaptive Moment Estimation）和[RMSprop](http://shichaoxin.com/2020/03/13/深度学习基础-第十八课-RMSprop/)、[Momentum梯度下降法](http://shichaoxin.com/2020/03/05/深度学习基础-第十七课-Momentum梯度下降法/)是被广泛应用并且经受了大量考验的优化算法，适用于不同的深度学习结构。

Adam优化算法实际上就是将RMSprop和Momentum结合在一起。

接下来看下Adam优化算法的详细步骤：

* 初始化：$V_{dw}=0,S_{dw}=0,V_{db}=0,S_{db}=0$
* On iteration t:
	* Compute $dw,db$ using current mini-batch
	* $V_{dw}=\beta _1 V_{dw}+(1-\beta_1)dw;V_{db}=\beta _1 V_{db}+(1-\beta_1)db$
	* $S_{dw}=\beta_2 S_{dw}+(1-\beta_2)(dw)^2;S_{db}=\beta_2 S_{db}+(1-\beta_2)(db)^2$
	* $V^{corrected}_{dw}=V_{dw}/(1-\beta _1^t);V^{corrected}_{db}=V_{db}/(1-\beta_1 ^t)$
	* $S^{corrected}_{dw}=S_{dw}/(1-\beta _2^t);S^{corrected}_{db}=S_{db}/(1-\beta_2 ^t)$
	* $w:=w-\alpha \frac{V^{corrected}_{dw}}{\sqrt{S^{corrected}_{dw}}+\epsilon};b:=b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}}+\epsilon}$

Adam算法设计很多超参数：

1. $\alpha$：学习率，需要自行尝试得到合适的值。
2. $\beta_1$：常用的值是0.9。
3. $\beta_2$：Adam算法的发明者推荐使用0.999。
4. $\epsilon$：Adam算法的发明者建议为$10^{-8}$。

其中超参数$\beta_1,\beta_2,\epsilon$通常使用推荐值即可，没有调整的必要，对结果影响不大。