---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘VarifocalNetï¼šAn IoU-aware Dense Object Detector
subtitle:   VarifocalNetï¼ˆVFNetï¼‰ï¼ŒVarifocal Lossï¼ŒIACS
date:       2024-09-25
author:     x-jeff
header-img: blogimg/20181013.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

ç°åœ¨çš„ç›®æ ‡æ£€æµ‹å™¨ï¼Œæ— è®ºæ˜¯å•é˜¶æ®µæ–¹æ³•è¿˜æ˜¯ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé€šå¸¸é¦–å…ˆä¼šç”Ÿæˆä¸€ç»„å†—ä½™çš„bboxï¼Œå¹¶å¸¦æœ‰åˆ†ç±»åˆ†æ•°ï¼Œç„¶åå†ä½¿ç”¨NMSæ¥å»é™¤åŒä¸€ç›®æ ‡ä¸Šçš„é‡å¤bboxã€‚é€šå¸¸åˆ†ç±»åˆ†æ•°è¢«ç”¨äºåœ¨NMSä¸­å¯¹bboxè¿›è¡Œæ’åºã€‚ç„¶è€Œï¼Œè¿™ä¼šå½±å“æ£€æµ‹æ€§èƒ½ï¼Œå› ä¸ºåˆ†ç±»åˆ†æ•°å¹¶ä¸æ€»æ˜¯èƒ½å¤Ÿå¾ˆå¥½çš„ä¼°è®¡bboxçš„å®šä½ç²¾åº¦ï¼Œå› æ­¤é‚£äº›å®šä½å‡†ç¡®ä½†åˆ†ç±»åˆ†æ•°è¾ƒä½çš„æ£€æµ‹å¯èƒ½ä¼šåœ¨NMSä¸­è¢«è¯¯åˆ ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç°æœ‰çš„å¯†é›†ç›®æ ‡æ£€æµ‹å™¨é¢„æµ‹[é¢å¤–çš„IoUåˆ†æ•°](http://shichaoxin.com/2024/08/16/è®ºæ–‡é˜…è¯»-IoU-aware-Single-stage-Object-Detector-for-Accurate-Localization/)æˆ–[centernessåˆ†æ•°](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)ä½œä¸ºå¯¹å®šä½ç²¾åº¦çš„ä¼°è®¡ï¼Œå¹¶å°†å…¶ä¸åˆ†ç±»åˆ†æ•°ç›¸ä¹˜æ¥å¯¹NMSä¸­çš„bboxè¿›è¡Œæ’åºã€‚è¿™äº›æ–¹æ³•å¯ä»¥ç¼“è§£åˆ†ç±»åˆ†æ•°å’Œç›®æ ‡å®šä½ç²¾åº¦ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºå¦‚æœå°†ä¸¤ä¸ªä¸å®Œç¾çš„é¢„æµ‹ç›¸ä¹˜å¯èƒ½ä¼šå¯¼è‡´æ›´å·®çš„æ’åºï¼Œæˆ‘ä»¬ä¹Ÿåœ¨å®éªŒä¸­å±•ç¤ºäº†è¿™ç§æ–¹æ³•æ‰€èƒ½è¾¾åˆ°çš„æ€§èƒ½æ˜¯æœ‰é™çš„ã€‚æ­¤å¤–ï¼Œæ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç½‘ç»œåˆ†æ”¯å¹¶ä¸ä¼˜é›…ï¼Œè¿˜ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚

é‚£ä¹ˆæˆ‘ä»¬èƒ½å¦ä¸é¢å¤–é¢„æµ‹å®šä½ç²¾åº¦åˆ†æ•°ï¼Œè€Œæ˜¯å°†å…¶ä¸åˆ†ç±»åˆ†æ•°åˆå¹¶ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œé¢„æµ‹ä¸€ä¸ªå¯ä»¥æ„ŸçŸ¥å®šä½ç²¾åº¦çš„åˆ†ç±»åˆ†æ•°ï¼ˆè®°ä¸º**IACS**ï¼Œlocalization-aware or **I**oU-**a**ware **C**lassification **S**coreï¼‰ã€‚

æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®ï¼š

1. å±•ç¤ºäº†IACSçš„ä¼˜åŠ¿ã€‚
2. æå‡ºäº†ä¸€ç§æ–°çš„**Varifocal Loss**ï¼Œç”¨äºè®­ç»ƒå¯†é›†ç›®æ ‡æ£€æµ‹å™¨æ¥å›å½’IACSã€‚
3. è®¾è®¡äº†ä¸€ç§æ–°çš„æ˜Ÿå½¢bboxç‰¹å¾è¡¨ç¤ºæ–¹æ³•ã€‚
4. å¼€å‘äº†ä¸€ç§æ–°çš„å¯†é›†ç›®æ ‡æ£€æµ‹å™¨ï¼Œç§°ä¸º**VarifocalNet**æˆ–**VFNet**ã€‚

æˆ‘ä»¬çš„æ–¹æ³•å¦‚Fig1æ‰€ç¤ºã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/1.png)

# 2.Related Work

ä¸å†è¯¦è¿°ã€‚

# 3.Motivation

åœ¨æœ¬éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç ”ç©¶äº†[FCOS](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)çš„æ€§èƒ½ä¸Šé™ï¼Œå¹¶è¯†åˆ«äº†å…¶ä¸»è¦ç“¶é¢ˆã€‚è¿˜å±•ç¤ºäº†ä½¿ç”¨IACSä½œä¸ºæ’åºæ ‡å‡†çš„é‡è¦æ€§ã€‚

Fig2å±•ç¤ºäº†[FCOS](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/) headè¾“å‡ºçš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¾“å‡ºåŒ…å«3éƒ¨åˆ†ï¼šåˆ†ç±»åˆ†æ•°ã€bboxå’Œcenternessåˆ†æ•°ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/2.png)

æœ¬æ–‡å®é™…ç ”ç©¶çš„æ˜¯[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)ï¼Œå…¶åœ¨COCO train2017ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæµ‹è¯•ç»“æœè§è¡¨1ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/3.png)

å¦‚è¡¨1æ‰€ç¤ºï¼Œæˆ‘ä»¬ä¾æ¬¡å°†[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)çš„è¾“å‡ºç»„ä»¶æ›¿æ¢ä¸ºGTï¼Œç„¶åå†æ‰§è¡ŒNMSã€‚`w/ctr`è¡¨ç¤ºåœ¨æ¨ç†é˜¶æ®µä½¿ç”¨centernessåˆ†æ•°ã€‚`gt_ctr`è¡¨ç¤ºåœ¨æ¨ç†é˜¶æ®µï¼Œå°†é¢„æµ‹çš„centernessåˆ†æ•°æ›¿æ¢ä¸º[GT centernessåˆ†æ•°](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/#33center-ness-for-fcos)ã€‚`gt_ctr_iou`è¡¨ç¤ºåœ¨æ¨ç†é˜¶æ®µï¼Œå°†é¢„æµ‹çš„centernessåˆ†æ•°æ›¿æ¢ä¸ºé¢„æµ‹bboxå’ŒGT bboxçš„IoUã€‚`gt_bbox`è¡¨ç¤ºåœ¨æ¨ç†é˜¶æ®µï¼Œå°†é¢„æµ‹çš„bboxæ›¿æ¢ä¸ºGT bboxï¼Œæ³¨æ„ï¼Œå› ä¸ºæœ‰å¯èƒ½å‘ç”Ÿåˆ†ç±»é”™è¯¯ï¼Œæ‰€ä»¥å³ä½¿æ›¿æ¢ä¸ºGT bboxï¼Œå‡†ç¡®ç‡ä¹Ÿä¸æ˜¯100%ã€‚`gt_cls`è¡¨ç¤ºåœ¨æ¨ç†é˜¶æ®µï¼Œå°†åˆ†æ•°åˆ†æ•°æ›¿æ¢ä¸ºçœŸå®ç±»åˆ«ï¼Œåˆ†ç±»æ­£ç¡®ä¸º1ï¼Œåˆ†ç±»é”™è¯¯ä¸º0ã€‚`gt_cls_iou`è¡¨ç¤ºå°†åˆ†ç±»åˆ†æ•°æ›¿æ¢ä¸ºé¢„æµ‹bboxå’ŒGT bboxçš„IoUã€‚

ä»è¡¨1å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹çš„[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)çš„APä¸º39.2%ï¼Œå³ä½¿æ›¿æ¢äº†`gt_ctr`ï¼ˆ41.1%çš„APï¼‰æˆ–`gt_ctr_iou`ï¼ˆ43.5%çš„APï¼‰ï¼Œä¹Ÿæ²¡æœ‰å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ä¸ä½¿ç”¨centernessåˆ†æ•°ï¼Œä»…æ›¿æ¢`gt_bbox`çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¾¾åˆ°äº†56.1%çš„APã€‚å¦‚æœæ˜¯ä»…æ›¿æ¢`gt_cls`ï¼Œåˆ™APä¸º43.1%ï¼Œä½†åœ¨centernessåˆ†æ•°çš„åŠ æŒä¸‹ï¼ŒAPå¯ä»¥æå‡è‡³58.1%ï¼Œè¯´æ˜centernessåˆ†æ•°åœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥åŒºåˆ†å‡†ç¡®çš„å’Œä¸å‡†ç¡®çš„bboxã€‚

æœ€ä»¤äººæƒŠè®¶çš„ç»“æœæ˜¯åœ¨ä¸ä½¿ç”¨centernessçš„æƒ…å†µä¸‹ï¼Œä»…æ›¿æ¢`gt_cls_iou`ç«Ÿç„¶è¾¾åˆ°äº†74.7%çš„APï¼Œæ˜¾è‘—é«˜äºå…¶ä»–æƒ…å†µã€‚è¿™æ­ç¤ºäº†åœ¨è¯¸å¤šé¢„æµ‹çš„å€™é€‰bboxä¸­ï¼Œå·²ç»å­˜åœ¨å‡†ç¡®çš„bboxï¼Œè€Œå®ç°å“è¶Šæ£€æµ‹æ€§èƒ½çš„å…³é”®å°±åœ¨äºä»é¢„æµ‹çš„å€™é€‰bboxä¸­å‡†ç¡®çš„é€‰æ‹©å‡ºé«˜è´¨é‡çš„bboxã€‚ä¸Šè¿°å®éªŒä¹Ÿè¡¨æ˜ï¼Œå°†åˆ†ç±»åˆ†æ•°æ›¿æ¢ä¸ºIoUæ˜¯æœ€æœ‰æ½œåŠ›çš„ï¼ŒåŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†**IACSï¼ˆIoU-aware Classification Scoreï¼‰**ã€‚

# 4.VarifocalNet

æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å¯†é›†ç›®æ ‡æ£€æµ‹å™¨ï¼Œç§°ä¸ºVarifocalNetï¼Œç®€ç§°VFNetï¼Œå…¶åŸºäº[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)ï¼Œä½†ç§»é™¤äº†centernessåˆ†æ”¯ã€‚å’Œ[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)ç›¸æ¯”ï¼ŒVFNetæœ‰3ä¸ªæ–°ç»„ä»¶ï¼švarifcoal lossã€æ˜Ÿå½¢çš„bboxç‰¹å¾è¡¨å¾ã€bboxçš„refineã€‚

## 4.1.IACSâ€“IoU-Aware Classification Score

IACSæ›¿ä»£äº†ä¼ ç»Ÿçš„åˆ†ç±»åˆ†æ•°ï¼Œåœ¨GTç±»åˆ«æ ‡ç­¾çš„ä½ç½®ä¸Šï¼Œå…¶å€¼ä¸ºé¢„æµ‹bboxå’ŒGT bboxçš„IoUã€‚

## 4.2.Varifocal Loss

åœ¨[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)çš„å¯å‘ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†Varifocal Lossã€‚

æˆ‘ä»¬é¦–å…ˆæ¥å¤ä¹ ä¸‹[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)ã€‚[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)ç”¨äºè§£å†³å¯†é›†ç›®æ ‡æ£€æµ‹å™¨åœ¨è®­ç»ƒæ—¶å‰æ™¯ç±»åˆ«å’ŒèƒŒæ™¯ç±»åˆ«çš„æç«¯ä¸å¹³è¡¡é—®é¢˜ã€‚å…¶å®šä¹‰ä¸ºï¼š

$$\text{FL}(p,y) = \begin{cases} -\alpha (1-p)^{\gamma}\log (p) & \text{if} \  y=1 \\ - (1-\alpha) p^{\gamma}\log (1-p) & \text{otherwise} \end{cases} \tag{1}$$

å…¶ä¸­ï¼Œ$y \in \\{ \pm 1 \\}$è¡¨ç¤ºGTç±»åˆ«ï¼Œ$p \in [0,1]$è¡¨ç¤ºå‰æ™¯ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚å‰æ™¯ç±»åˆ«çš„è°ƒæ§å› å­ä¸º$(1-p)^{\gamma}$ï¼ŒèƒŒæ™¯ç±»åˆ«çš„è°ƒæ§å› å­ä¸º$p^{\gamma}$ï¼Œè¿™äº›è°ƒæ§å› å­å¯ä»¥å‡å°‘å®¹æ˜“æ ·æœ¬çš„æŸå¤±è´¡çŒ®ï¼Œå¢åŠ å¯¹å›°éš¾æ ·æœ¬çš„é‡è§†ã€‚

æˆ‘ä»¬å€Ÿé‰´äº†[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)çš„æ ·æœ¬åŠ æƒæ€æƒ³ï¼Œæ¥è§£å†³åœ¨å¯†é›†ç›®æ ‡æ£€æµ‹å™¨ä¸­ä½¿ç”¨IACSè®­ç»ƒæ—¶çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ç„¶è€Œï¼Œä¸[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)å¤„ç†æ­£è´Ÿæ ·æœ¬æ—¶çš„ä¸€è§†åŒä»ä¸åŒï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œäº†ä¸å¯¹ç§°å¤„ç†ã€‚Varifocal Lossçš„å®šä¹‰å¦‚ä¸‹ï¼š

$$\text{VFL}(p,q) = \begin{cases} -q(q\log (p) + (1-q)\log (1-p)) & q>0 \\ -\alpha p^{\gamma} \log (1-p) & q=0 \end{cases} \tag{2}$$

å…¶ä¸­ï¼Œ$p$æ˜¯é¢„æµ‹çš„IACSï¼Œ$q$æ˜¯ç›®æ ‡åˆ†æ•°ï¼ˆtarget scoreï¼‰ã€‚å¯¹äºå‰æ™¯ç‚¹ï¼ŒGTç±»åˆ«çš„$q$æ˜¯é¢„æµ‹bboxå’ŒGT bboxçš„IoUï¼Œä¸æ˜¯GTç±»åˆ«çš„$q$åˆ™ä¸º0ã€‚å¯¹äºèƒŒæ™¯ç‚¹ï¼Œæ‰€æœ‰ç±»åˆ«çš„$q$éƒ½æ˜¯0ã€‚å¦‚Fig1æ‰€ç¤ºã€‚

å¦‚å¼(2)æ‰€ç¤ºï¼ŒVarifocal Lossä»…é€šè¿‡$p^{\gamma}$æ¥å‡å°‘è´Ÿæ ·æœ¬ï¼ˆ$q=0$ï¼‰å¯¹æŸå¤±çš„è´¡çŒ®ï¼Œè€Œä¸ä¼šä»¥åŒæ ·çš„æ–¹å¼é™ä½æ­£æ ·æœ¬$q>0$çš„æƒé‡ï¼Œè¿™æ˜¯å› ä¸ºæ­£æ ·æœ¬ç›¸æ¯”è´Ÿæ ·æœ¬è¦å°‘å¾ˆå¤šï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™å®ƒä»¬å®è´µçš„å­¦ä¹ ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨$q$å¯¹æ­£æ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œå¦‚æœä¸€ä¸ªæ­£æ ·æœ¬çš„`gt_IoU`è¾ƒé«˜ï¼Œå®ƒå¯¹æŸå¤±çš„è´¡çŒ®å°†ç›¸å¯¹è¾ƒå¤§ï¼Œè¿™ä½¿å¾—è®­ç»ƒå°†é›†ä¸­åœ¨é«˜è´¨é‡çš„æ­£æ ·æœ¬ä¸Šã€‚

ä¸ºäº†å¹³è¡¡æ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„æŸå¤±ï¼Œæˆ‘ä»¬åœ¨è´Ÿæ ·æœ¬çš„æŸå¤±é¡¹ä¸­æ·»åŠ äº†å¯è°ƒçš„ç¼©æ”¾å› å­$\alpha$ã€‚

## 4.3.Star-Shaped Box Feature Representation

å¦‚Fig1æ‰€ç¤ºï¼Œçº¢è‰²bboxä¸ºé¢„æµ‹çš„åˆå§‹bboxï¼Œåœ¨[FCOS](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)ä¸­ï¼Œè¿™ä¸ªbboxå¯ä»¥ç”¨ä¸€ä¸ª4ç»´å‘é‡è¡¨ç¤ºï¼Œå³$(l',t',r',b')$ï¼Œåˆ†åˆ«è¡¨ç¤ºä½ç½®$(x,y)$åˆ°bboxçš„å·¦ã€ä¸Šã€å³ã€ä¸‹è¾¹ç•Œçš„è·ç¦»ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬é‡‡æ ·ä¹ä¸ªç‚¹ï¼Œå¦‚Fig1ä¸­çš„é»„è‰²ç‚¹æ‰€ç¤ºï¼Œå³$(x,y),(x-l',y),(x,y-t'),(x+r',y),(x,y+b'),(x-l',y-t'),(x+r',y-t'),(x-l',y+b'),(x+r',y+b')$ï¼Œç„¶åå°†è¿™ä¹ä¸ªç‚¹æ˜ å°„åˆ°feature mapä¸­ã€‚æ˜ å°„åï¼Œå‘¨å›´ç‚¹åˆ°$(x,y)$çš„offsetç”¨äºå®šä¹‰[å¯å˜å½¢å·ç§¯](http://shichaoxin.com/2024/07/25/è®ºæ–‡é˜…è¯»-Deformable-Convolutional-Networks/)ï¼Œè§Fig3ä¸­çš„Star Dconvæ­¥éª¤ã€‚è¿™äº›ç‚¹çš„é€‰æ‹©å¹¶æ²¡æœ‰å¼•å…¥é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚

## 4.4.Bounding Box Refinement

å¦‚Fig1æ‰€ç¤ºï¼ŒåŸºäºåˆå§‹çš„çº¢è‰²bboxï¼ˆ$(l',t',r',b')$ï¼‰ï¼Œæˆ‘ä»¬é¢„æµ‹äº†ä¸€ç»„ç¼©æ”¾å› å­$(\Delta l, \Delta t, \Delta r, \Delta b)$ï¼Œåˆ™refineåçš„è“è‰²bboxå¯è¡¨ç¤ºä¸º$(l,t,r,b)=(\Delta l \times l', \Delta t \times t', \Delta r \times r', \Delta b \times b')$ã€‚

## 4.5.VarifocalNet

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/4.png)

VFNetçš„backboneä»¥åŠFPNéƒ¨åˆ†å’Œ[FCOS](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)éƒ½æ˜¯ä¸€æ ·çš„ã€‚å’Œ[FCOS](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-FCOS-Fully-Convolutional-One-Stage-Object-Detection/)çš„ä¸åŒä¹‹å¤„åœ¨äºheadçš„ç»“æ„ã€‚VFNetçš„headåŒ…å«ä¸¤ä¸ªå­ç½‘ç»œã€‚å®šä½å­ç½‘ç»œç”¨äºbboxçš„å›å½’ä»¥åŠåç»­çš„refineã€‚å…¶è¾“å…¥æ˜¯æ¥è‡ªFPNæ¯ä¸ªå±‚çº§çš„feature mapï¼Œé¦–å…ˆæ˜¯3ä¸ª$3 \times 3$çš„å·ç§¯ï¼Œæ¿€æ´»å‡½æ•°ä¸ºReLUï¼Œé€šé“æ•°ä¸º256ã€‚åç»­çš„ç»“æ„å¦‚Fig3æ‰€ç¤ºï¼Œå·²ç»å¾ˆæ¸…æ™°äº†ï¼Œä¸å†èµ˜è¿°ã€‚

å¦ä¸€ä¸ªå­ç½‘ç»œç”¨äºé¢„æµ‹IACSï¼Œæ¯ä¸ªç©ºé—´ä½ç½®è¾“å‡ºä¸€ä¸ª$C$ç»´çš„å‘é‡ï¼ˆ$C$ä¸ºç±»åˆ«æ•°ï¼‰ï¼Œå‘é‡ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç›®æ ‡å­˜åœ¨ç½®ä¿¡åº¦å’Œå®šä½ç²¾åº¦çš„è”åˆè¡¨ç¤ºã€‚

## 4.6.Loss Function and Inference

ğŸ‘‰**Loss Function.**

VFNetè®­ç»ƒæ‰€ç”¨çš„æŸå¤±å‡½æ•°ä¸ºï¼š

$$\begin{align} Loss &= \frac{1}{N_{pos}} \sum_i \sum_c VFL (p_{c,i},q_{c,i}) \\&+ \frac{\lambda_0}{N_{pos}} \sum_i q_{c^*,i} L_{bbox} (bbox'_i,bbox^*_i) \\&+ \frac{\lambda_1}{N_{pos}} \sum_i q_{c^*,i} L_{bbox} (bbox_i,bbox^*_i) \end{align} \tag{3}$$

å…¶ä¸­ï¼Œ$p_{c,i}$å’Œ$q_{c,i}$åˆ†åˆ«è¡¨ç¤ºåœ¨FPNæ¯ä¸ªå±‚çº§çš„feature mapä¸­æ¯ä¸ªä½ç½®$i$ä¸Šç±»åˆ«$c$çš„é¢„æµ‹IACSå’Œç›®æ ‡åˆ†æ•°ã€‚$L_{bbox}$ä¸º[GIoU Loss](http://shichaoxin.com/2024/01/04/è®ºæ–‡é˜…è¯»-YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/#34yolov4)ï¼Œ$bbox'\_i$è¡¨ç¤ºé¢„æµ‹çš„åˆå§‹bboxï¼Œ$bbox\_i$è¡¨ç¤ºrefineåçš„bboxï¼Œ$bbox^\*\_i$è¡¨ç¤ºGT bboxã€‚$\lambda\_0$é€šå¸¸è®¾ä¸º1.5ï¼Œ$\lambda\_1$é€šå¸¸è®¾ä¸º2.0ã€‚$N\_{pos}$æ˜¯å‰æ™¯ç‚¹çš„æ•°é‡ã€‚å¦‚ç¬¬3éƒ¨åˆ†æ‰€ä»‹ç»çš„ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨[ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)æ¥å®šä¹‰å‰æ™¯ç‚¹å’ŒèƒŒæ™¯ç‚¹ã€‚

ğŸ‘‰**Inference.**

æ¨ç†é˜¶æ®µï¼Œè¾“å…¥å›¾åƒé€šè¿‡ç½‘ç»œçš„å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹ç»“æœï¼Œç„¶åä½¿ç”¨NMSç§»é™¤å†—ä½™çš„é¢„æµ‹ã€‚

# 5.Experiments

ğŸ‘‰**Dataset and Evaluation Metrics.**

æˆ‘ä»¬ä½¿ç”¨MS COCO 2017 benchmarkæ¥è¯„ä¼°VFNetã€‚åœ¨train2017ä¸Šè®­ç»ƒï¼Œåœ¨val2017ä¸Šè¿›è¡Œæ¶ˆèå®éªŒï¼Œåœ¨test-devä¸Šå’Œå…¶ä»–æ£€æµ‹å™¨è¿›è¡Œç»“æœæ¯”è¾ƒã€‚

ğŸ‘‰**Implementation and Training Details.**

ä½¿ç”¨MMDetectionå®ç°VFNetã€‚é™¤éç‰¹æ®Šå£°æ˜ï¼Œæˆ‘ä»¬ä½¿ç”¨MMDetectionä¸­çš„é»˜è®¤è¶…å‚æ•°ã€‚åˆå§‹å­¦ä¹ ç‡ä¸º0.01ï¼Œä½¿ç”¨çº¿æ€§warm-upç­–ç•¥ï¼Œwarm-up ratioè®¾ç½®ä¸º0.1ã€‚åœ¨æ¶ˆèå®éªŒå’Œæ€§èƒ½å¯¹æ¯”ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†8å—V100 GPUï¼Œæ€»çš„batch sizeä¸º16ï¼Œå³æ¯å—GPUå¤„ç†2å¼ å›¾åƒã€‚

åœ¨val2017ä¸Šçš„æ¶ˆèå®éªŒï¼Œä½¿ç”¨[ResNet-50](http://shichaoxin.com/2022/01/07/è®ºæ–‡é˜…è¯»-Deep-Residual-Learning-for-Image-Recognition/)ä½œä¸ºbackboneï¼Œå…±è®­ç»ƒäº†12ä¸ªepochï¼ˆ1x training scheduleï¼‰ã€‚è¾“å…¥å›¾åƒåœ¨ä¸æ”¹å˜é•¿å®½æ¯”çš„æƒ…å†µä¸‹ï¼Œæœ€å¤§resizeåˆ°$1333 \times 800$ã€‚æ•°æ®æ‰©å±•ä»…ç”¨äº†éšæœºæ°´å¹³ç¿»è½¬ã€‚

åœ¨test-devä¸Šï¼Œå’Œå…¶ä»–SOTAæ–¹æ³•è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„backboneè®­ç»ƒäº†VFNetï¼Œå…¶ä¸­æŸäº›backboneä½¿ç”¨äº†[å¯å˜å½¢å·ç§¯å±‚](http://shichaoxin.com/2024/07/25/è®ºæ–‡é˜…è¯»-Deformable-Convolutional-Networks/)ï¼ˆæ ‡è®°ä¸ºDCNï¼‰ã€‚å¦‚æœbackboneä½¿ç”¨äº†DCNï¼Œåˆ™æˆ‘ä»¬ä¹Ÿå°†å…¶æ’å…¥åˆ°æ˜Ÿå½¢å¯å˜å½¢å·ç§¯ä¹‹å‰çš„æœ€åä¸€å±‚ã€‚å…±è®­ç»ƒäº†24ä¸ªepochï¼ˆ2x training schemeï¼‰ï¼Œå¹¶ä½¿ç”¨äº†MSTrainï¼ˆmulti-scale trainingï¼‰ï¼Œæ¯æ¬¡è¿­ä»£æœ€å¤§çš„å›¾åƒå°ºå¯¸æ˜¯ä»ä¸€ä¸ªèŒƒå›´å†…éšæœºé€‰æ‹©çš„ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬åœ¨å®éªŒä¸­ä½¿ç”¨äº†2ä¸ªå›¾åƒå°ºå¯¸èŒƒå›´ã€‚ä¸ºäº†å’Œbaselineå…¬å¹³çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨çš„å°ºå¯¸èŒƒå›´ä¸º$1333 \times [640:800]$ï¼›æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®éªŒäº†æ›´å¹¿çš„å°ºå¯¸èŒƒå›´ï¼š$1333 \times [480:960]$ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ä½¿ç”¨äº†MSTrainï¼Œåœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬ä»ç„¶ä¿æŒå›¾åƒçš„æœ€å¤§å°ºå¯¸ä¸º$1333 \times 800$ï¼Œå°½ç®¡æ›´å¤§çš„å°ºå¯¸å¯ä»¥å¾—åˆ°ç¨å¾®å¥½ä¸€ç‚¹çš„æ€§èƒ½ï¼ˆ$1333 \times 900$çš„å°ºå¯¸å¯ä»¥å°†APæé«˜çº¦0.4ï¼‰ã€‚

ğŸ‘‰**Inference Details.**

åœ¨æ¨ç†é˜¶æ®µï¼Œè¾“å…¥å›¾åƒè¢«resizeï¼ˆæœ€å¤§å°ºå¯¸ä¸º$1333 \times 800$ï¼‰åé€å…¥ç½‘ç»œï¼Œå¾—åˆ°é¢„æµ‹çš„bboxå’Œå¯¹åº”çš„IACSã€‚æˆ‘ä»¬é¦–å…ˆè¿‡æ»¤æ‰$p_{max} \leqslant 0.05$çš„bboxï¼Œç„¶åæ¯ä¸ªFPNå±‚çº§é€‰æ‹©IACSåˆ†æ•°æœ€é«˜çš„1000ä¸ªbboxã€‚ç„¶åæ‰§è¡ŒNMSï¼ˆé˜ˆå€¼ä¸º0.6ï¼‰å¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚

## 5.1.Ablation Study

### 5.1.1.Varifocal Loss

æˆ‘ä»¬æµ‹è¯•äº†Varifocal Lossçš„ä¸¤ä¸ªè¶…å‚æ•°ï¼š$\alpha$å’Œ$\gamma$ã€‚æµ‹è¯•$\alpha$çš„å–å€¼èŒƒå›´ä¸º0.5åˆ°1.5ï¼Œæµ‹è¯•$\gamma$çš„å–å€¼èŒƒå›´ä¸º1.0åˆ°3.0ï¼Œè¡¨2åªå±•ç¤ºäº†æœ€ä¼˜$\alpha$çš„ç»“æœã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/5.png)

ä»è¡¨2å¯ä»¥çœ‹å‡ºï¼ŒVarifocal Losså¯¹$\alpha$å’Œ$\gamma$çš„å–å€¼å¹¶ä¸æ•æ„Ÿã€‚å½“$\alpha=0.75,\gamma=2.0$æ—¶æ€§èƒ½æœ€å¥½ï¼Œæˆ‘ä»¬åœ¨æ¥ä¸‹æ¥çš„å®éªŒä¸­ä¹Ÿé‡‡ç”¨è¿™ä¸¤ä¸ªå€¼ã€‚

### 5.1.2.Individual Component Contribution

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/6.png)

è¡¨3ä¸­ï¼Œraw VFNetæŒ‡çš„æ˜¯ç§»é™¤äº†centernessåˆ†æ”¯çš„[FCOS+ATSS](http://shichaoxin.com/2024/09/25/è®ºæ–‡é˜…è¯»-Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/)ï¼ˆè®­ç»ƒä½¿ç”¨[Focal Loss](http://shichaoxin.com/2024/02/22/è®ºæ–‡é˜…è¯»-Focal-Loss-for-Dense-Object-Detection/)ï¼‰ã€‚

## 5.2.Comparison with State-of-the-Art

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/7.png)

FPSçš„æµ‹è¯•åŸºäºNvidia V100 GPUã€‚

## 5.3.VarifocalNet-X

æˆ‘ä»¬è¿˜é’ˆå¯¹åŸå§‹çš„VFNetåšäº†ä¸€äº›æ‰©å±•ï¼Œç§°ä¸ºVFNet-Xï¼Œè¿™äº›æ‰©å±•åŒ…æ‹¬ï¼š

* å°†FPNæ›¿æ¢ä¸º[PAFPN](http://shichaoxin.com/2023/12/28/è®ºæ–‡é˜…è¯»-Path-Aggregation-Network-for-Instance-Segmentation/)ï¼Œå¹¶ä¸”ä½¿ç”¨äº†[DCN](http://shichaoxin.com/2024/07/25/è®ºæ–‡é˜…è¯»-Deformable-Convolutional-Networks/)å’Œ[group normalizationï¼ˆGNï¼‰](http://shichaoxin.com/2024/08/20/è®ºæ–‡é˜…è¯»-Group-Normalization/)ã€‚
* å°†headå †å çš„3ä¸ªå·ç§¯å±‚æ‰©å±•ä¸ºå †å 4ä¸ªå·ç§¯å±‚ï¼Œå°†é€šé“æ•°ä»256æ‰©å±•è‡³384ã€‚
* ä½¿ç”¨éšæœºè£å‰ªå’Œcutoutä½œä¸ºé¢å¤–çš„æ•°æ®æ‰©å±•æ–¹å¼ã€‚
* ä½¿ç”¨æ›´å¤§çš„MSTrainå°ºå¯¸èŒƒå›´ï¼Œå³ä»$750 \times 500$åˆ°$2100 \times 1400$ï¼Œåˆå§‹è®­ç»ƒ41ä¸ªepochã€‚
* åœ¨è®­ç»ƒVFNet-Xæ—¶ä½¿ç”¨äº†SWAï¼ˆstochastic weight averagingï¼‰æŠ€æœ¯ï¼Œè¿™ä½¿APæå‡äº†1.2ä¸ªç‚¹ã€‚åœ¨åˆå§‹è®­ç»ƒ41ä¸ªepochä¹‹åï¼Œåˆä½¿ç”¨cyclic learning rate scheduleè®­ç»ƒäº†18ä¸ªepochï¼Œç„¶åå°†è¿™18ä¸ªcheckpointsåšç®€å•çš„å¹³å‡å¾—åˆ°æˆ‘ä»¬æœ€ç»ˆçš„æ¨¡å‹ã€‚

>SWAè®ºæ–‡ï¼šPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.ã€‚

VFNet-Xåœ¨COCO test-devä¸Šçš„æ€§èƒ½è§è¡¨4ã€‚å½“åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨å°ºå¯¸$1333 \times 800$ï¼Œä¸”ä½¿ç”¨[soft-NMS](http://shichaoxin.com/2024/08/13/è®ºæ–‡é˜…è¯»-PP-YOLO-An-Effective-and-Efficient-Implementation-of-Object-Detector/#32selection-of-tricks)æ—¶ï¼ŒVFNet-X-800è¾¾åˆ°äº†53.7çš„APï¼Œè€Œå½“å°†å›¾åƒå°ºå¯¸å¢åŠ åˆ°$1800 \times 1200$æ—¶ï¼ŒVFNet-X-1200è¾¾åˆ°äº†SOTAçš„æˆç»©ï¼Œå³55.1çš„APã€‚ä¸€äº›å¯è§†åŒ–ç»“æœè§Fig4ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/8.png)

## 5.4.Generality and Superiority of Varifocal Loss

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/VFNet/9.png)

è¡¨5çš„ç»“æœåŸºäºval2017ã€‚è¿™äº›æ–¹æ³•çš„backboneéƒ½ä½¿ç”¨[ResNet-50](http://shichaoxin.com/2022/01/07/è®ºæ–‡é˜…è¯»-Deep-Residual-Learning-for-Image-Recognition/)ã€‚

# 6.Conclusion

ä¸å†èµ˜è¿°ã€‚

# 7.åŸæ–‡é“¾æ¥

ğŸ‘½[VarifocalNetï¼šAn IoU-aware Dense Object Detector](https://github.com/x-jeff/AI_Papers/blob/master/2024/VarifocalNetï¼šAn%20IoU-aware%20Dense%20Object%20Detector.pdf)