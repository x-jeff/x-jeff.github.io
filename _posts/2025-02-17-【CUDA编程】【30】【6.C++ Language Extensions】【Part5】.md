---
layout:     post
title:      ã€CUDAç¼–ç¨‹ã€‘ã€30ã€‘ã€6.C++ Language Extensionsã€‘ã€Part5ã€‘
subtitle:   Warp Reduce Functionsï¼ŒWarp Shuffle Functionsï¼ŒNanosleep Functionï¼ŒWarp Matrix Functionsï¼ŒDPX
date:       2025-02-17
author:     x-jeff
header-img: blogimg/20210203.jpg
catalog: true
tags:
    - CUDA C++ Programming Guide
---
>ã€CUDAç¼–ç¨‹ã€‘ç³»åˆ—åšå®¢å‚è€ƒNVIDIAå®˜æ–¹æ–‡æ¡£[â€œCUDA C++ Programming Guideï¼ˆv12.6ï¼‰â€](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)ã€‚  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Warp Reduce Functions

`__reduce_sync(unsigned mask, T value)`å†…ç½®å‡½æ•°åœ¨åŒæ­¥ç”±`mask`æŒ‡å®šçš„çº¿ç¨‹åï¼Œå¯¹æä¾›çš„`value`æ•°æ®æ‰§è¡Œå½’çº¦æ“ä½œï¼ˆreduction operationï¼‰ã€‚å¯¹äº`{add, min, max}`æ“ä½œï¼Œç±»å‹`T`å¯ä»¥æ˜¯æœ‰ç¬¦å·æˆ–æ— ç¬¦å·ï¼›å¯¹äº`{and, or, xor}`æ“ä½œï¼Œç±»å‹`T`åªèƒ½æ˜¯æ— ç¬¦å·ã€‚

ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›åœ¨8.xåŠä»¥ä¸Šçš„deviceã€‚

## 1.1.Synopsis

```c++
// add/min/max
unsigned __reduce_add_sync(unsigned mask, unsigned value);
unsigned __reduce_min_sync(unsigned mask, unsigned value);
unsigned __reduce_max_sync(unsigned mask, unsigned value);
int __reduce_add_sync(unsigned mask, int value);
int __reduce_min_sync(unsigned mask, int value);
int __reduce_max_sync(unsigned mask, int value);

// and/or/xor
unsigned __reduce_and_sync(unsigned mask, unsigned value);
unsigned __reduce_or_sync(unsigned mask, unsigned value);
unsigned __reduce_xor_sync(unsigned mask, unsigned value);
```

## 1.2.Description

ğŸ‘‰`__reduce_add_sync`ã€`__reduce_min_sync`ã€`__reduce_max_sync`ï¼š

å¯¹`mask`æŒ‡å®šçš„æ‰€æœ‰çº¿ç¨‹ä¸­çš„`value`æ‰§è¡ŒåŠ ã€æœ€å°å€¼æˆ–æœ€å¤§å€¼ç­‰ç®—æœ¯å½’çº¦æ“ä½œï¼Œå¹¶è¿”å›ç»“æœã€‚

ğŸ‘‰`__reduce_and_sync`ã€`__reduce_or_sync`ã€`__reduce_xor_sync`ï¼š

å¯¹`mask`æŒ‡å®šçš„æ‰€æœ‰çº¿ç¨‹ä¸­çš„`value`æ‰§è¡Œä¸ã€æˆ–ã€å¼‚æˆ–ç­‰é€»è¾‘å½’çº¦æ“ä½œï¼Œå¹¶è¿”å›ç»“æœã€‚

`mask`ç”¨äºæŒ‡å®šå“ªäº›çº¿ç¨‹å°†å‚ä¸æ“ä½œã€‚

ä»¥ä¸Šè¿™äº›å†…ç½®å‡½æ•°ä¸ä¿è¯å­˜åœ¨[å†…å­˜å±éšœ](https://shichaoxin.com/2025/01/14/CUDA%E7%BC%96%E7%A8%8B-26-6.C++-Language-Extensions-Part1/#5memory-fence-functions)ã€‚

# 2.Warp Shuffle Functions

`__shfl_sync`ã€`__shfl_up_sync`ã€`__shfl_down_sync`ã€`__shfl_xor_sync`ç”¨äºåœ¨ä¸€ä¸ªwarpå†…çš„çº¿ç¨‹ä¹‹é—´äº¤æ¢å˜é‡ã€‚

ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›åœ¨5.0åŠä»¥ä¸Šçš„deviceã€‚

`__shfl`ã€`__shfl_up`ã€`__shfl_down`ã€`__shfl_xor`åœ¨CUDA 9.0ä¸­å·²ç»è¢«å¼ƒç”¨ã€‚

å¯¹äºè®¡ç®—èƒ½åŠ›åœ¨7.xåŠä»¥ä¸Šçš„deviceï¼Œå°†ä¸å†æ”¯æŒ`__shfl`ã€`__shfl_up`ã€`__shfl_down`ã€`__shfl_xor`ï¼Œæ¨èä½¿ç”¨å®ƒä»¬å¯¹åº”çš„syncç‰ˆæœ¬ã€‚

## 2.1.Synopsis

```c++
T __shfl_sync(unsigned mask, T var, int srcLane, int width=warpSize);
T __shfl_up_sync(unsigned mask, T var, unsigned int delta, int width=warpSize);
T __shfl_down_sync(unsigned mask, T var, unsigned int delta, int width=warpSize);
T __shfl_xor_sync(unsigned mask, T var, int laneMask, int width=warpSize);
```

ç±»å‹`T`å¯ä»¥æ˜¯`int`ã€`unsigned int`ã€`long`ã€`unsigned long`ã€`long long`ã€`unsigned long long`ã€`float`æˆ–`double`ã€‚å½“åŒ…å«`cuda_fp16.h`å¤´æ–‡ä»¶æ—¶ï¼Œç±»å‹`T`å¯ä»¥æ˜¯`__half`æˆ–`__half2`ã€‚å½“åŒ…å«`cuda_bf16.h`å¤´æ–‡ä»¶æ—¶ï¼Œç±»å‹`T`å¯ä»¥æ˜¯`__nv_bfloat16`æˆ–`__nv_bfloat162`ã€‚

## 2.2.Description

`__shfl_sync()`å†…ç½®å‡½æ•°å…è®¸warpå†…çº¿ç¨‹ä¹‹é—´äº¤æ¢å˜é‡ï¼Œè€Œæ— éœ€ä½¿ç”¨å…±äº«å†…å­˜ã€‚äº¤æ¢åŒæ—¶å‘ç”Ÿåœ¨warpå†…çš„æ‰€æœ‰æ´»è·ƒçº¿ç¨‹ï¼ˆç”±`mask`æŒ‡å®šï¼‰ä¹‹é—´ã€‚æ ¹æ®æ•°æ®ç±»å‹ï¼Œæ¯ä¸ªçº¿ç¨‹å¯ä»¥ç§»åŠ¨4æˆ–8å­—èŠ‚çš„æ•°æ®ã€‚

ä¸€ä¸ªwarpå†…çš„çº¿ç¨‹ä¹Ÿè¢«ç§°ä¸ºlaneï¼Œå…¶ç´¢å¼•èŒƒå›´ä»0åˆ°`warpSize-1`ï¼ˆåŒ…å«ä¸¤ç«¯ï¼‰ã€‚å¯¹threadIndexã€warpIndexã€laneIndexçš„è§£é‡Šï¼ˆIndexå’ŒIDæ˜¯ä¸€ä¸ªæ„æ€ï¼‰ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/1.png)

è¿™é‡Œè¯´çš„laneIndexï¼ˆæˆ–ç§°ä¸ºlane IDï¼‰æŒ‡çš„æ˜¯ç‰©ç†lane IDï¼Œå…¶ç¼–å·å§‹ç»ˆæ˜¯ä»0åˆ°`warpSize-1`ã€‚æ³¨æ„å’Œä¸‹æ–‡æåˆ°çš„é€»è¾‘lane IDåŒºåˆ†ã€‚

æ¥ä¸‹æ¥è§£é‡Šä¸‹è¿™4ä¸ªå‡½æ•°éƒ½æœ‰çš„å‚æ•°`mask`å’Œ`width`ï¼š

* `mask`ç”¨äºæŒ‡å®šå“ªäº›çº¿ç¨‹æ˜¯æ´»è·ƒçš„ã€‚åªèƒ½ä»æ´»è·ƒçš„çº¿ç¨‹è¯»å–æ•°æ®ï¼Œå¦‚æœä»ä¸æ´»è·ƒçš„çº¿ç¨‹ä¸­è¯»å–æ•°æ®ï¼Œåˆ™è¯»å–å€¼æ˜¯æœªå®šä¹‰çš„ã€‚
* `width`ç”¨äºwarpå†…çº¿ç¨‹çš„åˆ†ç»„ã€‚`width`çš„å€¼åªèƒ½æ˜¯2çš„å¹‚ä¸”èŒƒå›´åœ¨`[1, warpSize]`å†…ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ`width`çš„å€¼åªèƒ½æ˜¯1ã€2ã€4ã€8ã€16ã€32ã€‚å¦‚æœ`width`æ˜¯å…¶ä»–å€¼ï¼Œåˆ™ç»“æœæ˜¯æœªå®šä¹‰çš„ã€‚`width`çš„é»˜è®¤å€¼ä¸º`warpSize`ï¼Œå³32ã€‚å¦‚æœ`width`çš„å€¼å°äº`warpSize`ï¼Œåˆ™warpå†…çš„çº¿ç¨‹ä¼šè¢«åˆ’åˆ†ä¸ºå¤šä¸ªå­ç»„ï¼Œæ¯ä¸ªå­ç»„å†…çº¿ç¨‹çš„é€»è¾‘lane IDèŒƒå›´ä¸º`[0:width-1]`ã€‚

ğŸ‘‰`__shfl_sync()`ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/2.png)

å¯¹åº”çš„ä»£ç ç¤ºä¾‹ï¼š

```c++
#include "cuda_runtime.h"
#include <stdio.h>

__global__ void test_shfl_sync() {
    int laneId = threadIdx.x % 32;  
    int x = threadIdx.x;            
    unsigned mask = 0xffffffff;
    int width = 16;

    //è¿”å›å€¼yä¸ºé€»è¾‘lane IDä¸º2çš„çº¿ç¨‹ä¸­å˜é‡xçš„å€¼
    int y = __shfl_sync(mask, x, 2, width); //å’Œint y = __shfl_sync(mask, x, 18, 16);ç»“æœä¸€æ ·

    printf("physical lane ID %d (logical lane ID %d): x=%d, y=%d\n", laneId, (laneId % width), x, y);
}

int main() {
    test_shfl_sync << <1, 32 >> > ();
    cudaDeviceSynchronize();
    return 0;
}
```

è¾“å‡ºä¸ºï¼š

```
physical lane ID 0 (logical lane ID 0): x=0, y=2
physical lane ID 1 (logical lane ID 1): x=1, y=2
physical lane ID 2 (logical lane ID 2): x=2, y=2
physical lane ID 3 (logical lane ID 3): x=3, y=2
physical lane ID 4 (logical lane ID 4): x=4, y=2
physical lane ID 5 (logical lane ID 5): x=5, y=2
physical lane ID 6 (logical lane ID 6): x=6, y=2
physical lane ID 7 (logical lane ID 7): x=7, y=2
physical lane ID 8 (logical lane ID 8): x=8, y=2
physical lane ID 9 (logical lane ID 9): x=9, y=2
physical lane ID 10 (logical lane ID 10): x=10, y=2
physical lane ID 11 (logical lane ID 11): x=11, y=2
physical lane ID 12 (logical lane ID 12): x=12, y=2
physical lane ID 13 (logical lane ID 13): x=13, y=2
physical lane ID 14 (logical lane ID 14): x=14, y=2
physical lane ID 15 (logical lane ID 15): x=15, y=2
physical lane ID 16 (logical lane ID 0): x=16, y=18
physical lane ID 17 (logical lane ID 1): x=17, y=18
physical lane ID 18 (logical lane ID 2): x=18, y=18
physical lane ID 19 (logical lane ID 3): x=19, y=18
physical lane ID 20 (logical lane ID 4): x=20, y=18
physical lane ID 21 (logical lane ID 5): x=21, y=18
physical lane ID 22 (logical lane ID 6): x=22, y=18
physical lane ID 23 (logical lane ID 7): x=23, y=18
physical lane ID 24 (logical lane ID 8): x=24, y=18
physical lane ID 25 (logical lane ID 9): x=25, y=18
physical lane ID 26 (logical lane ID 10): x=26, y=18
physical lane ID 27 (logical lane ID 11): x=27, y=18
physical lane ID 28 (logical lane ID 12): x=28, y=18
physical lane ID 29 (logical lane ID 13): x=29, y=18
physical lane ID 30 (logical lane ID 14): x=30, y=18
physical lane ID 31 (logical lane ID 15): x=31, y=18
```

ğŸ‘‰`__shfl_up_sync()`ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/3.png)

å¯¹åº”çš„ä»£ç ç¤ºä¾‹ï¼š

```c++
#include "cuda_runtime.h"
#include <stdio.h>

__global__ void test_shfl_sync() {
    int laneId = threadIdx.x % 32;  
    int x = threadIdx.x;            
    unsigned mask = 0xffffffff;
    int width = 16;

    int y = __shfl_up_sync(mask, x, 2, width);

    printf("physical lane ID %d (logical lane ID %d): x=%d, y=%d\n", laneId, (laneId % width), x, y);
}

int main() {
    test_shfl_sync << <1, 32 >> > ();
    cudaDeviceSynchronize();
    return 0;
}
```

è¾“å‡ºä¸ºï¼š

```
physical lane ID 0 (logical lane ID 0): x=0, y=0
physical lane ID 1 (logical lane ID 1): x=1, y=1
physical lane ID 2 (logical lane ID 2): x=2, y=0
physical lane ID 3 (logical lane ID 3): x=3, y=1
physical lane ID 4 (logical lane ID 4): x=4, y=2
physical lane ID 5 (logical lane ID 5): x=5, y=3
physical lane ID 6 (logical lane ID 6): x=6, y=4
physical lane ID 7 (logical lane ID 7): x=7, y=5
physical lane ID 8 (logical lane ID 8): x=8, y=6
physical lane ID 9 (logical lane ID 9): x=9, y=7
physical lane ID 10 (logical lane ID 10): x=10, y=8
physical lane ID 11 (logical lane ID 11): x=11, y=9
physical lane ID 12 (logical lane ID 12): x=12, y=10
physical lane ID 13 (logical lane ID 13): x=13, y=11
physical lane ID 14 (logical lane ID 14): x=14, y=12
physical lane ID 15 (logical lane ID 15): x=15, y=13
physical lane ID 16 (logical lane ID 0): x=16, y=16
physical lane ID 17 (logical lane ID 1): x=17, y=17
physical lane ID 18 (logical lane ID 2): x=18, y=16
physical lane ID 19 (logical lane ID 3): x=19, y=17
physical lane ID 20 (logical lane ID 4): x=20, y=18
physical lane ID 21 (logical lane ID 5): x=21, y=19
physical lane ID 22 (logical lane ID 6): x=22, y=20
physical lane ID 23 (logical lane ID 7): x=23, y=21
physical lane ID 24 (logical lane ID 8): x=24, y=22
physical lane ID 25 (logical lane ID 9): x=25, y=23
physical lane ID 26 (logical lane ID 10): x=26, y=24
physical lane ID 27 (logical lane ID 11): x=27, y=25
physical lane ID 28 (logical lane ID 12): x=28, y=26
physical lane ID 29 (logical lane ID 13): x=29, y=27
physical lane ID 30 (logical lane ID 14): x=30, y=28
physical lane ID 31 (logical lane ID 15): x=31, y=29
```

ğŸ‘‰`__shfl_down_sync()`ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/4.png)

å¯¹åº”çš„ä»£ç ç¤ºä¾‹ï¼š

```c++
#include "cuda_runtime.h"
#include <stdio.h>

__global__ void test_shfl_sync() {
    int laneId = threadIdx.x % 32;  
    int x = threadIdx.x;            
    unsigned mask = 0xffffffff;
    int width = 8;

    int y = __shfl_down_sync(mask, x, 3, width);

    printf("physical lane ID %d (logical lane ID %d): x=%d, y=%d\n", laneId, (laneId % width), x, y);
}

int main() {
    test_shfl_sync << <1, 32 >> > ();
    cudaDeviceSynchronize();
    return 0;
}
```

è¾“å‡ºä¸ºï¼š

```
physical lane ID 0 (logical lane ID 0): x=0, y=3
physical lane ID 1 (logical lane ID 1): x=1, y=4
physical lane ID 2 (logical lane ID 2): x=2, y=5
physical lane ID 3 (logical lane ID 3): x=3, y=6
physical lane ID 4 (logical lane ID 4): x=4, y=7
physical lane ID 5 (logical lane ID 5): x=5, y=5
physical lane ID 6 (logical lane ID 6): x=6, y=6
physical lane ID 7 (logical lane ID 7): x=7, y=7
physical lane ID 8 (logical lane ID 0): x=8, y=11
physical lane ID 9 (logical lane ID 1): x=9, y=12
physical lane ID 10 (logical lane ID 2): x=10, y=13
physical lane ID 11 (logical lane ID 3): x=11, y=14
physical lane ID 12 (logical lane ID 4): x=12, y=15
physical lane ID 13 (logical lane ID 5): x=13, y=13
physical lane ID 14 (logical lane ID 6): x=14, y=14
physical lane ID 15 (logical lane ID 7): x=15, y=15
physical lane ID 16 (logical lane ID 0): x=16, y=19
physical lane ID 17 (logical lane ID 1): x=17, y=20
physical lane ID 18 (logical lane ID 2): x=18, y=21
physical lane ID 19 (logical lane ID 3): x=19, y=22
physical lane ID 20 (logical lane ID 4): x=20, y=23
physical lane ID 21 (logical lane ID 5): x=21, y=21
physical lane ID 22 (logical lane ID 6): x=22, y=22
physical lane ID 23 (logical lane ID 7): x=23, y=23
physical lane ID 24 (logical lane ID 0): x=24, y=27
physical lane ID 25 (logical lane ID 1): x=25, y=28
physical lane ID 26 (logical lane ID 2): x=26, y=29
physical lane ID 27 (logical lane ID 3): x=27, y=30
physical lane ID 28 (logical lane ID 4): x=28, y=31
physical lane ID 29 (logical lane ID 5): x=29, y=29
physical lane ID 30 (logical lane ID 6): x=30, y=30
physical lane ID 31 (logical lane ID 7): x=31, y=31
```

ğŸ‘‰`__shfl_xor_sync()`ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/5.png)

å¯¹åº”çš„ä»£ç ç¤ºä¾‹ï¼š

```c++
#include "cuda_runtime.h"
#include <stdio.h>

__global__ void test_shfl_sync() {
    int laneId = threadIdx.x % 32;  
    int x = threadIdx.x;            
    unsigned mask = 0xffffffff;
    int width = 32;

    int y = __shfl_xor_sync(mask, x, 3, width);

    printf("physical lane ID %d (logical lane ID %d): x=%d, y=%d\n", laneId, (laneId % width), x, y);
}

int main() {
    test_shfl_sync << <1, 32 >> > ();
    cudaDeviceSynchronize();
    return 0;
}
```

è¾“å‡ºä¸ºï¼š

```
physical lane ID 0 (logical lane ID 0): x=0, y=3
physical lane ID 1 (logical lane ID 1): x=1, y=2
physical lane ID 2 (logical lane ID 2): x=2, y=1
physical lane ID 3 (logical lane ID 3): x=3, y=0
physical lane ID 4 (logical lane ID 4): x=4, y=7
physical lane ID 5 (logical lane ID 5): x=5, y=6
physical lane ID 6 (logical lane ID 6): x=6, y=5
physical lane ID 7 (logical lane ID 7): x=7, y=4
physical lane ID 8 (logical lane ID 8): x=8, y=11
physical lane ID 9 (logical lane ID 9): x=9, y=10
physical lane ID 10 (logical lane ID 10): x=10, y=9
physical lane ID 11 (logical lane ID 11): x=11, y=8
physical lane ID 12 (logical lane ID 12): x=12, y=15
physical lane ID 13 (logical lane ID 13): x=13, y=14
physical lane ID 14 (logical lane ID 14): x=14, y=13
physical lane ID 15 (logical lane ID 15): x=15, y=12
physical lane ID 16 (logical lane ID 16): x=16, y=19
physical lane ID 17 (logical lane ID 17): x=17, y=18
physical lane ID 18 (logical lane ID 18): x=18, y=17
physical lane ID 19 (logical lane ID 19): x=19, y=16
physical lane ID 20 (logical lane ID 20): x=20, y=23
physical lane ID 21 (logical lane ID 21): x=21, y=22
physical lane ID 22 (logical lane ID 22): x=22, y=21
physical lane ID 23 (logical lane ID 23): x=23, y=20
physical lane ID 24 (logical lane ID 24): x=24, y=27
physical lane ID 25 (logical lane ID 25): x=25, y=26
physical lane ID 26 (logical lane ID 26): x=26, y=25
physical lane ID 27 (logical lane ID 27): x=27, y=24
physical lane ID 28 (logical lane ID 28): x=28, y=31
physical lane ID 29 (logical lane ID 29): x=29, y=30
physical lane ID 30 (logical lane ID 30): x=30, y=29
physical lane ID 31 (logical lane ID 31): x=31, y=28
```

ä»¥`int y = __shfl_xor_sync(mask, x, 3, width);`ä¸ºä¾‹ï¼Œå‡è®¾çº¿ç¨‹é€»è¾‘lane IDä¸º4ï¼Œå¯¹åº”çš„äºŒè¿›åˆ¶ä¸º100ï¼Œå‚æ•°`laneMask`ä¸º3ï¼Œå¯¹åº”çš„äºŒè¿›åˆ¶ä¸º011ï¼ŒäºŒè€…æŒ‰ä½å¼‚æˆ–çš„ç»“æœæ˜¯111ï¼Œå³åè¿›åˆ¶çš„7ï¼Œä¹Ÿå°±æ˜¯è¯´é€»è¾‘lane IDä¸º4çš„çº¿ç¨‹å’Œé€»è¾‘lane IDä¸º7çš„çº¿ç¨‹è¿›è¡Œäº¤æ¢ã€‚

ä»¥ä¸Šè¿™äº›å†…ç½®å‡½æ•°ä¸ä¿è¯å­˜åœ¨[å†…å­˜å±éšœ](https://shichaoxin.com/2025/01/14/CUDA%E7%BC%96%E7%A8%8B-26-6.C++-Language-Extensions-Part1/#5memory-fence-functions)ã€‚

## 2.3.Examples

### 2.3.1.Broadcast of a single value across a warp

```c++
#include <stdio.h>

__global__ void bcast(int arg) {
    int laneId = threadIdx.x & 0x1f;
    int value;
    if (laneId == 0)        // Note unused variable for
        value = arg;        // all threads except lane 0
    value = __shfl_sync(0xffffffff, value, 0);   // Synchronize all threads in warp, and get "value" from lane 0
    if (value != arg)
        printf("Thread %d failed.\n", threadIdx.x);
}

int main() {
    bcast<<< 1, 32 >>>(1234);
    cudaDeviceSynchronize();

    return 0;
}
```

### 2.3.2.Inclusive plus-scan across sub-partitions of 8 threads

```c++
#include <stdio.h>

__global__ void scan4() {
    int laneId = threadIdx.x & 0x1f;
    // Seed sample starting value (inverse of lane ID)
    int value = 31 - laneId;

    // Loop to accumulate scan within my partition.
    // Scan requires log2(n) == 3 steps for 8 threads
    // It works by an accumulated sum up the warp
    // by 1, 2, 4, 8 etc. steps.
    for (int i=1; i<=4; i*=2) {
        // We do the __shfl_sync unconditionally so that we
        // can read even from threads which won't do a
        // sum, and then conditionally assign the result.
        int n = __shfl_up_sync(0xffffffff, value, i, 8);
        if ((laneId & 7) >= i)
            value += n;
    }

    printf("Thread %d final value = %d\n", threadIdx.x, value);
}

int main() {
    scan4<<< 1, 32 >>>();
    cudaDeviceSynchronize();

    return 0;
}
```

### 2.3.3.Reduction across a warp

```c++
#include <stdio.h>

__global__ void warpReduce() {
    int laneId = threadIdx.x & 0x1f;
    // Seed starting value as inverse lane ID
    int value = 31 - laneId;

    // Use XOR mode to perform butterfly reduction
    for (int i=16; i>=1; i/=2)
        value += __shfl_xor_sync(0xffffffff, value, i, 32);

    // "value" now contains the sum across all threads
    printf("Thread %d final value = %d\n", threadIdx.x, value);
}

int main() {
    warpReduce<<< 1, 32 >>>();
    cudaDeviceSynchronize();

    return 0;
}
```

# 3.Nanosleep Function

## 3.1.Synopsis

```c++
void __nanosleep(unsigned ns);
```

## 3.2.Description

`__nanosleep(ns)`ä½¿çº¿ç¨‹æš‚åœå¤§çº¦`ns`çº³ç§’çš„æ—¶é—´ã€‚æœ€å¤§ç¡çœ æ—¶é—´å¤§çº¦ä¸º1æ¯«ç§’ã€‚

è¯¥å‡½æ•°åªæ”¯æŒè®¡ç®—èƒ½åŠ›åœ¨7.0åŠä»¥ä¸Šçš„deviceã€‚

## 3.3.Example

```c++
__device__ void mutex_lock(unsigned int *mutex) {
    unsigned int ns = 8;
    while (atomicCAS(mutex, 0, 1) == 1) {
        __nanosleep(ns);
        if (ns < 256) {
            ns *= 2;
        }
    }
}

__device__ void mutex_unlock(unsigned int *mutex) {
    atomicExch(mutex, 0);
}
```

# 4.Warp Matrix Functions

C++ warpçŸ©é˜µæ“ä½œåˆ©ç”¨[Tensoræ ¸å¿ƒ](https://shichaoxin.com/2024/09/10/CUDA%E7%BC%96%E7%A8%8B-1-1.Introduction/#3a-scalable-programming-model)åŠ é€Ÿå½¢å¦‚`D=A*B+C`çš„çŸ©é˜µè¿ç®—ã€‚è¿™äº›æ“ä½œæ”¯æŒæ··åˆç²¾åº¦æµ®ç‚¹æ•°æ®ï¼Œé€‚ç”¨äºè®¡ç®—èƒ½åŠ›7.0åŠä»¥ä¸Šçš„deviceã€‚è¿™éœ€è¦ä¸€ä¸ªwarpå†…çš„æ‰€æœ‰çº¿ç¨‹ååŒæ“ä½œã€‚æ­¤å¤–ï¼Œè¿™äº›æ“ä½œä»…åœ¨æ•´ä¸ªwarpä¸­çš„æ¡ä»¶è®¡ç®—ç»“æœç›¸åŒçš„æƒ…å†µä¸‹æ‰èƒ½åœ¨æ¡ä»¶ä»£ç ä¸­è¿è¡Œï¼Œå¦åˆ™ä»£ç æ‰§è¡Œå¯èƒ½ä¼šå¡ä½ã€‚

## 4.1.Description

ä»¥ä¸‹æ‰€æœ‰å‡½æ•°å’Œç±»å‹éƒ½å®šä¹‰åœ¨`nvcuda::wmma`å‘½åç©ºé—´ä¸­ã€‚å­å­—èŠ‚æ“ä½œï¼ˆsub-byte operationsï¼ŒæŒ‡æ“ä½œçš„æ•°æ®ç²’åº¦å°äº8æ¯”ç‰¹ï¼‰å°šä¸ç¨³å®šï¼Œå³å…¶æ•°æ®ç»“æ„å’ŒAPIå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä¸”åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å¯èƒ½ä¸å…¼å®¹ã€‚æ­¤é™„åŠ åŠŸèƒ½å®šä¹‰åœ¨`nvcuda::wmma::experimental`å‘½åç©ºé—´ä¸­ã€‚

```c++
template<typename Use, int m, int n, int k, typename T, typename Layout=void> class fragment;

void load_matrix_sync(fragment<...> &a, const T* mptr, unsigned ldm);
void load_matrix_sync(fragment<...> &a, const T* mptr, unsigned ldm, layout_t layout);
void store_matrix_sync(T* mptr, const fragment<...> &a, unsigned ldm, layout_t layout);
void fill_fragment(fragment<...> &a, const T& v);
void mma_sync(fragment<...> &d, const fragment<...> &a, const fragment<...> &b, const fragment<...> &c, bool satf=false);
```

`fragment`æ˜¯ä¸€ä¸ªé‡è½½çš„ç±»ï¼ˆå‚è§ï¼š[ç±»æ¨¡æ¿](https://shichaoxin.com/2024/02/18/C++%E5%9F%BA%E7%A1%80-%E7%AC%AC%E4%B9%9D%E5%8D%81%E4%B8%89%E8%AF%BE-%E6%A8%A1%E6%9D%BF%E4%B8%8E%E6%B3%9B%E5%9E%8B%E7%BC%96%E7%A8%8B-%E5%AE%9A%E4%B9%89%E6%A8%A1%E6%9D%BF/#3%E7%B1%BB%E6%A8%A1%E6%9D%BF)ï¼‰ï¼ŒåŒ…å«ä¸€ä¸ªçŸ©é˜µç‰‡æ®µï¼Œè¯¥çŸ©é˜µç‰‡æ®µåˆ†å¸ƒåœ¨warpä¸­çš„æ‰€æœ‰çº¿ç¨‹ä¸Šã€‚çŸ©é˜µå…ƒç´ åœ¨`fragment`ä¸­çš„å†…éƒ¨å­˜å‚¨æ–¹å¼æœªæŒ‡å®šï¼Œå¹¶å¯èƒ½åœ¨æœªæ¥æ¶æ„ä¸­å‘ç”Ÿå˜åŒ–ã€‚

åªæœ‰æŸäº›æ¨¡æ¿å‚æ•°ç»„åˆæ˜¯å…è®¸çš„ã€‚ç¬¬ä¸€ä¸ªå‚æ•°`Use`çš„å€¼å¯ä»¥æ˜¯ï¼š

* `matrix_a`ï¼šè¡¨ç¤º`D=A*B+C`ä¸­çš„çŸ©é˜µ`A`ï¼Œç»´åº¦ä¸º`m x k`ã€‚
* `matrix_b`ï¼šè¡¨ç¤º`D=A*B+C`ä¸­çš„çŸ©é˜µ`B`ï¼Œç»´åº¦ä¸º`k x n`ã€‚
* `accumulator`ï¼šè¡¨ç¤º`D=A*B+C`ä¸­çš„çŸ©é˜µ`C`æˆ–çŸ©é˜µ`D`ï¼Œç»´åº¦ä¸º`m x n`ã€‚

å‚æ•°`T`è¡¨ç¤ºæ•°æ®ç±»å‹ï¼Œå¯ä»¥æ˜¯ï¼š

* é€‚ç”¨äº`matrix_a`æˆ–`matrix_b`ï¼š`double`ã€`float`ã€`__half`ã€`__nv_bfloat16`ã€`char`ã€`unsigned char`ã€‚
* é€‚ç”¨äº`accumulator`ï¼š`double`ã€`float`ã€`int`ã€`__half`ã€‚

`matrix_a`ã€`matrix_b`ã€`accumulator`çš„æ•°æ®ç±»å‹ç»„åˆä»¥åŠå¯¹åº”çš„çŸ©é˜µå¤§å°æ˜¯æœ‰é™åˆ¶çš„ï¼Œè¯¦è§ç¬¬4.6éƒ¨åˆ†ã€‚å½“`Use`ä¸º`matrix_a`æˆ–`matrix_b`æ—¶ï¼Œåœ¨å®šä¹‰`matrix_a`çŸ©é˜µæˆ–`matrix_b`çŸ©é˜µæ—¶ï¼Œå¿…é¡»æŒ‡å®šå‚æ•°`Layout`ã€‚å‚æ•°`Layout`å¯ä»¥æ˜¯ï¼š

* `row_major`ï¼šçŸ©é˜µè¡Œä¸­çš„å…ƒç´ åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚
* `col_major`ï¼šçŸ©é˜µåˆ—ä¸­çš„å…ƒç´ åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚

å½“`Use`ä¸º`accumulator`æ—¶ï¼Œå¦‚æœæ˜¯å®šä¹‰`accumulator`çŸ©é˜µï¼Œåˆ™å‚æ•°`Layout`åº”ä¸º`void`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œä½†å¦‚æœåŠ è½½æˆ–å­˜å‚¨`accumulator`çŸ©é˜µæ—¶ï¼ˆå³è°ƒç”¨`load_matrix_sync`æˆ–`store_matrix_sync`æ—¶ï¼‰ï¼Œåˆ™éœ€è¦æŒ‡å®šå‚æ•°`Layout`ã€‚

`load_matrix_sync`å‡½æ•°ä¼šåœ¨æ‰€æœ‰warp lanesåˆ°è¾¾`load_matrix_sync`è°ƒç”¨ç‚¹åï¼Œä»å†…å­˜ä¸­åŠ è½½çŸ©é˜µç‰‡æ®µã€‚`mptr`å¿…é¡»æ˜¯ä¸€ä¸ª256ä½å¯¹é½çš„æŒ‡é’ˆï¼ŒæŒ‡å‘çŸ©é˜µä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚`ldm`ä¸ºè¿ç»­è¡Œï¼ˆé’ˆå¯¹row major layoutï¼‰æˆ–è¿ç»­åˆ—ï¼ˆé’ˆå¯¹column major layoutï¼‰ä¹‹é—´çš„æ­¥é•¿ï¼Œå¦‚æœçŸ©é˜µå…ƒç´ ç±»å‹ä¸º`__half`ï¼Œåˆ™`ldm`å¿…é¡»æ˜¯8çš„å€æ•°ï¼›å¦‚æœçŸ©é˜µå…ƒç´ ç±»å‹ä¸º`float`ï¼Œåˆ™`ldm`å¿…é¡»æ˜¯4çš„å€æ•°ã€‚å¦‚æœåŠ è½½`accumulator`çŸ©é˜µï¼Œå¿…é¡»æŒ‡å®šå‚æ•°`layout`ä¸º`mem_row_major`æˆ–`mem_col_major`ã€‚å¦‚æœåŠ è½½`matrix_a`çŸ©é˜µæˆ–`matrix_b`çŸ©é˜µï¼Œåˆ™å‚æ•°`layout`ä¼šä»å®šä¹‰è¯¥çŸ©é˜µæ—¶æŒ‡å®šçš„layoutæ¨æ–­å‡ºæ¥ã€‚`mptr`ã€`ldm`ã€`layout`ä»¥åŠ`a`ä¸­æ‰€æœ‰çš„æ¨¡æ¿å‚æ•°å¿…é¡»åœ¨warpçš„æ‰€æœ‰çº¿ç¨‹ä¸­ä¿æŒä¸€è‡´ã€‚`load_matrix_sync`å¿…é¡»è¢«warpä¸­çš„æ‰€æœ‰çº¿ç¨‹æ‰€è°ƒç”¨ï¼Œå¦åˆ™ç»“æœæ˜¯æœªå®šä¹‰çš„ã€‚

`store_matrix_sync`å‡½æ•°ä¼šåœ¨æ‰€æœ‰warp lanesåˆ°è¾¾`store_matrix_sync`è°ƒç”¨ç‚¹åï¼Œå°†çŸ©é˜µç‰‡æ®µä¿å­˜è‡³å†…å­˜ä¸­ã€‚`mptr`å¿…é¡»æ˜¯ä¸€ä¸ª256ä½å¯¹é½çš„æŒ‡é’ˆï¼ŒæŒ‡å‘çŸ©é˜µä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚`ldm`ä¸ºè¿ç»­è¡Œï¼ˆé’ˆå¯¹row major layoutï¼‰æˆ–è¿ç»­åˆ—ï¼ˆé’ˆå¯¹column major layoutï¼‰ä¹‹é—´çš„æ­¥é•¿ï¼Œå¦‚æœçŸ©é˜µå…ƒç´ ç±»å‹ä¸º`__half`ï¼Œåˆ™`ldm`å¿…é¡»æ˜¯8çš„å€æ•°ï¼›å¦‚æœçŸ©é˜µå…ƒç´ ç±»å‹ä¸º`float`ï¼Œåˆ™`ldm`å¿…é¡»æ˜¯4çš„å€æ•°ã€‚è¾“å‡ºçŸ©é˜µçš„layoutå¿…é¡»æ˜¯`mem_row_major`æˆ–`mem_col_major`ã€‚`mptr`ã€`ldm`ã€`layout`ä»¥åŠ`a`ä¸­æ‰€æœ‰çš„æ¨¡æ¿å‚æ•°å¿…é¡»åœ¨warpçš„æ‰€æœ‰çº¿ç¨‹ä¸­ä¿æŒä¸€è‡´ã€‚

`fill_fragment`ç”¨å¸¸é‡å€¼`v`å¡«å……ä¸€ä¸ªçŸ©é˜µç‰‡æ®µã€‚ç”±äºçŸ©é˜µå…ƒç´ åœ¨çŸ©é˜µç‰‡æ®µä¸­çš„æ˜ å°„æ–¹å¼æœªæŒ‡å®šï¼Œå› æ­¤è¯¥å‡½æ•°é€šå¸¸éœ€è¦ç”±warpä¸­çš„æ‰€æœ‰çº¿ç¨‹è°ƒç”¨ï¼Œå¹¶ä¸”ä½¿ç”¨ç›¸åŒçš„å¸¸é‡å€¼`v`ã€‚

`mma_sync`å‡½æ•°åœ¨ç­‰å¾…æ‰€æœ‰warp lanesåˆ°è¾¾`mma_sync`è°ƒç”¨ç‚¹åï¼Œæ‰§è¡ŒwarpåŒæ­¥çŸ©é˜µçš„ä¹˜-åŠ è¿ç®—`D=A*B+C`ã€‚ä¹Ÿæ”¯æŒ`C=A*B+C`è¿ç®—ã€‚`satf`çš„å€¼ä»¥åŠæ¯ä¸ªçŸ©é˜µç‰‡æ®µçš„æ¨¡æ¿å‚æ•°åœ¨warpçš„æ‰€æœ‰çº¿ç¨‹ä¸­å¿…é¡»éƒ½æ˜¯ä¸€æ ·çš„ã€‚`mma_sync`å¿…é¡»è¢«warpä¸­çš„æ‰€æœ‰çº¿ç¨‹æ‰€è°ƒç”¨ï¼Œå¦åˆ™ç»“æœæ˜¯æœªå®šä¹‰çš„ã€‚

å½“`satf`ï¼ˆsaturate to finite valueï¼‰ä¸º`true`æ—¶ï¼ŒçŸ©é˜µåŠ æ“ä½œä¼šéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

* å½“çŸ©é˜µå…ƒç´ çš„å€¼ä¸ºæ­£æ— ç©·æ—¶ï¼Œä¿å­˜ä¸º`+MAX_NORM`ã€‚
* å½“çŸ©é˜µå…ƒç´ çš„å€¼ä¸ºè´Ÿæ— ç©·æ—¶ï¼Œä¿å­˜ä¸º`-MAX_NORM`ã€‚
* å½“çŸ©é˜µå…ƒç´ çš„å€¼ä¸ºNaNæ—¶ï¼Œä¿å­˜ä¸º`+0`ã€‚

ç”±äºæ¯ä¸ªçº¿ç¨‹çš„`fragment`ä¸­çŸ©é˜µå…ƒç´ çš„æ˜ å°„æ–¹å¼æœªæŒ‡å®šï¼Œæ‰€ä»¥å¿…é¡»åœ¨è°ƒç”¨`store_matrix_sync`åï¼Œä»å†…å­˜ï¼ˆå…±äº«å†…å­˜æˆ–å…¨å±€å†…å­˜ï¼‰ä¸­è®¿é—®å•ç‹¬çš„çŸ©é˜µå…ƒç´ ã€‚è®¿é—®å•ä¸ªå…ƒç´ ï¼š

```c++
enum fragment<Use, m, n, k, T, Layout>::num_elements;
T fragment<Use, m, n, k, T, Layout>::x[num_elements];
```

ä¸€ä¸ªä¾‹å­ï¼š

```c++
wmma::fragment<wmma::accumulator, 16, 16, 16, float> frag;
float alpha = 0.5f; // Same value for all threads in warp
/*...*/
for(int t=0; t<frag.num_elements; t++)
frag.x[t] *= alpha;
```

## 4.2.Alternate Floating Point

Tensor Coresåœ¨è®¡ç®—èƒ½åŠ›8.0åŠæ›´é«˜çš„deviceä¸Šæ”¯æŒå¤šç§æµ®ç‚¹è¿ç®—ç±»å‹ã€‚

`__nv_bfloat16`

è¿™ç§æ•°æ®æ ¼å¼æ˜¯fp16ï¼ˆåŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰çš„æ›¿ä»£æ ¼å¼ï¼Œå®ƒçš„æ•°å€¼èŒƒå›´ä¸f32ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰ç›¸åŒï¼Œä½†ç²¾åº¦é™ä½ï¼ˆä»…7ä½ç²¾åº¦ï¼‰ã€‚å¼•å…¥`cuda_bf16.h`å¤´æ–‡ä»¶åå¯ä»¥ç›´æ¥ä½¿ç”¨`__nv_bfloat16`ç±»å‹ã€‚`matrix_a`çŸ©é˜µå’Œ`matrix_b`çŸ©é˜µå¯ä»¥ä½¿ç”¨`__nv_bfloat16`ç±»å‹ï¼Œä½†æ­¤æ—¶`accumulator`çŸ©é˜µå¿…é¡»æ˜¯`float`ç±»å‹ã€‚`__nv_bfloat16`æ”¯æŒçš„çŸ©é˜µå¤§å°å’Œæ“ä½œä¸`__half`ç›¸åŒã€‚

`tf32`

è¿™ç§æ•°æ®æ ¼å¼æ˜¯Tensor Coresæ”¯æŒçš„ä¸€ç§ç‰¹æ®Šçš„æµ®ç‚¹ç±»å‹ï¼Œå®ƒçš„æ•°å€¼èŒƒå›´ä¸f32ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰ç›¸åŒï¼Œä½†ç²¾åº¦é™ä½ï¼ˆå¤§äºç­‰äº10ä½ç²¾åº¦ï¼‰ã€‚`tf32`æ ¼å¼çš„å†…éƒ¨å­˜å‚¨æ–¹å¼ç”±ç¡¬ä»¶å®ç°å†³å®šï¼Œç”¨æˆ·æ— æ³•ç›´æ¥æ§åˆ¶ã€‚å¦‚æœæƒ³åœ¨WMMAï¼ˆWarp Matrix Multiply-Accumulateï¼‰æ“ä½œä¸­ä½¿ç”¨è¿™ç§æ•°æ®æ ¼å¼ï¼Œå¿…é¡»æ‰‹åŠ¨å°†è¾“å…¥çŸ©é˜µè½¬æ¢ä¸º`tf32`æ ¼å¼ã€‚

ä¸ºäº†æ–¹ä¾¿è¿™ç§è½¬æ¢ï¼Œæä¾›äº†å†…ç½®å‡½æ•°`__float_to_tf32`ã€‚è¯¥å†…ç½®å‡½æ•°çš„è¾“å…¥å’Œè¾“å‡ºçš„æ•°æ®ç±»å‹éƒ½æ˜¯`float`ï¼Œä½†æ•°å€¼è®¡ç®—å®é™…ä¸Šæ˜¯ä»¥`tf32`ç²¾åº¦æ‰§è¡Œçš„ã€‚æ³¨æ„ï¼Œè¯¥ç²¾åº¦ä»…èƒ½åœ¨Tensor Coresä¸Šä½¿ç”¨ï¼Œä¸”ä¸èƒ½ä¸å…¶ä»–`float`ç±»å‹æ··ç”¨ï¼Œå¦åˆ™ç»“æœçš„ç²¾åº¦å’ŒèŒƒå›´æ˜¯æœªå®šä¹‰çš„ã€‚

å¦‚æœè¾“å…¥çŸ©é˜µï¼ˆ`matrix_a`æˆ–`matrix_b`ï¼‰ä½¿ç”¨äº†tf32ç²¾åº¦ï¼Œé‚£ä¹ˆ`accumulator`å°±å¿…é¡»ä½¿ç”¨`float`ç±»å‹ï¼Œä¸”åªæ”¯æŒçŸ©é˜µå¤§å°ä¸º$16 \times 16 \times 8$ï¼ˆm-n-kï¼‰ï¼Œå…·ä½“è§`mma.h`å¤´æ–‡ä»¶ä¸­çš„ç›¸å…³ä»£ç ï¼š

```c++
  template<> class fragment<matrix_a, 16, 16, 8, precision::tf32, row_major> : public __frag_base<precision::tf32, 4> {};
  template<> class fragment<matrix_a, 16, 16, 8, precision::tf32, col_major> : public __frag_base<precision::tf32, 4> {};
  template<> class fragment<matrix_b, 16, 16, 8, precision::tf32, row_major> : public __frag_base<precision::tf32, 4> {};
  template<> class fragment<matrix_b, 16, 16, 8, precision::tf32, col_major> : public __frag_base<precision::tf32, 4> {};
  template<> class fragment<accumulator, 16, 16, 8, float> : public __frag_base<float, 8> {};
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ•°æ®ä»ç„¶æ˜¯æŒ‰ç…§`float`æ ¼å¼å­˜å‚¨çš„ï¼Œä½†å®é™…è®¡ç®—æ—¶çš„æ ¼å¼ä¸º`tf32`ã€‚è°ƒç”¨`storage_element_type<T>`å¯ä»¥å¾—åˆ°å†…éƒ¨å­˜å‚¨æ‰€ç”¨çš„æ•°æ®æ ¼å¼ï¼Œå³`float`ã€‚è°ƒç”¨`element_type<T>`å¯ä»¥å¾—åˆ°å®é™…è®¡ç®—æ—¶æ‰€ç”¨çš„æ•°æ®æ ¼å¼ï¼Œå³`tf32`ã€‚

## 4.3.Double Precision

åœ¨è®¡ç®—èƒ½åŠ›8.0åŠä»¥ä¸Šçš„deviceä¸­ï¼ŒTensor Coresæ”¯æŒåŒç²¾åº¦æµ®ç‚¹è¿ç®—ã€‚è¦ä½¿ç”¨è¿™ä¸ªæ–°åŠŸèƒ½ï¼Œ`fragment`å¿…é¡»æŒ‡å®šæ•°æ®ç±»å‹ä¸º`double`ã€‚`mma_sync`æ“ä½œå°†éµå¾ª.rnè§„åˆ™ã€‚.rnå…¨ç§°ä¸ºrounds to nearest evenï¼Œå…¶è§„åˆ™æ˜¯å››èˆå…­å…¥äº”å–æœ€è¿‘çš„å¶æ•°ï¼Œæ¯”å¦‚ï¼š

```
roundeven(+2.4) = +2.0
roundeven(-2.4) = -2.0
roundeven(+2.5) = +2.0
roundeven(-2.5) = -2.0
roundeven(+2.6) = +3.0
roundeven(-2.6) = -3.0
roundeven(+3.5) = +4.0
roundeven(-3.5) = -4.0
roundeven(-0.0) = -0.0
roundeven(-Inf) = -inf
```

## 4.4.Sub-byte Operations

å­å­—èŠ‚WMMAè¿ç®—å¯ä»¥åˆ©ç”¨Tensor Coresçš„ä½ç²¾åº¦è®¡ç®—èƒ½åŠ›ï¼Œå…¶å®šä¹‰åœ¨`nvcuda::wmma::experimental`å‘½åç©ºé—´ä¸­ï¼š

```c++
namespace experimental {
    namespace precision {
        struct u4; // 4-bit unsigned
        struct s4; // 4-bit signed
        struct b1; // 1-bit
   }
    enum bmmaBitOp {
        bmmaBitOpXOR = 1, // compute_75 minimum
        bmmaBitOpAND = 2  // compute_80 minimum
    };
    enum bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 };
}
```

å¯¹äº4æ¯”ç‰¹ç²¾åº¦ï¼Œå¯ç”¨çš„APIä¿æŒä¸å˜ï¼Œä½†å¿…é¡»æ˜¾å¼æŒ‡å®š`experimental::precision::u4`æˆ–`experimental::precision::s4`ä½œä¸ºfragmentçš„æ•°æ®ç±»å‹ã€‚ç”±äºfragmentå†…çš„å…ƒç´ æ˜¯ç´§å‡‘å­˜å‚¨çš„ï¼Œå› æ­¤`num_storage_elements`å°äº`num_elements`ã€‚`num_elements`è¡¨ç¤ºå­å­—èŠ‚æ•°æ®ç±»å‹çš„æ€»å…ƒç´ æ•°é‡ã€‚ä»`element_type<T>`åˆ°`storage_element_type<T>`çš„æ˜ å°„å¦‚ä¸‹ï¼š

```c++
experimental::precision::u4 -> unsigned (8 elements in 1 storage element) //8ä¸ª4æ¯”ç‰¹çš„å…ƒç´ å­˜å‚¨åœ¨1ä¸ª32æ¯”ç‰¹çš„å­˜å‚¨å•å…ƒä¸­
experimental::precision::s4 -> int (8 elements in 1 storage element) //8ä¸ª4æ¯”ç‰¹çš„å…ƒç´ å­˜å‚¨åœ¨1ä¸ª32æ¯”ç‰¹çš„å­˜å‚¨å•å…ƒä¸­
experimental::precision::b1 -> unsigned (32 elements in 1 storage element) //32ä¸ª1æ¯”ç‰¹å…ƒç´ å­˜å‚¨åœ¨1ä¸ª32æ¯”ç‰¹çš„å­˜å‚¨å•å…ƒä¸­
T -> T  //all other types
```

å¯¹äºå­å­—èŠ‚fragmentçš„layoutï¼Œ`matrix_a`å¿…é¡»ä½¿ç”¨`row_major`ï¼Œ`matrix_b`å¿…é¡»ä½¿ç”¨`col_major`ã€‚

å¯¹äºå­å­—èŠ‚è¿ç®—ï¼Œ`load_matrix_sync`ä¸­çš„`ldm`å‚æ•°å¿…é¡»æ»¡è¶³ï¼š

* å¯¹äº`experimental::precision::u4`å’Œ`experimental::precision::s4`ï¼Œ`ldm`å¿…é¡»æ˜¯32çš„å€æ•°ã€‚
* å¯¹äº`experimental::precision::b1`ï¼Œ`ldm`å¿…é¡»æ˜¯128çš„å€æ•°ã€‚

>æ³¨æ„ï¼š
>
>å¦‚ä¸‹MMAæŒ‡ä»¤å°†åœ¨`sm_90`ä¸­è¢«åºŸé™¤ï¼š
>
>* `experimental::precision::u4`
>* `experimental::precision::s4`
>* å½“`bmmaBitOp`è®¾ç½®ä¸º`bmmaBitOpXOR`æ—¶ï¼Œ`experimental::precision::b1`

`bmma_sync`å‡½æ•°åœ¨ç­‰å¾…æ‰€æœ‰warp lanesåˆ°è¾¾`bmma_sync`è°ƒç”¨ç‚¹åï¼Œæ‰§è¡ŒwarpåŒæ­¥æ¯”ç‰¹çŸ©é˜µçš„ä¹˜-åŠ è¿ç®—`D = (A op B) + C`ï¼Œå…¶ä¸­ï¼Œ`op`ç”±é€»è¾‘è¿ç®—`bmmaBitOp`å’Œ`bmmaAccumulateOp`å®šä¹‰çš„ç´¯åŠ æ“ä½œç»„æˆã€‚å¯ç”¨çš„æ“ä½œæœ‰ï¼š

* `bmmaBitOpXOR`ï¼š`matrix_a`çš„128æ¯”ç‰¹çš„è¡Œä¸`matrix_b`çš„128æ¯”ç‰¹çš„åˆ—è¿›è¡ŒæŒ‰ä½å¼‚æˆ–æ“ä½œã€‚
* `bmmaBitOpAND`ï¼š`matrix_a`çš„128æ¯”ç‰¹çš„è¡Œä¸`matrix_b`çš„128æ¯”ç‰¹çš„åˆ—è¿›è¡ŒæŒ‰ä½ä¸æ“ä½œï¼Œä»…æ”¯æŒè®¡ç®—èƒ½åŠ›åœ¨8.0åŠä»¥ä¸Šçš„deviceã€‚
* ç´¯åŠ æ“ä½œé€šå¸¸æ˜¯`bmmaAccumulateOpPOPC`ï¼Œå³ç»Ÿè®¡1çš„æ•°é‡ã€‚

## 4.5.Restrictions

Tensor Coresè¦æ±‚çš„ç‰¹æ®Šæ ¼å¼åœ¨ä¸åŒçš„ä¸»è¦ï¼ˆmajorï¼‰å’Œæ¬¡è¦ï¼ˆminorï¼‰deviceæ¶æ„ä¹‹é—´å¯èƒ½æœ‰æ‰€ä¸åŒã€‚è¿™ä½¿å¾—WMMAè®¡ç®—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºçº¿ç¨‹ä»…æŒæœ‰çŸ©é˜µç‰‡æ®µfragmentï¼ˆå³æ¶æ„ç‰¹å®šçš„ABIæ•°æ®ç»“æ„ï¼‰ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„çŸ©é˜µã€‚å¼€å‘è€…æ— æ³•å‡è®¾fragmentæ˜¯å¦‚ä½•æ˜ å°„åˆ°å¯„å­˜å™¨çš„ï¼Œå› ä¸ºæ˜ å°„ç­–ç•¥å–å†³äºGPUæ¶æ„ã€‚

>ABIï¼šApplication Binary Interfaceï¼Œè¯‘ä¸ºåº”ç”¨äºŒè¿›åˆ¶æ¥å£ï¼Œæ˜¯äºŒè¿›åˆ¶çº§åˆ«å®šä¹‰ç¨‹åºå¦‚ä½•åœ¨ç³»ç»Ÿä¸Šè¿è¡Œçš„ä¸€ç»„è§„åˆ™ï¼ŒåŒ…æ‹¬å‡½æ•°è°ƒç”¨çº¦å®šã€å¯„å­˜å™¨ä½¿ç”¨è§„åˆ™ã€å†…å­˜å¸ƒå±€ã€æŒ‡ä»¤é›†å…¼å®¹æ€§ã€åŠ¨æ€é“¾æ¥å’Œå…±äº«åº“æ ¼å¼ç­‰ã€‚

ç”±äºfragmentæ˜¯åŸºäºç‰¹å®šæ¶æ„çš„ï¼Œå¦‚æœå‡½æ•°Aå’Œå‡½æ•°Båˆ†åˆ«ä¸ºä¸åŒæ¶æ„ç¼–è¯‘ï¼Œå¹¶åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šé“¾æ¥æ‰§è¡Œï¼Œåˆ™è·¨æ¶æ„ä¼ é€’fragmentæ˜¯ä¸å®‰å…¨çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œfragmentçš„å¤§å°å’Œlayoutå¯èƒ½å› æ¶æ„ä¸åŒè€Œå˜åŒ–ï¼Œåœ¨ä¸åŒ¹é…çš„æ¶æ„ä¸Šä½¿ç”¨WMMA APIï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯ç»“æœç”šè‡³æ•°æ®æŸåã€‚

ä¸åŒæ¶æ„ä¹‹é—´ï¼Œæ¯”å¦‚`sm_70`å’Œ`sm_75`ä¹‹é—´ï¼Œfragmentçš„layoutæ˜¯ä¸åŒçš„ã€‚

```c++
fragA.cu: void foo() { wmma::fragment<...> mat_a; bar(&mat_a); }
fragB.cu: void bar(wmma::fragment<...> *mat_a) { // operate on mat_a }
```

```
// sm_70 fragment layout
$> nvcc -dc -arch=compute_70 -code=sm_70 fragA.cu -o fragA.o
// sm_75 fragment layout
$> nvcc -dc -arch=compute_75 -code=sm_75 fragB.cu -o fragB.o
// Linking the two together
$> nvcc -dlink -arch=sm_75 fragA.o fragB.o -o frag.o
```

è¿™ç§æœªå®šä¹‰è¡Œä¸ºå¯èƒ½åœ¨ç¼–è¯‘æ—¶å’Œè¿è¡Œæ—¶å·¥å…·ä¸­éƒ½æ— æ³•è¢«æ£€æµ‹åˆ°ï¼Œå› æ­¤éœ€è¦ç‰¹åˆ«æ³¨æ„ç¡®ä¿fragmentçš„layoutä¿æŒä¸€è‡´ã€‚è¿™ç§é“¾æ¥é—®é¢˜æœ€æœ‰å¯èƒ½å‘ç”Ÿåœ¨ï¼šå½“é“¾æ¥åˆ°ä¸€ä¸ªæ—§ç‰ˆåº“æ—¶ï¼Œä¸”è¯¥åº“æ˜¯é‡‡ç”¨ä¸åŒæ¶æ„ç¼–è¯‘çš„ã€‚

è¯·æ³¨æ„ï¼Œåœ¨å¼±é“¾æ¥çš„æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚CUDA C++å†…è”å‡½æ•°ï¼‰ï¼Œé“¾æ¥å™¨å¯èƒ½ä¼šé€‰æ‹©ä»»ä½•å¯ç”¨çš„å‡½æ•°å®šä¹‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸åŒç¼–è¯‘å•å…ƒä¹‹é—´çš„fragmentå‘ç”Ÿéšå¼ä¼ é€’ã€‚

ä¸ºäº†é¿å…æ­¤ç±»é—®é¢˜ï¼ŒçŸ©é˜µåº”å§‹ç»ˆå…ˆå­˜å‚¨åˆ°å†…å­˜ï¼Œç„¶åå†é€šè¿‡å¤–éƒ¨æ¥å£è¿›è¡Œä¼ è¾“ï¼ˆä¾‹å¦‚ä½¿ç”¨`wmma::store_matrix_sync(dst, â€¦);`ï¼‰ï¼Œè¿™æ ·ï¼ŒçŸ©é˜µå°±å¯ä»¥ä½œä¸ºæŒ‡é’ˆç±»å‹ä¼ é€’ç»™`bar()`ï¼Œä¾‹å¦‚`float *dst`ï¼Œä»¥ç¡®ä¿æ•°æ®åœ¨ä¸åŒæ¶æ„ä¹‹é—´çš„å…¼å®¹æ€§å’Œæ­£ç¡®æ€§ã€‚

è¯·æ³¨æ„ï¼Œ`sm_70`ä»£ç å¯ä»¥åœ¨`sm_75`è®¾å¤‡ä¸Šè¿è¡Œï¼Œå› æ­¤ä¸Šé¢çš„`sm_75`ä»£ç å¯ä»¥æ›´æ”¹ä¸º`sm_70`ï¼Œå¹¶ä¸”ä»ç„¶å¯ä»¥åœ¨`sm_75`ä¸Šæ­£ç¡®æ‰§è¡Œã€‚ç„¶è€Œï¼Œå½“ä½ çš„åº”ç”¨ç¨‹åºéœ€è¦ä¸å…¶ä»–å•ç‹¬ç¼–è¯‘çš„`sm_75`äºŒè¿›åˆ¶æ–‡ä»¶è¿›è¡Œé“¾æ¥æ—¶ï¼Œæ¨èä½¿ç”¨`sm_75`åŸç”Ÿä»£ç ï¼Œä»¥ç¡®ä¿æœ€ä½³å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚

## 4.6.Element Types and Matrix Sizes

Tensor Coresæ”¯æŒå¤šç§å…ƒç´ ç±»å‹å’ŒçŸ©é˜µå¤§å°ã€‚ä¸‹è¡¨å±•ç¤ºäº†`matrix_a`ã€`matrix_b`å’Œ`accumulator`æ”¯æŒçš„å„ç§ç»„åˆï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/6.png)

æ”¯æŒçš„æµ®ç‚¹è¿ç®—ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/7.png)

æ”¯æŒçš„åŒç²¾åº¦è¿ç®—ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/8.png)

æ”¯æŒçš„å­å­—èŠ‚æ“ä½œï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/CUDAGuide/30/9.png)

## 4.7.Example

ä¸‹é¢çš„ä»£ç ç¤ºä¾‹æ˜¯åœ¨ä¸€ä¸ªwarpå†…æ‰§è¡Œ$16 \times 16 \times 16$çš„çŸ©é˜µä¹˜æ³•ã€‚

```c++
#include <mma.h>
using namespace nvcuda;

__global__ void wmma_ker(half *a, half *b, float *c) {
   // Declare the fragments
   wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;
   wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;
   wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;

   // Initialize the output to zero
   wmma::fill_fragment(c_frag, 0.0f);

   // Load the inputs
   wmma::load_matrix_sync(a_frag, a, 16);
   wmma::load_matrix_sync(b_frag, b, 16);

   // Perform the matrix multiplication
   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

   // Store the output
   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);
}
```

# 5.DPX

DPXæ˜¯ä¸€ç»„å‡½æ•°ï¼Œæœ€å¤šæ”¯æŒä¸‰ä¸ª16ä½å’Œ32ä½çš„æœ‰ç¬¦å·æˆ–æ— ç¬¦å·æ•´æ•°å‚æ•°ï¼Œå¹¶å¯é€‰ä½¿ç”¨ReLUå‡½æ•°ã€‚DPXæ”¯æŒçš„æ“ä½œæœ‰ï¼š

* è®¡ç®—ä¸‰ä¸ªå‚æ•°ä¸­çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼š
    * `__vimax3_s32`ã€`__vimax3_s16x2`ã€`__vimax3_u32`ã€`__vimax3_u16x2`
    * `__vimin3_s32`ã€`__vimin3_s16x2`ã€`__vimin3_u32`ã€`__vimin3_u16x2`
* è®¡ç®—ä¸¤ä¸ªå‚æ•°ä¸­çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼Œå¹¶åº”ç”¨ReLUå‡½æ•°ï¼š
    * `__vimax_s32_relu`ã€`__vimax_s16x2_relu`
    * `__vimin_s32_relu`ã€`__vimin_s16x2_relu`
* è®¡ç®—ä¸‰ä¸ªå‚æ•°ä¸­çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼Œå¹¶åº”ç”¨ReLUå‡½æ•°ï¼š
    * `__vimax3_s32_relu`ã€`__vimax3_s16x2_relu`
    * `__vimin3_s32_relu`ã€`__vimin3_s16x2_relu`
* è®¡ç®—ä¸¤ä¸ªå‚æ•°ä¸­çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼š
    * `__vibmax_s32`ã€`__vibmax_u32`ã€`__vibmax_s16x2`ã€`__vibmax_u16x2`
    * `__vibmin_s32`ã€`__vibmin_u32`ã€`__vibmin_s16x2`ã€`__vibmin_u16x2`
* æ¯”è¾ƒå‰ä¸¤ä¸ªå‚æ•°ä¹‹å’Œä¸ç¬¬ä¸‰ä¸ªå‚æ•°ï¼š
    * `__viaddmax_s32`ã€`__viaddmax_s16x2`ã€`__viaddmax_u32`ã€`__viaddmax_u16x2`
    * `__viaddmin_s32`ã€`__viaddmin_s16x2`ã€`__viaddmin_u32`ã€`__viaddmin_u16x2`
* æ¯”è¾ƒå‰ä¸¤ä¸ªå‚æ•°ä¹‹å’Œä¸ç¬¬ä¸‰ä¸ªå‚æ•°ï¼Œå¹¶åº”ç”¨ReLUå‡½æ•°ï¼š
    * `__viaddmax_s32_relu`ã€`__viaddmax_s16x2_relu`
    * `__viaddmin_s32_relu`ã€`__viaddmin_s16x2_relu`

è¿™äº›æŒ‡ä»¤åœ¨è®¡ç®—èƒ½åŠ›9åŠä»¥ä¸Šçš„deviceä¸Šè¿›è¡Œç¡¬ä»¶åŠ é€Ÿï¼Œå¹¶åœ¨è¾ƒè€çš„deviceä¸Šæä¾›è½¯ä»¶ä»¿çœŸï¼ˆsoftware emulationï¼‰ã€‚

DPXåœ¨å®ç°åŠ¨æ€è§„åˆ’ç®—æ³•æ—¶å°¤å…¶æœ‰ç”¨ï¼Œä¾‹å¦‚åŸºå› ç»„å­¦ä¸­çš„Smith-Watermanç®—æ³•æˆ–Needleman-Wunschç®—æ³•ï¼Œä»¥åŠåœ¨è·¯å¾„ä¼˜åŒ–ä¸­çš„Floyd-Warshallç®—æ³•ã€‚

## 5.1.Examples

ä¸‰ä¸ª32ä½æ•´æ•°ï¼Œè®¡ç®—æœ€å¤§å€¼ï¼Œå¹¶åº”ç”¨ReLUï¼š

```c++
const int a = -15;
const int b = 8;
const int c = 5;
int max_value_0 = __vimax3_s32_relu(a, b, c); // max(-15, 8, 5, 0) = 8
const int d = -2;
const int e = -4;
int max_value_1 = __vimax3_s32_relu(a, d, e); // max(-15, -2, -4, 0) = 0
```

è®¡ç®—å‰ä¸¤ä¸ªå‚æ•°çš„å’Œï¼Œå°†å…¶å’Œç¬¬ä¸‰ä¸ªå‚æ•°æ¯”è¾ƒæ±‚æœ€å¤§å€¼ï¼Œç„¶ååº”ç”¨ReLUå‡½æ•°ï¼š

```c++
const int a = -5;
const int b = 6;
const int c = -2;
int max_value_0 = __viaddmax_s32_relu(a, b, c); // max(-5 + 6, -2, 0) = max(1, -2, 0) = 1
const int d = 4;
int max_value_1 = __viaddmax_s32_relu(a, d, c); // max(-5 + 4, -2, 0) = max(-1, -2, 0) = 0
```

ä¸¤ä¸ª32ä½æ— ç¬¦å·æ•´å‹æ•°ï¼Œæ±‚æœ€å°å€¼ï¼š

```c++
const unsigned int a = 9;
const unsigned int b = 6;
bool smaller_value;
unsigned int min_value = __vibmin_u32(a, b, &smaller_value); // min_value is 6, smaller_value is true
```

æ±‚3ä¸ªæ— ç¬¦å·16ä½æ•´å‹æ•°çš„æœ€å¤§å€¼ï¼š

```c++
const unsigned a = 0x00050002;
const unsigned b = 0x00070004;
const unsigned c = 0x00020006;
unsigned int max_value = __vimax3_u16x2(a, b, c); // max(5, 7, 2) and max(2, 4, 6), so max_value is 0x00070006
```

# 6.å‚è€ƒèµ„æ–™

1. [CUDAä¸­çš„Warp Shuffle](https://blog.csdn.net/kunhe0512/article/details/125492263)
