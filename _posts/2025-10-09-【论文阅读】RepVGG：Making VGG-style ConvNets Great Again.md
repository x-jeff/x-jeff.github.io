---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘RepVGGï¼šMaking VGG-style ConvNets Great Again
subtitle:   RepVGG
date:       2025-10-09
author:     x-jeff
header-img: blogimg/20210828.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

ç»å…¸çš„ç½‘ç»œç»“æ„[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)ç”±å·ç§¯ã€ReLUã€poolingå †å è€Œæˆï¼Œç»“æ„éå¸¸ç®€å•ï¼Œä½†åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚éšç€Inceptionï¼ˆ[Inception-v1](https://shichaoxin.com/2021/06/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Going-deeper-with-convolutions/)ã€[BN-Inception](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)ã€[Inception-v2/v3](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/)ã€[Inception-v4/ResNet](https://shichaoxin.com/2022/01/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Inception-v4,-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/)ï¼‰ã€[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ã€[DenseNet](https://shichaoxin.com/2023/11/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Densely-Connected-Convolutional-Networks/)çš„å‡ºç°ï¼Œç ”ç©¶æ–¹å‘é€æ¸è½¬ä¸ºå¤æ‚çš„ç½‘ç»œæ¶æ„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚

å°½ç®¡è¿™äº›å¤æ‚çš„ç½‘ç»œæ¶æ„çš„ç¡®èƒ½å¸¦æ¥æ›´é«˜çš„ç²¾åº¦ï¼Œä½†å®ƒä»¬ä¹Ÿæœ‰æ˜æ˜¾çš„ç¼ºç‚¹ï¼š

1. å¤šåˆ†æ”¯è®¾è®¡ï¼ˆæ¯”å¦‚[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)å’Œ[Inception](https://shichaoxin.com/2021/06/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Going-deeper-with-convolutions/)ï¼‰ä½¿å¾—æ¨¡å‹éš¾ä»¥å®ç°å’Œå®šåˆ¶ï¼ŒåŒæ—¶å‡æ…¢æ¨ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜åˆ©ç”¨ç‡ã€‚
2. æŸäº›ç»„ä»¶ï¼ˆæ¯”å¦‚[depthwiseå·ç§¯](https://shichaoxin.com/2024/02/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RTMDet-An-Empirical-Study-of-Designing-Real-Time-Object-Detectors/#32model-architecture)å’Œ[channel shuffle](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)ï¼‰å¢åŠ äº†[MAC](https://shichaoxin.com/2025/04/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection/#21memory-access-cost)ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹å„ç§è®¾å¤‡çš„è‰¯å¥½æ”¯æŒã€‚

ç”±äºå½±å“æ¨ç†é€Ÿåº¦çš„å› ç´ å¾ˆå¤šï¼ŒFLOPså¹¶ä¸èƒ½ç²¾ç¡®åæ˜ å®é™…é€Ÿåº¦ã€‚è™½ç„¶ä¸€äº›æ–°æ¨¡å‹çš„FLOPsæ¯”ä¼ ç»Ÿæ¨¡å‹ï¼ˆå¦‚[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)ã€[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼‰æ›´ä½ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¸€å®šè¿è¡Œå¾—æ›´å¿«ã€‚å› æ­¤ï¼Œ[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)å’ŒåŸå§‹ç‰ˆæœ¬çš„[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å®é™…åº”ç”¨ä¸­ä»è¢«å¤§é‡ä½¿ç”¨ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/1.png)

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RepVGGï¼šä¸€ç§[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)é£æ ¼çš„æ¶æ„ï¼Œå…¶æ€§èƒ½ä¼˜äºè®¸å¤šå¤æ‚çš„æ¨¡å‹ï¼ˆè§Fig1ï¼‰ã€‚RepVGGå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

* [VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)é£æ ¼ï¼Œç»“æ„ç®€å•ï¼Œæ²¡æœ‰ä»»ä½•åˆ†æ”¯ï¼Œæ¯ä¸€å±‚ä»…æ¥æ”¶å‰ä¸€å±‚çš„è¾“å‡ºï¼Œå¹¶å°†ç»“æœä¼ é€’ç»™ä¸‹ä¸€å±‚ã€‚
* æ¨¡å‹ä¸»ä½“ä»…ä½¿ç”¨$3\times 3$å·ç§¯å’ŒReLUã€‚
* å…¶å…·ä½“æ¶æ„å®Œå…¨æ˜¯äººå·¥è®¾è®¡çš„ï¼Œä¸éœ€è¦è‡ªåŠ¨æœç´¢æˆ–å¤æ‚è®¾è®¡ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/2.png)

å¦‚Fig2æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé˜¶æ®µä¸ºRepVGGå¼•å…¥äº†æ’ç­‰æ˜ å°„ï¼ˆidentityï¼‰å’Œ$1\times 1$åˆ†æ”¯ï¼Œå…¶çµæ„Ÿæ¥è‡ª[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼Œä½†æ–¹å¼æœ‰æ‰€ä¸åŒï¼Œè¿™äº›åˆ†æ”¯å¯ä»¥åœ¨è®­ç»ƒå®Œæˆåé€šè¿‡ç»“æ„é‡å‚æ•°åŒ–å»æ‰ã€‚å…·ä½“æ“ä½œæ˜¯ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬åˆ©ç”¨ä»£æ•°å˜æ¢ï¼Œå°†identityåˆ†æ”¯è§†ä¸ºé€€åŒ–çš„$1\times 1$å·ç§¯ï¼Œè€Œ$1\times 1$å·ç§¯åˆå¯ä»¥è§†ä¸ºé€€åŒ–çš„$3\times 3$å·ç§¯ï¼Œä»è€ŒæŠŠåŸå§‹$3\times 3$å·ç§¯ã€identityåˆ†æ”¯ã€$1\times 1$åˆ†æ”¯ä»¥åŠ[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚çš„å‚æ•°åˆå¹¶ä¸ºä¸€ä¸ªç­‰ä»·çš„$3\times 3$å·ç§¯ã€‚æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹å°±æ˜¯ä¸€ä¸ªç”±$3\times 3$å·ç§¯å †å è€Œæˆçš„ç®€å•ç»“æ„ï¼Œå¯ç›´æ¥ç”¨äºæ¨ç†ä¸éƒ¨ç½²ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨ç†é˜¶æ®µçš„RepVGGä»…åŒ…å«ä¸€ç§è¿ç®—ç®—å­ï¼š$3\times 3$å·ç§¯+ReLUã€‚è¿™ä½¿å¾—RepVGGåœ¨GPUç­‰é€šç”¨è®¡ç®—è®¾å¤‡ä¸Šéå¸¸é«˜æ•ˆã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/3.png)

å¦‚Fig3æ‰€ç¤ºï¼Œåœ¨[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ä¸­ï¼Œå³°å€¼æƒ…å†µä¸‹ï¼Œmemoryä¸­è¦åŒæ—¶å­˜å‚¨ä¸¤ä¸ªåˆ†æ”¯çš„ä¿¡æ¯ï¼Œæ‰€ä»¥å³°å€¼å†…å­˜å¤§çº¦æ˜¯è¾“å…¥çš„2å€ï¼Œè€Œå¯¹äºRepVGGæ¥è¯´ï¼Œå…¶åªæœ‰ä¸€ä¸ªè·¯å¾„ï¼Œæ‰€ä»¥å³°å€¼å†…å­˜ä¸€èˆ¬æ˜¯è¾“å…¥çš„1å€ã€‚

# 2.Related Work

ä¸å†èµ˜è¿°ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/5.png)

# 3.Building RepVGG via Structural Re-param

## 3.1.Simple is Fast, Memory-economical, Flexible

ä½¿ç”¨plainå·ç§¯ç½‘ç»œè‡³å°‘æœ‰3ä¸ªåŸå› ï¼šå¿«ã€èŠ‚çœå†…å­˜ï¼ˆè§Fig3ï¼‰ã€çµæ´»ã€‚

## 3.2.Training-time Multi-branch Architecture

ä½†æ˜¯plainå·ç§¯ç½‘ç»œæœ‰ä¸ªè‡´å‘½ç¼ºé™·ï¼šæ€§èƒ½ä¸è¶³ã€‚å› æ­¤å‚ç…§[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæˆ‘ä»¬å¼•å…¥äº†å¤šåˆ†æ”¯ï¼š$y = x+g(x)+f(x)$ï¼Œå…¶ä¸­ï¼Œ$x$æ¥è‡ªidentityåˆ†æ”¯ï¼Œ$g(x)$æ¥è‡ª$1\times 1$å·ç§¯åˆ†æ”¯ï¼Œ$f(x)$æ¥è‡ª$3\times 3$å·ç§¯åˆ†æ”¯ã€‚æˆ‘ä»¬å°†è‹¥å¹²ä¸ªè¿™æ ·çš„blockå †å èµ·æ¥ï¼Œæ„å»ºè®­ç»ƒæ—¶çš„æ¨¡å‹ã€‚

## 3.3.Re-param for Plain Inference-time Model

æœ¬éƒ¨åˆ†ä»‹ç»å¦‚ä½•å°†è®­ç»ƒå¥½çš„å¤šåˆ†æ”¯blockè½¬æ¢ä¸ºä¸€ä¸ªå•ä¸€çš„$3\times 3$å·ç§¯å±‚ï¼Œä»è€Œç”¨äºæ¨ç†é˜¶æ®µã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªåˆ†æ”¯çš„å·ç§¯ä¹‹åéƒ½åº”ç”¨äº†[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)ï¼Œåœ¨[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)ä¹‹åæ‰è¿›è¡Œçš„ç›¸åŠ æ“ä½œï¼Œå…·ä½“å¦‚Fig4æ‰€ç¤ºã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/4.png)

ç”¨$W^{(3)} \in \mathbb{R}^{C_2 \times C_1 \times 3 \times 3}$è¡¨ç¤º$3\times 3$å·ç§¯æ ¸ï¼Œå…¶ä¸­$C_1$è¡¨ç¤ºè¾“å…¥é€šé“æ•°ï¼Œ$C_2$è¡¨ç¤ºè¾“å‡ºé€šé“æ•°ã€‚ç±»ä¼¼çš„ï¼Œæˆ‘ä»¬ç”¨$W^{(1)}\in \mathbb{R}^{C_2 \times C_1}$è¡¨ç¤º$1\times 1$å·ç§¯æ ¸ã€‚$3 \times 3$å·ç§¯ä¹‹åçš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)æ‰€ç”¨åˆ°çš„å‚æ•°ç”¨$\mu^{(3)},\sigma^{(3)},\gamma^{(3)},\beta^{(3)}$è¡¨ç¤ºï¼Œ$1\times 1$å·ç§¯ä¹‹åçš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)æ‰€ç”¨åˆ°çš„å‚æ•°ç”¨$\mu^{(1)},\sigma^{(1)},\gamma^{(1)},\beta^{(1)}$è¡¨ç¤ºï¼Œidentityåˆ†æ”¯çš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)æ‰€ç”¨åˆ°çš„å‚æ•°ç”¨$\mu^{(0)},\sigma^{(0)},\gamma^{(0)},\beta^{(0)}$è¡¨ç¤ºã€‚blockçš„è¾“å…¥è¡¨ç¤ºä¸º$M^{(1)}\in \mathbb{R}^{N \times C_1 \times H_1 \times W_1}$ï¼Œblockçš„è¾“å‡ºè¡¨ç¤ºä¸º$M^{(2)}\in \mathbb{R}^{N \times C_2 \times H_2 \times W_2}$ï¼Œç”¨`*`è¡¨ç¤ºå·ç§¯æ“ä½œã€‚å¦‚æœæœ‰$C_1= C_2, H_1 = H_2, W_1=W_2$ï¼Œåˆ™blockçš„è¾“å‡ºå¯ç”¨ä¸‹å¼è®¡ç®—ï¼š

$$\begin{align} M^{(2)} &= \text{bn} (M^{(1)} * W^{(3)}, \mu^{(3)},\sigma^{(3)},\gamma^{(3)},\beta^{(3)}) \\&+ \text{bn} (M^{(1)} * W^{(1)}, \mu^{(1)},\sigma^{(1)},\gamma^{(1)},\beta^{(1)}) \\&+ \text{bn} (M^{(1)}, \mu^{(0)},\sigma^{(0)},\gamma^{(0)},\beta^{(0)}) \end{align} \tag{1}$$

æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨identityåˆ†æ”¯ï¼Œåˆ™åªéœ€æŠŠå¼(1)ä¸­çš„ç¬¬3é¡¹å»æ‰å°±è¡Œã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œ[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)çš„è®¡ç®—å¯è¡¨ç¤ºä¸ºï¼š

$$\text{bn} (M,\mu,\sigma,\gamma,\beta)_{:,i,:,:} = (M_{:,i,:,:}-\mu_i)\frac{\gamma_i}{\sigma_i}+\beta_i, \  \forall 1 \leqslant i \leqslant C_2 \tag{2}$$

æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ª[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å’Œå…¶å‰ç½®çš„å·ç§¯åˆå¹¶ä¸ºä¸€ä¸ªå·ç§¯å’Œä¸€ä¸ªåç½®é¡¹ã€‚å³å°†å‚æ•°$\\{ W,\mu,\sigma,\gamma,\beta \\}$è½¬æ¢ä¸º$\\{ W',b' \\}$ï¼Œè½¬æ¢å…¬å¼è§ä¸‹ï¼š

$$W'_{i,:,:,:} = \frac{\gamma_i}{\sigma_i} W_{i,:,:,:}, \quad b'_i = -\frac{\mu_i \gamma_i}{\sigma_i}+\beta_i , \ \forall 1 \leqslant i \leqslant C_2 \tag{3}$$

[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚å’Œå·ç§¯å±‚èåˆä¹‹åçš„è®¡ç®—ä¸ºï¼š

$$\text{bn} (M * W, \mu, \sigma, \gamma, \beta)_{:,i,:,:} = (M * W')_{:,i,:,:} + b_i' \tag{4}$$

è¿™ç§å˜æ¢å¯¹identityåˆ†æ”¯ä¹Ÿé€‚ç”¨ï¼Œå› ä¸ºidentityæ“ä½œå¯ä»¥è§†ä¸ºä¸€ä¸ª$1\times 1$å·ç§¯ï¼Œå·ç§¯æ ¸æ˜¯å•ä½çŸ©é˜µã€‚é€šè¿‡è¿™ç§å˜æ¢ï¼Œç¬¬ä¸€ä¸ªåˆ†æ”¯å˜æˆäº†1ä¸ª$3\times 3$å·ç§¯å’Œ1ä¸ªåç½®å‘é‡ï¼Œç¬¬äºŒä¸ªåˆ†æ”¯å˜æˆäº†1ä¸ª$1\times 1$å·ç§¯å’Œ1ä¸ªåç½®å‘é‡ï¼Œç¬¬ä¸‰ä¸ªåˆ†æ”¯å˜æˆäº†1ä¸ª$1\times 1$å·ç§¯å’Œ1ä¸ªåç½®å‘é‡ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æŠŠè¿™ä¸‰ä¸ªåˆ†æ”¯åˆå¹¶æˆä¸€ä¸ªå·ç§¯å’Œä¸€ä¸ªåç½®å‘é‡ï¼Œåˆå¹¶ç­–ç•¥å¦‚Fig4(B)æ‰€ç¤ºï¼Œ3ä¸ªåˆ†æ”¯çš„åç½®å‘é‡å¯ä»¥ç›´æ¥ç›¸åŠ å¾—åˆ°ä¸€ä¸ªåç½®å‘é‡ï¼Œ2ä¸ª$1\times 1$å·ç§¯å¯ä»¥ç”¨0 paddingåˆ°$3\times 3$å¤§å°ï¼Œç„¶åå°†3ä¸ª$3\times 3$å·ç§¯ç›¸åŠ å¾—åˆ°ä¸€ä¸ª$3\times 3$å·ç§¯ã€‚è¿™æ ·ï¼Œ3ä¸ªåˆ†æ”¯å°±è¢«åˆå¹¶æˆäº†ä¸€ä¸ªå•ä¸€çš„$3\times 3$å·ç§¯å±‚ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§ç­‰ä»·è½¬æ¢è¦æ±‚$3\times 3$å·ç§¯å’Œ$1\times 1$å·ç§¯çš„æ­¥é•¿ç›¸åŒï¼Œæ­¤å¤–ï¼Œ$1\times 1$å·ç§¯çš„paddingè¦æ¯”$3\times 3$å°‘1ï¼Œæ¯”å¦‚ï¼Œ$3\times 3$å·ç§¯é€šå¸¸è®¾ç½®padding=1ï¼Œæ­¤æ—¶$1\times 1$å·ç§¯åº”è®¾ç½®padding=0ã€‚

## 3.4.Architectural Specification

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/6.png)

RepVGGæ˜¯ä¸€ç§[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)é£æ ¼çš„æ¶æ„ï¼Œé‡‡ç”¨plainçš„æ‹“æ‰‘ç»“æ„ï¼Œå¤§é‡ä½¿ç”¨$3\times 3$å·ç§¯ï¼Œä½†å’Œ[VGG](https://shichaoxin.com/2021/02/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/)ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä½¿ç”¨max poolingï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ç½‘ç»œæ¶æ„ä¸»ä½“åªæœ‰ä¸€ç§ç±»å‹çš„æ“ä½œã€‚æˆ‘ä»¬å°†å¤§é‡çš„$3\times 3$å·ç§¯å±‚åˆ†ä¸º5ä¸ªé˜¶æ®µï¼Œå…¶ä¸­æ¯ä¸ªé˜¶æ®µä¸­çš„ç¬¬ä¸€å±‚æ­¥é•¿ä¸º2ã€‚å¯¹äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¨è¿æ¥å±‚åæ¥ä¸€ä¸ªglobal average poolingä½œä¸ºheadã€‚å¯¹äºå…¶ä»–ç±»å‹çš„ä»»åŠ¡ï¼Œå¯ä»¥å°†ç‰¹å®šçš„headæ¥åœ¨ä»»æ„ä¸€å±‚çš„åé¢ã€‚

æ¯ä¸ªé˜¶æ®µå†…å±‚çš„æ•°é‡éµå¾ªä¸‰ä¸ªç®€å•çš„åŸåˆ™ï¼š1ï¼‰ç¬¬ä¸€ä¸ªé˜¶æ®µè¦å¤„ç†å¾ˆå¤§çš„åˆ†è¾¨ç‡ï¼Œè¿™ä¼šå¾ˆè€—æ—¶ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªé˜¶æ®µåªåŒ…å«ä¸€å±‚ï¼Œä»¥ä¿è¯ä½å»¶è¿Ÿï¼›2ï¼‰æœ€åä¸€ä¸ªé˜¶æ®µåº”è¯¥æœ‰æ›´å¤šçš„é€šé“æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»…ç”¨ä¸€å±‚æ¥ä¿å­˜å‚æ•°ï¼›3ï¼‰ä»¿ç…§[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼Œæˆ‘ä»¬åœ¨å€’æ•°ç¬¬äºŒä¸ªé˜¶æ®µä¸­è®¾ç½®äº†æœ€å¤šçš„å±‚æ•°ã€‚

åœ¨è¡¨2ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†RepVGG-Aå’ŒRepVGG-Bçš„ç»“æ„ã€‚é€šé“ç¼©æ”¾å› å­$b$é€šå¸¸è®¾ç½®çš„è¦æ¯”å› å­$a$å¤§ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›æœ€åä¸€å±‚å¯ä»¥è·å¾—æ›´ä¸°å¯Œçš„ç‰¹å¾ç”¨äºåˆ†ç±»æˆ–å…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[åˆ†ç»„å·ç§¯](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)æ¥è¿›ä¸€æ­¥é™ä½å‚æ•°é‡å’Œè®¡ç®—æˆæœ¬ã€‚é™¤å»ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åªå¯¹å¥‡æ•°å±‚ä½¿ç”¨[åˆ†ç»„å·ç§¯](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)ã€‚åˆ†ç»„æ•°$g$é€šå¸¸å…¨å±€è®¾ç½®ä¸º1ï¼Œ2æˆ–4ã€‚

# 4.Experiments

## 4.1.RepVGG for ImageNet Classification

æ„å»ºäº†ä¸€ç³»åˆ—ä¸åŒè§„æ¨¡çš„RepVGGæ¨¡å‹ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/7.png)

å¯¹äºè½»é‡çº§å’Œä¸­é‡çº§çš„æ¨¡å‹ï¼Œè®­ç»ƒåªä½¿ç”¨äº†ç®€å•çš„æ•°æ®æ‰©å±•ï¼ŒåŒ…æ‹¬éšæœºè£å‰ªå’Œå·¦å³ç¿»è½¬ã€‚ä½¿ç”¨äº†8å—GPUï¼Œå…¨å±€batch sizeè®¾ç½®ä¸º256ï¼Œåˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º0.1ï¼Œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ä½¿ç”¨[cosine annealing](https://shichaoxin.com/2024/07/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/#51cosine-learning-rate-decay)ï¼Œmomentum=0.9ï¼Œweight decay=$10^{-4}$ã€‚å¯¹äºé‡é‡çº§æ¨¡å‹ï¼ŒåŒ…æ‹¬RegNetX-12GFã€[EfficientNet-B3](https://shichaoxin.com/2024/12/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks/)å’ŒRepVGG-B3ï¼Œä½¿ç”¨5ä¸ªepochç”¨äº[warmup](https://shichaoxin.com/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#6b12self-supervision)ï¼Œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ä½¿ç”¨[cosine annealing](https://shichaoxin.com/2024/07/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/#51cosine-learning-rate-decay)ï¼Œè¿˜ä½¿ç”¨äº†[label smoothing](https://shichaoxin.com/2021/11/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Rethinking-the-Inception-Architecture-for-Computer-Vision/#7model-regularization-via-label-smoothing)ã€[MixUp](https://shichaoxin.com/2024/01/14/YOLO%E7%B3%BB%E5%88%97-YOLOv5/#3data-augmentation-techniques)ã€éšæœºè£å‰ªå’Œç¿»è½¬ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/8.png)

å…¶ä¸­ï¼Œåç¼€"g2/g4"è¡¨ç¤ºä½¿ç”¨äº†[åˆ†ç»„å·ç§¯](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/#31channel-shuffle-for-group-convolutions)ï¼Œ"g2"è¡¨ç¤ºåˆ†ç»„æ•°$g=2$ï¼Œ"g4"è¡¨ç¤ºåˆ†ç»„æ•°$g=4$ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/9.png)

## 4.2.Structural Re-parameterization is the Key

RepVGG blockæœ‰3ä¸ªåˆ†æ”¯ï¼Œåœ¨è¡¨6ä¸­ï¼Œç¬¬ä¸€è¡Œæ˜¯ç§»é™¤identityåˆ†æ”¯å’Œ$1\times 1$åˆ†æ”¯çš„æµ‹è¯•ç»“æœï¼Œç¬¬äºŒè¡Œæ˜¯ä»…ç§»é™¤$1\times 1$åˆ†æ”¯çš„æµ‹è¯•ç»“æœï¼Œç¬¬ä¸‰è¡Œæ˜¯ä»…ç§»é™¤identityåˆ†æ”¯çš„æµ‹è¯•ç»“æœï¼Œç¬¬å››è¡Œæ˜¯ä¿ç•™æ‰€æœ‰3ä¸ªåˆ†æ”¯çš„æµ‹è¯•ç»“æœã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/10.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/11.png)

åœ¨è¡¨7ä¸­åˆ—å‡ºäº†ä¸åŒRepVGG-B0å˜ä½“çš„æ€§èƒ½æ¯”è¾ƒï¼š

* **Identity w/o BN**ï¼šç§»é™¤identityçš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)ã€‚
* **Post-addition BN**ï¼šç§»é™¤ä¸‰ä¸ªåˆ†æ”¯çš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚ï¼Œç„¶ååœ¨ä¸‰ä¸ªåˆ†æ”¯ç›¸åŠ æ“ä½œçš„åé¢åŠ ä¸€ä¸ª[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚ã€‚
* **+ReLU in branches**ï¼šåœ¨åŸå§‹çš„RepVGGç»“æ„ä¸­ï¼Œè®­ç»ƒé˜¶æ®µåœ¨åˆ†æ”¯ç›¸åŠ æ“ä½œä¹‹åæ‰ä¼šReLUï¼Œç°åœ¨åœ¨æ¯ä¸ªåˆ†æ”¯å†…çš„[BN](https://shichaoxin.com/2021/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚ä¹‹åéƒ½åŠ ä¸ŠReLUæ“ä½œï¼Œä»¥æ­¤æ¥çœ‹ä¸‹æ›´å¤šçš„éçº¿æ€§æ“ä½œæ˜¯å¦å¯ä»¥æé«˜æ€§èƒ½ã€‚
* **DiracNet**ï¼šä¸€ç§ç»è¿‡é‡å‚æ•°åŒ–çš„å·ç§¯ï¼Œå¯ä»¥æŠŠidentityæ“ä½œèå…¥åˆ°å·ç§¯ä¸­å»ã€‚
* **Trivial Re-param**ï¼šä¸€ç§æ›´ä¸ºç®€å•çš„é‡å‚æ•°åŒ–å·ç§¯ï¼Œç›´æ¥å°†identityæ ¸åŠ åˆ°$3\times 3$æ ¸ä¸Šï¼Œå¯è§†ä¸ºä¸€ç§é€€åŒ–ç‰ˆæœ¬çš„DiracNetã€‚
* **Asymmetric Conv Block (ACB)**ï¼šå¯è§†ä¸ºå¦ä¸€ç§ç»“æ„åŒ–çš„é‡å‚æ•°æ–¹æ³•ã€‚
* **Residual Reorg**ï¼šä»¿ç…§[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼ˆæ¯ä¸ªblockå†…æœ‰ä¸¤å±‚ï¼‰ï¼Œæˆ‘ä»¬å¯¹RepVGG-B0çš„ç»“æ„è¿›è¡Œäº†é‡æ„ã€‚å¯¹äºç¬¬1ä¸ªå’Œç¬¬5ä¸ªé˜¶æ®µï¼Œåªæœ‰ä¸€ä¸ª$3\times 3$å·ç§¯å±‚ï¼›å¯¹äºç¬¬2ã€3ã€4ä¸ªé˜¶æ®µï¼Œæ¯ä¸¤å±‚æ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼Œæ‰€ä»¥å¯¹äºç¬¬2ã€3ã€4ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«ä¼šæœ‰2ã€3ã€8ä¸ªæ®‹å·®å—ã€‚

è¡¨7ä¸­çš„"Full-featured reparam"è¡¨ç¤ºbaselineï¼Œå³åŸå§‹çš„RepVGG-B0ã€‚

## 4.3.Semantic Segmentation

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/RepVGG/12.png)

æˆ‘ä»¬åœ¨Cityscapesæ•°æ®é›†ä¸­ï¼ŒéªŒè¯äº†ç»è¿‡ImageNeté¢„è®­ç»ƒçš„RepVGGåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨PSPNetæ¡†æ¶ï¼Œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ä½¿ç”¨poly learning rate policyï¼š

$$lr=lr_{init} \times \left( 1 - \frac{iter}{max\_iter} \right)^{power}$$

>PSPNetè§è®ºæ–‡ï¼šHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6230â€“6239. IEEE Computer Society, 2017.ã€‚

å…¶ä¸­ï¼Œ$lr_{init}$ä¸ºåˆå§‹å­¦ä¹ ç‡ï¼Œè¿™é‡Œè®¾ç½®ä¸º0.01ã€‚iterå’Œmax\_iteræŒ‡çš„æ˜¯å…¨å±€è¿­ä»£æ¬¡æ•°ï¼Œå³batchæ•°ï¼Œiterè¡¨ç¤ºå·²ç»è¿è¡Œçš„batchæ•°ï¼Œmax\_iterè¡¨ç¤ºæœ€å¤§batchæ•°ï¼Œé€šå¸¸ç­‰äºepochæ•°é‡ä¹˜ä¸Šæ¯ä¸ªepochå†…çš„batchæ•°é‡ã€‚powerè¿™é‡Œè®¾ç½®ä¸º0.9ã€‚

weight decayè®¾ç½®ä¸º$10^{-4}$ï¼Œä½¿ç”¨8å—GPUï¼Œå…¨å±€batch sizeè®¾ç½®ä¸º16ï¼Œå…±è®­ç»ƒ40ä¸ªepochã€‚ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯æŠŠ[ResNet-50å’ŒResNet-101](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)çš„backboneåˆ†åˆ«æ›¿æ¢ä¸ºRepVGG-B1g2å’ŒRepVGG-B2ï¼Œå…¶ä»–è®¾ç½®å‡ä¿æŒä¸€è‡´ã€‚åœ¨å®˜æ–¹çš„PSPNet-50/101ä¸­ï¼Œå…¶åœ¨[ResNet-50/101](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)çš„æœ€åä¸¤ä¸ªé˜¶æ®µä½¿ç”¨äº†ç©ºæ´å·ç§¯ï¼Œä¸ºäº†éµå¾ªè¿™ä¸€è®¾è®¡ï¼ŒRepVGG-B1g2å’ŒRepVGG-B2çš„æœ€åä¸¤ä¸ªé˜¶æ®µçš„æ‰€æœ‰$3\times 3$å·ç§¯å±‚ä¹Ÿéƒ½ä½¿ç”¨äº†ç©ºæ´å·ç§¯ã€‚ä½†æ˜¯ï¼Œå½“å‰çš„$3\times 3$ç©ºæ´å·ç§¯å®ç°å¹¶ä¸å……åˆ†ï¼ˆè™½ç„¶å’Œå¸¸è§„çš„$3\times 3$å·ç§¯çš„FLOPsä¸€æ ·ï¼‰ï¼Œå…¶ä¼šå¯¼è‡´æ¨ç†å˜æ…¢ã€‚ä¸ºäº†ä¾¿äºæ¯”è¾ƒï¼Œæˆ‘ä»¬æ„å»ºäº†å¦å¤–ä¸¤ä¸ªPSPNetsï¼ˆæ ‡è®°ä¸ºfastï¼‰ï¼Œå…¶ä»…åœ¨æœ€å5å±‚ä½¿ç”¨ç©ºæ´å·ç§¯ï¼ˆå³stage4çš„æœ€å4å±‚å’Œstage5çš„ä¸€å±‚ï¼‰ï¼Œè¿™æ ·çš„è¯ï¼ŒPSPNetsèƒ½æ¯”å…¶å¯¹åº”çš„ä»¥[ResNet-50/101](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ä¸ºbackboneçš„æ¨¡å‹è¿è¡Œçš„ç¨å¾®å¿«ä¸€äº›ã€‚

## 4.4.Limitations

RepVGGæ˜¯ä¸€ä¸ªå¿«é€Ÿã€ç®€å•ä¸”å®ç”¨çš„CNNæ¶æ„ï¼Œå…¶è®¾è®¡ç›®æ ‡æ˜¯åœ¨GPUå’Œä¸“ç”¨ç¡¬ä»¶ä¸Šå®ç°æœ€é«˜çš„è¿è¡Œé€Ÿåº¦ï¼Œè€Œå¹¶ä¸æ˜¯åˆ»æ„å‡å°‘å‚æ•°æ•°é‡ã€‚RepVGGåœ¨å‚æ•°åˆ©ç”¨ç‡ä¸Šä¼˜äº[ResNet](https://shichaoxin.com/2022/01/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Residual-Learning-for-Image-Recognition/)ï¼Œä½†åœ¨ä½åŠŸè€—è®¾å¤‡ä¸Šï¼Œé€Šè‰²äº[MobileNets](https://shichaoxin.com/2024/12/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/)å’Œ[ShuffleNets](https://shichaoxin.com/2025/08/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices/)ç­‰ä¸“ä¸ºç§»åŠ¨ç«¯è®¾è®¡çš„æ¨¡å‹ã€‚

# 5.Conclusion

ä¸å†èµ˜è¿°ã€‚

# 6.åŸæ–‡é“¾æ¥

ğŸ‘½[RepVGGï¼šMaking VGG-style ConvNets Great Again](https://github.com/x-jeff/AI_Papers/blob/master/2025/RepVGG%EF%BC%9AMaking%20VGG-style%20ConvNets%20Great%20Again.pdf)