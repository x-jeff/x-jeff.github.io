---
layout:     post
title:      【深度学习基础】第二十四课：softmax函数的导数
subtitle:   softmax函数在神经网络中的反向传播
date:       2020-05-12
author:     x-jeff
header-img: blogimg/20200512.jpg
catalog: true
tags:
    - Deep Learning Series
---
>【深度学习基础】系列博客为学习Coursera上吴恩达深度学习课程所做的课程笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.softmax函数

👉[softmax函数详解](http://shichaoxin.com/2019/09/04/深度学习基础-第二课-softmax分类器和交叉熵损失函数/)。

# 2.softmax函数的导数

假设神经网络输出层的激活函数为softmax函数，用以解决多分类问题。在反向传播时，就需要计算softmax函数的导数，这也就是本文着重介绍的内容。

我们只需关注输出层即可，其余层和之前介绍的二分类模型一样，不再赘述。

我们先考虑只有一个样本的情况，输出层的$z^{[L]},a^{[L]}$以及标签y为（假设预测类别共有K类）：

$$z^{[L]}= \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_K \\ \end{bmatrix} ; a^{[L]} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_K \\ \end{bmatrix};y=\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_K \\ \end{bmatrix}$$

其中：

$$a_j=\frac{e^{z_j}}{\sum^K_{k=1}e^{z_k}},j\in [1,2,...,K] \tag{1}$$

使用[交叉熵损失函数](http://shichaoxin.com/2019/09/04/深度学习基础-第二课-softmax分类器和交叉熵损失函数/)构建cost function：

$$C=-\sum^K_{k=1} y_k \ln a_k \tag{2}$$

其中，$y_k$等于0或1。

很容易算出：

$$\frac{\partial C}{\partial a_j}=-\frac{y_j}{a_j} \tag{3}$$

接下来我们只要算出$\frac{\partial a_j}{ \partial z_i}$即可。这里之所以是$z_i$而不是$z_j$是因为使用softmax计算得到的$a_j$包含了$z^{[L]}$中的所有元素，也就是说$a_j$可对$z^{[L]}$中的任意元素求导，而不仅仅只是$z_j$。因此，我们分两种情况讨论。

👉情况一：$i=j$

$$\begin{align} \frac{\partial a_j}{\partial z_i} &= \frac{\partial a_i}{\partial z_i} \\&=  \frac{e^{z_i} \sum^K_{k=1}e^{z_k} - e^{2z
_i} }{ (\sum^K_{k=1}e^{z_k})^2 } \\&= \frac{e^{z_i}}{\sum^K_{k=1}e^{z_k}} - (\frac{e^{z_i}}{\sum^K_{k=1}e^{z_k}})^2 \\&= a_i-a_i^2 \\&= a_i(1-a_i) \tag{4} \end{align}$$

👉情况二：$i\neq j$

$$\frac{\partial a_j}{\partial z_i} = -\frac{e^{z_j} e^{z_i}}{(\sum^K_{k=1}e^{z_k})^2} = -a_j a_i \tag{5} $$

结合式(3)、式(4)、式(5)可得：

$$\begin{align} \frac{\partial C}{\partial z_i} &= \sum^K_{k=1} \frac{\partial C}{\partial a_k} \cdot \frac{\partial a_k}{\partial z_i} \tag{6} \\&= -\frac{y_i}{a_i} \cdot a_i(1-a_i) + \sum^K_{k=1,k\neq i} (-\frac{y_k}{a_k}) \cdot (-a_k a_i) \tag{7} \\&= -y_i + y_i a_i + \sum^K_{k=1,k\neq i} y_k a_i \tag{8} \\&= -y_i + a_i (y_i + \sum^K_{k=1,k\neq i} y_k) \tag{9} \\&= a_i - y_i \end{align}$$

* 式(6)是因为$a^{[L]}$中每个元素的计算都包含了$z_i$。
* 式(7)中将k分为$k=i$和$k\neq i$两种情况。$k=i$的情况可以用到式(4)，$k\neq i$的情况用到式(5)。
* 式(9)中，$y_i + \sum^K_{k=1,k\neq i} y_k=1$。这是因为K个类别中只有一个为正确类别，其标签为1，其余错误类别标签均为0。

使用[向量化](http://shichaoxin.com/2019/11/22/深度学习基础-第五课-向量化/)方法：

$$\frac{\partial C}{\partial z^{[L]} }=a^{[L]}-y$$

可直接扩展到多个样本的情况：

$$\frac{\partial L}{\partial Z^{[L]} }=A^{[L]}-Y$$
