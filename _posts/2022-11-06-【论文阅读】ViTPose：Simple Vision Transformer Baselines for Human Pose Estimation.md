---
layout:     post
title:      ã€è®ºæ–‡é˜…è¯»ã€‘ViTPoseï¼šSimple Vision Transformer Baselines for Human Pose Estimation
subtitle:   ViTPoseï¼ŒHuman Pose Estimation
date:       2022-11-06
author:     x-jeff
header-img: blogimg/20221106.jpg
catalog: true
tags:
    - AI Papers
---  
>æœ¬æ–‡ä¸ºåŸåˆ›æ–‡ç« ï¼Œæœªç»æœ¬äººå…è®¸ï¼Œç¦æ­¢è½¬è½½ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

# 1.Introduction

>githubå®˜æ–¹repoï¼š[https://github.com/ViTAE-Transformer/ViTPose](https://github.com/ViTAE-Transformer/ViTPose)ã€‚

äººä½“å§¿æ€ä¼°è®¡æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸€ä¸ªé‡è¦çš„ä»»åŠ¡ç±»å‹ï¼Œå¹¶ä¸”å…¶åœ¨çœŸå®ä¸–ç•Œä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚äººä½“å§¿æ€ä¼°è®¡ä»»åŠ¡çš„ç›®æ ‡æ˜¯å®šä½äººä½“è§£å‰–å…³èŠ‚ç‚¹ï¼Œä½†ç”±äºå„ç§å½¢å¼çš„é®æŒ¡ã€æˆªæ–­ã€ç¼©æ”¾ä»¥åŠä¸åŒçš„äººç‰©å¤–è§‚ï¼Œè€Œå¯¼è‡´è¿™é¡¹ä»»åŠ¡å……æ»¡æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»å–å¾—äº†è¿…é€Ÿçš„è¿›å±•ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä½¿ç”¨çš„éƒ½æ˜¯CNNæ¡†æ¶ã€‚

è¿‘æœŸï¼Œ[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­éƒ½å±•ç°å‡ºäº†å¾ˆå¼ºçš„æ½œåŠ›ã€‚å—å…¶æˆåŠŸçš„å¯å‘ï¼Œå„ç§ä¸åŒçš„[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)æ¡†æ¶è¢«éƒ¨ç½²ç”¨äºå§¿æ€ä¼°è®¡ä»»åŠ¡ã€‚è¿™äº›æ–¹æ³•å¤§å¤šé‡‡ç”¨CNNä½œä¸ºbackboneï¼Œç„¶åä½¿ç”¨[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)æ¥refineæå–åˆ°çš„ç‰¹å¾ï¼Œæœ€åå¯¹å…³èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚æ¯”å¦‚ï¼ŒPRTRç»“åˆäº†[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)çš„encoderå’Œdecoderï¼Œä»¥çº§è”çš„æ–¹å¼é€æ­¥ç»†åŒ–ä¼°è®¡å…³èŠ‚ç‚¹çš„ä½ç½®ã€‚TokenPoseå’ŒTransPoseåˆ™ä»…ç”¨äº†[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)çš„encoderæ¥å¤„ç†CNNæå–åˆ°çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒHRFormerç›´æ¥ä½¿ç”¨[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¤šåˆ†è¾¨ç‡å¹¶è¡Œçš„[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)æ¨¡å—æ¥è·å–é«˜åˆ†è¾¨ç‡çš„representationã€‚ä»¥ä¸Šè¿™äº›æ–¹æ³•éƒ½åœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­è·å¾—äº†éå¸¸ä¸é”™çš„æˆç»©ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆéœ€è¦é¢å¤–çš„CNNç”¨äºç‰¹å¾æå–ï¼Œè¦ä¹ˆéœ€è¦ä»”ç»†æ­å»º[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)æ¡†æ¶ä»¥é€‚åº”ä»»åŠ¡ã€‚è¿™å°±ä½¿å¾—æˆ‘ä»¬åœ¨è€ƒè™‘ä¸€ä¸ªé—®é¢˜ï¼Œæœ€åŸå§‹ã€æœ€æœ´ç´ çš„[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)ï¼Œèƒ½åœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­è¡¨ç°å¦‚ä½•ï¼Ÿ

>PRTRåŸæ–‡ï¼šK. Li, S. Wang, X. Zhang, Y. Xu, W. Xu, and Z. Tu. Pose recognition with cascade transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1944â€“1953, June 2021.ã€‚
>
>TokenPoseåŸæ–‡ï¼šY. Li, S. Zhang, Z. Wang, S. Yang, W. Yang, S.-T. Xia, and E. Zhou. Tokenpose: Learning keypoint tokens for human pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.ã€‚
>
>TransPoseåŸæ–‡ï¼šS. Yang, Z. Quan, M. Nie, and W. Yang. Transpose: Keypoint localization via transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.ã€‚
>
>HRFormeråŸæ–‡ï¼šY.Yuan, R.Fu, L.Huang, W.Lin, C.Zhang, X.Chen, and J.Wang. Hrformer : High-resolution transformer for dense prediction. In Advances in Neural Information Processing Systems, 2021.ã€‚

ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ViTPoseæ¨¡å‹ï¼Œå¹¶åœ¨MS COCO Keypointæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒViTPoseä½¿ç”¨çº¯ç²¹çš„[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)ä½œä¸ºbackboneç›´æ¥æå–ç‰¹å¾ï¼Œå¹¶ä¸”backboneæ˜¯ç»è¿‡pre-trainedçš„ï¼ˆpre-trainçš„æ–¹æ³•ï¼špre-trained with masked image modeling pretext tasksï¼Œæ¯”å¦‚MAEï¼‰ã€‚ç„¶åï¼Œä¸€ä¸ªè½»é‡çº§çš„decoderç”¨äºå¤„ç†æå–åˆ°çš„ç‰¹å¾ã€‚å°½ç®¡ViTPoseæ²¡æœ‰åœ¨æ¨¡å‹æ¡†æ¶ä¸ŠèŠ±è´¹å¤ªå¤šå¿ƒæ€ï¼Œä½†æ˜¯å…¶ä¾ç„¶åœ¨MS COCO Keypoint test-dev setä¸Šå–å¾—äº†SOTAçš„æˆç»©ï¼ˆ80.9çš„APï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå³ä½¿æ˜¯ç®€å•æœ´ç´ çš„[Transformer](http://shichaoxin.com/2022/03/26/è®ºæ–‡é˜…è¯»-Attention-Is-All-You-Need/)æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥åœ¨å§¿æ€ä¼°è®¡ä¸­å–å¾—å¾ˆå¥½çš„æˆç»©ã€‚

>MAEåŸæ–‡ï¼šK. He, X. Chen, S. Xie, Y. Li, P. DollÃ¡r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000â€“16009, 2022.ã€‚

é™¤äº†æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ViTPoseåœ¨ä»¥ä¸‹4ä¸ªæ–¹é¢çš„ä¼˜ç§€è¡¨ç°ï¼š

ğŸ‘‰**1ï¼‰simplicity**

å¾—ç›Šäº[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)å¼ºå¤§çš„feature representation abilityï¼ŒViTPoseçš„æ¡†æ¶ç›¸å½“ç®€å•ã€‚ä¾‹å¦‚ï¼Œå®ƒä¸éœ€è¦ç‰¹å®šçš„é¢†åŸŸçŸ¥è¯†æ¥ç²¾å¿ƒçš„è®¾è®¡backboneã€‚è¿™ç§ç®€å•çš„ç»“æ„ä½¿å¾—ViTPoseå…·æœ‰è‰¯å¥½çš„å¹¶è¡Œæ€§ï¼Œä»è€Œåœ¨æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½æ–¹é¢è¾¾åˆ°äº†æ–°çš„Pareto frontï¼Œè¯¦è§Fig1ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/1.png)

Fig1ä¸­ï¼Œbubbleçš„å¤§å°ä»£è¡¨æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚

ğŸ‘‰**2ï¼‰scalability**

æ­¤å¤–ï¼Œç»“æ„çš„simplicityç»™ViTPoseå¸¦æ¥äº†ä¼˜å¼‚çš„scalabilityã€‚è¿™é‡Œçš„scalabilityæŒ‡çš„æ˜¯å¯ä»¥æ–¹ä¾¿çš„é€šè¿‡transformer layerså’Œfeature dimensionsæ¥æ§åˆ¶æ¨¡å‹çš„å¤§å°ï¼Œæ¯”å¦‚ï¼Œä½¿ç”¨[ViT-Bã€ViT-Læˆ–ViT-H](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#41setup)æ¥å¹³è¡¡æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚

ğŸ‘‰**3ï¼‰flexibility**

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ViTPoseåœ¨è®­ç»ƒä¸­çš„çµæ´»æ€§ã€‚ViTPoseåªéœ€ç¨ä½œä¿®æ”¹ï¼Œä¾¿å¯ä»¥å¾ˆå¥½çš„é€‚åº”ä¸åŒçš„input resolutionså’Œfeature resolutionsï¼Œå¹¶ä¸”å¯¹äºé«˜åˆ†è¾¨ç‡çš„è¾“å…¥ï¼ŒViTPoseå§‹ç»ˆå¯ä»¥æä¾›æ›´å‡†ç¡®çš„å§¿æ€ä¼°è®¡ç»“æœã€‚é™¤äº†åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ·»åŠ é¢å¤–çš„decoderä½¿å¾—ViTPoseå¯ä»¥åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè”åˆè®­ç»ƒï¼Œè¿™èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å› ä¸ºdecoderæ˜¯éå¸¸è½»é‡çº§çš„ï¼Œæ‰€ä»¥è¿™ç§è®­ç»ƒæ¨¡å¼æ‰€å¸¦æ¥çš„é¢å¤–è®¡ç®—æˆæœ¬å¹¶ä¸å¤šã€‚å½“æˆ‘ä»¬ä½¿ç”¨æ›´å°çš„æ— æ ‡ç­¾æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒæˆ–è€…åœ¨fine-tuneçš„æ—¶å€™å†»ç»“attention modulesï¼ŒViTPoseä»ç„¶å¯ä»¥å–å¾—SOTAçš„æˆç»©ï¼Œå¹¶ä¸”ç›¸æ¯”fully pre-trained finetuningï¼Œè¿™æ ·åšçš„è®­ç»ƒæˆæœ¬æ›´ä½ã€‚

ğŸ‘‰**4ï¼‰transferability**

é€šè¿‡ä¸€ä¸ªé¢å¤–çš„learnable knowledge tokenï¼Œå¯ä»¥å°†large ViTPose modelså­¦åˆ°çš„knowledgeè¿ç§»ç»™small ViTPose modelsï¼Œä»è€Œæå‡small ViTPose modelsçš„æ€§èƒ½ã€‚è¿™è¯´æ˜ViTPoseæœ‰ç€è‰¯å¥½çš„transferabilityã€‚

æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡çš„è´¡çŒ®æœ‰ä¸‰æ–¹é¢ï¼š

1. æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¨¡å‹ï¼ˆViTPoseï¼‰ç”¨äºäººä½“å§¿æ€ä¼°è®¡ã€‚åœ¨æ²¡æœ‰ç²¾ç»†è®¾è®¡å¤æ‚æ¡†æ¶çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶åœ¨MS COCO Keypointæ•°æ®é›†ä¸Šå–å¾—äº†SOTAçš„æˆç»©ã€‚
2. ç®€å•çš„ViTPoseæ¨¡å‹æœ‰ç€ä»¥ä¸‹ä»¤äººæƒŠè®¶çš„è‰¯å¥½èƒ½åŠ›ï¼šstructural simplicityã€model size scalabilityã€training paradigm flexibilityã€knowledge transferabilityã€‚è¿™äº›èƒ½åŠ›ä¸ºåŸºäº[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)çš„äººä½“å§¿æ€ä¼°è®¡ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¼ºå£®çš„baselineï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚
3. å’Œæµè¡Œçš„benchmarkè¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥ç ”ç©¶å’Œåˆ†æViTPoseçš„æ€§èƒ½ã€‚å¦‚æœä½¿ç”¨big vision transformer modelä½œä¸ºbackboneï¼ˆæ¯”å¦‚ä½¿ç”¨ViTAE-Gä½œä¸ºbackboneï¼‰ï¼Œå•ä¸ªçš„ViTPoseæ¨¡å‹å¯ä»¥åœ¨MS COCO Keypoint test-dev setä¸Šå–å¾—æœ€é«˜çš„80.9çš„APã€‚

>ViTAE-GåŸæ–‡ï¼šQ. Zhang, Y. Xu, J. Zhang, and D. Tao. Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond. arXiv preprint arXiv:2202.10108, 2022.ã€‚

# 2.Related Work

## 2.1.Vision transformer for pose estimation

å§¿æ€ä¼°è®¡ç»å†äº†ä»CNNåˆ°vision transformer networksçš„å¿«é€Ÿå‘å±•ã€‚æ—©æœŸçš„å·¥ä½œæ›´å¤šçš„å°†transformerè§†ä¸ºä¸€ä¸ªå¥½çš„decoderï¼Œä¾‹å¦‚TransPoseå°†å…¶ç›´æ¥ç”¨äºå¤„ç†CNNæå–åˆ°çš„ç‰¹å¾ã€‚TokenPoseé€šè¿‡é¢å¤–çš„tokensæ¥å®ç°token-based representationsï¼Œä»è€Œä¼°è®¡è¢«é®æŒ¡å…³èŠ‚ç‚¹çš„ä½ç½®ï¼Œå¹¶ä¸”é’ˆå¯¹ä¸åŒå…³èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†ä¸å†ä½¿ç”¨CNNæå–ç‰¹å¾ï¼ŒHRFormerç›´æ¥ç”¨transformersæå–é«˜åˆ†è¾¨ç‡çš„ç‰¹å¾ã€‚è¿™äº›åŸºäºtransformerçš„å§¿æ€ä¼°è®¡æ–¹æ³•éƒ½åœ¨æµè¡Œçš„keypoint estimation benchmarksä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚ä½†æ˜¯ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆéœ€è¦CNNæå–ç‰¹å¾ï¼Œè¦ä¹ˆéœ€è¦ä»”ç»†è®¾è®¡transformerçš„æ¡†æ¶ç»“æ„ã€‚å®ƒä»¬å¹¶æ²¡æœ‰æ·±å…¥æ¢ç´¢plain vision transformersåœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºplain vision transformersçš„ViTPoseï¼Œå¡«è¡¥äº†è¿™ä¸€ç ”ç©¶çš„ç©ºç™½ã€‚

## 2.2.Vision transformer pre-training

å—åˆ°[ViT](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/)æˆåŠŸçš„å¯å‘ï¼Œå¤§é‡ä¸åŒçš„vision transformer backbonesè¢«æå‡ºï¼Œå®ƒä»¬é€šå¸¸ä½¿ç”¨ImageNet-1Kæ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£çš„è®­ç»ƒã€‚æœ€è¿‘ï¼Œè‡ªç›‘ç£å­¦ä¹ è¢«æå‡ºç”¨äºplain vision transformersçš„è®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå§¿æ€ä¼°è®¡ä»»åŠ¡ï¼Œä½¿ç”¨plain vision transformersä½œä¸ºbackbonesï¼Œå¹¶é‡‡ç”¨masked image modelingï¼ˆMIMï¼Œä¸ªäººç†è§£ï¼šMIMå’Œ[ViTä¸­çš„è‡ªç›‘ç£è®­ç»ƒ](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#46self-supervision)ç±»ä¼¼ï¼‰çš„æ–¹æ³•æ¥é¢„è®­ç»ƒbackbonesã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å¯¹äºå§¿æ€ä¼°è®¡ä»»åŠ¡ï¼Œæ˜¯å¦æœ‰å¿…è¦ä½¿ç”¨ImageNet-1Kè¿›è¡Œé¢„è®­ç»ƒã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨å°å‹æ— æ ‡ç­¾çš„pose datasetsè¿›è¡Œé¢„è®­ç»ƒä¾ç„¶å¯ä»¥ä¸ºå§¿æ€ä¼°è®¡ä»»åŠ¡æä¾›good initializationã€‚

# 3.ViTPose

## 3.1.The simplicity of ViTPose

ğŸ‘‰**Structure simplicity.**

æœ¬æ–‡çš„ç›®æ ‡æ˜¯ï¼š1ï¼‰ä¸ºå§¿æ€ä¼°è®¡ä»»åŠ¡æä¾›ä¸€ä¸ªç®€å•ä¸”æœ‰æ•ˆçš„vision transformer baselineï¼›2ï¼‰æ¢ç´¢plain and non-hierarchical vision transformersçš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç»“æ„ä¼šå°½å¯èƒ½çš„ç®€å•ï¼Œå°½é‡ä¸ä½¿ç”¨ä¸€äº›èŠ±å“¨å¤æ‚çš„æ¨¡å—ï¼Œå°½ç®¡è¿™äº›æ¨¡å—å¯èƒ½ä¼šå¸¦æ¥æ€§èƒ½ä¸Šçš„æå‡ã€‚æˆ‘ä»¬ä»…ä»…æ˜¯åœ¨transformer backboneåé¢æ·»åŠ äº†å‡ ä¸ªdecoder layersï¼Œç”¨ä»¥è®¡ç®—heatmapï¼Œä»è€Œè¿›ä¸€æ­¥å¾—åˆ°å…³èŠ‚ç‚¹ï¼Œè§Fig2(a)ã€‚ä¸ºäº†ç®€å•åŒ–ï¼Œæˆ‘ä»¬åœ¨decoder layersä¸­æ²¡æœ‰ä½¿ç”¨skip-connectionsæˆ–cross-attentionsï¼Œåªæ˜¯ä½¿ç”¨äº†ç®€å•çš„åå·ç§¯å±‚å’Œä¸€ä¸ªé¢„æµ‹å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œè¾“å…¥æ˜¯ä¸€å¼ person instance imageï¼ˆè¡¨ç¤ºä¸º$X \in \mathcal{R} ^ {\mathcal{H} \times \mathcal{W} \times 3}$ï¼‰ï¼Œé¦–å…ˆå°†åŸå§‹å›¾åƒåˆ’åˆ†ä¸º$d \times d$ä¸ªpatchï¼ˆ$d$é»˜è®¤ä¸º16ï¼‰ï¼Œæ¯ä¸ªpatchç»è¿‡patch embedding layeråå¾—åˆ°ä¸€ä¸ªembedded tokenï¼ˆå¯è¡¨ç¤ºä¸º$F \in \mathcal{R} ^ {\frac{H}{d} \times \frac{W}{d} \times C}$ï¼‰ã€‚éšåembedded tokensè¢«å¤šä¸ªtransformer layersç»§ç»­å¤„ç†ï¼Œæ¯ä¸ªtransformer layeråŒ…å«ä¸€ä¸ªmulti-head self-attention (MHSA) layerå’Œä¸€ä¸ªfeed-forward network (FFN)ï¼Œå³ï¼š

$$F'_{i+1} = F_i + \text{MHSA} ( \text{LN} (F_i)), F_{i+1} = F'_{i+1} + \text{FFN} ( \text{LN} (F'_{i+1})) \tag{1}$$

å…¶ä¸­ï¼Œ$F_0 = \text{PatchEmbed} (X)$ï¼Œå³patch embedding layerå¾—åˆ°çš„ç‰¹å¾ã€‚å¯¹äºæ¯ä¸ªtransformer layerï¼Œç»´åº¦éƒ½æ˜¯å›ºå®šä¸€æ ·çš„ã€‚æ‰€ä»¥ï¼Œbackboneçš„è¾“å‡ºç»´åº¦ä¸º$F_{out} \in \mathcal{R} ^ {\frac{H}{d} \times \frac{W}{d} \times C}$ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/2.png)

æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§è½»é‡çº§çš„decoderæ¥å¤„ç†backboneæå–åˆ°çš„ç‰¹å¾å¹¶å®šä½å…³èŠ‚ç‚¹ã€‚ç¬¬ä¸€ç§æ˜¯classic decoderã€‚å®ƒåŒ…å«ä¸¤ä¸ªåå·ç§¯blocksï¼Œæ¯ä¸ªblockåŒ…å«ä¸€ä¸ªåå·ç§¯å±‚ã€[batch normalization](http://shichaoxin.com/2021/11/02/è®ºæ–‡é˜…è¯»-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/)å±‚å’Œ[ReLU](http://shichaoxin.com/2019/12/11/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬ä¸ƒè¯¾-æ¿€æ´»å‡½æ•°/#22reluå‡½æ•°)å±‚ã€‚æ¯ä¸ªblockå°†feature mapä¸Šé‡‡æ ·2å€ã€‚æœ€åé€šè¿‡ä¸€ä¸ªkernel sizeä¸º$1\times 1$çš„å·ç§¯å±‚å¾—åˆ°ç”¨äºå®šä½å…³èŠ‚ç‚¹çš„heatmapï¼Œå³ï¼š

$$K = \text{Conv} _{1\times 1} (\text{Deconv} ( \text{Deconv} (F_{out}) )) \tag{2}$$

å…¶ä¸­ï¼Œheatmapå¯è¡¨ç¤ºä¸º$K \in \mathcal{R} ^ {\frac{H}{4} \times \frac{W}{4} \times N_k}$ï¼Œ$N_k$ä¸ºå…³èŠ‚ç‚¹çš„æ•°ç›®ï¼Œä¾‹å¦‚å¯¹äºMS COCO datasetï¼Œ$N_k=17$ã€‚

å°½ç®¡classic decoderå·²ç»è¶³å¤Ÿç®€å•å’Œè½»é‡çº§äº†ï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯å°è¯•äº†æ›´ç®€å•çš„decoderï¼Œå¹¶ä¸”å¾—ç›Šäºvision transformer backboneå¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œè¿™ç§æ›´ç®€å•çš„decoderæ•ˆæœä¹Ÿä¸é”™ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç›´æ¥é€šè¿‡[åŒçº¿æ€§æ’å€¼](http://shichaoxin.com/2021/06/29/OpenCVåŸºç¡€-ç¬¬äºŒåè¯¾-åƒç´ é‡æ˜ å°„/#32inter_linear)å°†feature mapä¸Šé‡‡æ ·4å€ï¼Œç„¶åæ¥ä¸€ä¸ª[ReLU](http://shichaoxin.com/2019/12/11/æ·±åº¦å­¦ä¹ åŸºç¡€-ç¬¬ä¸ƒè¯¾-æ¿€æ´»å‡½æ•°/#22reluå‡½æ•°)ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªkernel sizeä¸º$3 \times 3$çš„å·ç§¯å±‚æ¥è·å¾—heatmapï¼Œå³ï¼š

$$K = \text{Conv}_{3\times 3} (\text{Bilinear} (\text{ReLU} (F_{out})) ) \tag{3}$$

å°½ç®¡è¿™ç§decoderçš„non-linear capacityæ›´å°ï¼Œä½†æ˜¯ç›¸æ¯”classic decoderå’Œå…¶ä»–ç ”ç©¶ä¸­ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„transformer-based decodersç›¸æ¯”ï¼Œè¿™ç§ç®€å•çš„decoderä¾æ—§èƒ½è·å¾—competitive performanceã€‚

## 3.2.The scalability of ViTPose

æˆ‘ä»¬å¯ä»¥é€šè¿‡å †å ä¸åŒæ•°é‡çš„transformer layersä»¥åŠå¢å‡ç‰¹å¾ç»´åº¦æ¥è½»æ˜“çš„æ§åˆ¶æ¨¡å‹çš„å¤§å°ã€‚ä¸ºäº†ç ”ç©¶ViTPoseçš„scalabilityï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒå¤§å°çš„pre-trained backbonesï¼Œå¹¶åœ¨MS COCO datasetè¿›è¡Œfinetuneã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨[ViT-Bï¼ŒViT-Lï¼ŒViT-H](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#41setup)å’ŒViTAE-Gä½œä¸ºbackboneï¼Œå¹¶æ­é…classic decoderç”¨äºå§¿æ€ä¼°è®¡ï¼Œå‘ç°éšç€æ¨¡å‹å¤§å°çš„å¢åŠ ï¼Œæ€§èƒ½ä¸€ç›´æœ‰ç¨³æ­¥æå‡ã€‚å¯¹äº[ViT-H](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#41setup)å’ŒViTAE-Gï¼Œåœ¨é¢„è®­ç»ƒæ—¶è®¾ç½®patch sizeä¸º$14 \times 14$ï¼Œè€Œ[ViT-Bå’ŒViT-L](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#41setup)ä½¿ç”¨çš„patch sizeä¸º$16 \times 16$ï¼ˆéœ€è¦ä½¿ç”¨zero paddingä»¥ä¿è¯same settingï¼‰ã€‚

## 3.3.The flexibility of ViTPose

ğŸ‘‰**Pre-training data flexibility.**

ä¸ºäº†æ¢ç´¢æ•°æ®çš„çµæ´»æ€§ï¼Œé™¤äº†ä½¿ç”¨ImageNetè¿›è¡Œé¢„è®­ç»ƒå¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨MS COCOæ•°æ®é›†ä»¥åŠMS COCO+AI Challengerçš„è”åˆæ•°æ®é›†ä¸Šï¼Œé€šè¿‡MAEæ–¹æ³•å¯¹backbonesè¿›è¡Œé¢„è®­ç»ƒï¼Œå³éšæœºmaskæ‰75%çš„patchesï¼Œç„¶åå†å°†å®ƒä»¬é‡å»ºå‡ºæ¥ã€‚æ¥ç€åœ¨MS COCO datasetä¸Šfinetuneã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯æ˜ViTPoseå¯ä»¥ä»ä¸åŒå¤§å°çš„æ•°æ®é›†ä¸­å¾—åˆ°è‰¯å¥½çš„é¢„è®­ç»ƒç»“æœã€‚

ğŸ‘‰**Resolution flexibility.**

æˆ‘ä»¬é€šè¿‡æ”¹å˜è¾“å…¥å›¾åƒçš„å¤§å°å’Œä¸‹é‡‡æ ·æ¯”ä¾‹$d$æ¥æµ‹è¯•ViTPoseåœ¨è¾“å…¥åˆ†è¾¨ç‡å’Œç‰¹å¾åˆ†è¾¨ç‡ä¸Šçš„çµæ´»æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†ä½¿ViTPoseæ”¯æŒæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬åªéœ€è°ƒæ•´è¾“å…¥å›¾åƒçš„å¤§å°å¹¶ç›¸åº”åœ°è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä½¿æ¨¡å‹æ”¯æŒæ›´ä½çš„ä¸‹é‡‡æ ·æ¯”ä¾‹ï¼Œå³æ›´é«˜çš„ç‰¹å¾åˆ†è¾¨ç‡ï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•çš„æ›´æ”¹äº†patch embedding layerçš„strideå¹¶ä¿æŒæ¯ä¸ªpatchçš„sizeï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¹‹å‰çš„patchåˆ’åˆ†æ˜¯æ²¡æœ‰é‡å çš„ï¼Œè¿™æ ·æ“ä½œä¹‹åç›¸é‚»çš„patchä¹‹é—´æ˜¯å­˜åœ¨é‡å éƒ¨åˆ†çš„ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯æ›´é«˜çš„è¾“å…¥åˆ†è¾¨ç‡è¿˜æ˜¯æ›´é«˜çš„ç‰¹å¾åˆ†è¾¨ç‡ï¼ŒViTPoseçš„æ€§èƒ½éƒ½ä¼šæŒç»­æé«˜ã€‚

ğŸ‘‰**Attention type flexibility.**

åœ¨é«˜åˆ†è¾¨ç‡feature mapä¸Šä½¿ç”¨å®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶ä¼šå¯¼è‡´å·¨å¤§çš„å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚å› æ­¤æˆ‘ä»¬ä½¿ç”¨window-based attention with relative position embeddingæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯å¦‚æœæ‰€æœ‰çš„transformer blockséƒ½ä½¿ç”¨è¿™ç§æ–¹æ³•çš„è¯ä¼šå¯¼è‡´æ€§èƒ½çš„ä¸‹é™ï¼Œå› ä¸ºä¸§å¤±äº†å…¨å±€ä¿¡æ¯çš„ç¼˜æ•…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§æ–¹æ¡ˆï¼š

1. Shift window
	* ä¹‹å‰çš„window-based attentionä½¿ç”¨çš„æ˜¯ä¸€ä¸ªfixed windowsç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼Œè€Œç°åœ¨æˆ‘ä»¬æ”¹ç”¨shift-windowæœºåˆ¶ï¼ˆå¼•è‡ªSwin-Transformerï¼‰æ¥å¸®åŠ©ä¼ é€’ç›¸é‚»windowsçš„ä¿¡æ¯ã€‚
2. Pooling window
	* é™¤äº†shift-windowæœºåˆ¶ï¼Œæˆ‘ä»¬è¿˜å°è¯•äº†å¦ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å¯¹æ¯ä¸ªwindowå†…çš„tokensè¿›è¡Œpoolingæ“ä½œä»¥è·å¾—global context featureã€‚è¿™äº›ç‰¹å¾ä¼šè¢«å–‚å…¥æ¯ä¸ªwindowå½“ä½œkey tokenså’Œvalue tokensï¼Œä»¥æ­¤æ¥è·å¾—cross-window feature communicationï¼ˆè¿™é‡Œä¸å¤ªç†è§£ï¼Œkey tokenså’Œvalue tokensä¸åº”è¯¥æ˜¯å­¦å‡ºæ¥çš„å—ï¼Ÿï¼‰ã€‚

æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ä¸¤ç§æ–¹æ¡ˆæ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥ä¸€èµ·è¢«ç”¨æ¥æå‡æ€§èƒ½å¹¶é™ä½å†…å­˜å ç”¨ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„å‚æ•°å’Œæ¨¡å—ï¼Œåªéœ€è¦å¯¹æ³¨æ„åŠ›è®¡ç®—åšä¸€äº›ç®€å•çš„ä¿®æ”¹ã€‚

>window-based attention with relative position embeddingç›¸å…³çš„ä¸¤ç¯‡è®ºæ–‡ï¼š
>
>1. Y. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision transformer backbones for object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.ã€‚
>
>2. Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.ã€‚

ğŸ‘‰**Finetuning flexibility.**

æˆ‘ä»¬åˆ†åˆ«ç”¨3ç§æ–¹å¼åœ¨MS COCOæ•°æ®é›†ä¸Šfinetuneäº†ViTPoseï¼š1ï¼‰all parameters unfrozenï¼›2ï¼‰MHSA modules frozenï¼›3ï¼‰FFN modules frozenã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ–¹å¼2å–å¾—äº†å’Œæ–¹å¼1ä¸ç›¸ä¸Šä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

ğŸ‘‰**Task flexibility.**

ç”±äºViTPoseçš„decoderæ˜¯ç®€å•ä¸”è½»é‡çº§çš„ï¼Œå› æ­¤åœ¨å…±ç”¨åŒä¸€ä¸ªbackbone encoderçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿‡å¤šé¢å¤–çš„æˆæœ¬å°±å¯ä»¥ä½¿ç”¨å¤šä¸ªdecodersæ¥å¤„ç†å¤šä¸ªä¸åŒçš„å§¿æ€ä¼°è®¡æ•°æ®é›†ã€‚å¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬ä»å¤šä¸ªè®­ç»ƒæ•°æ®é›†ä¸­éšæœºé‡‡æ ·ã€‚

## 3.4.The transferability of ViTPose

æå‡å°æ¨¡å‹æ€§èƒ½çš„ä¸€ä¸ªå¸¸ç”¨æ–¹æ³•å°±æ˜¯å°†å¤§æ¨¡å‹çš„knowledgeè¿ç§»è¿‡æ¥ï¼Œæ¯”å¦‚knowledge distillationã€‚å‡è®¾æœ‰teacher network $T$å’Œstudent network $S$ï¼Œä¸€ä¸ªç®€å•çš„distillationæ–¹æ³•æ˜¯æ·»åŠ ä¸€ä¸ªdistillation loss $L_{t \to s}^{od}$æ¥ä¿ƒä½¿student networkçš„è¾“å‡ºå»æ¨¡ä»¿teacher networkçš„è¾“å‡ºï¼Œå³ï¼š

$$L_{t \to s}^{od} = \text{MSE}(K_s, K_t) \tag{4}$$

ç»™å®šåŒä¸€è¾“å…¥ï¼Œ$K_s,K_t$åˆ†åˆ«ä¸ºstudent networkå’Œteacher networkçš„è¾“å‡ºã€‚

>knowledge distillationç›¸å…³è®ºæ–‡ï¼š
>
>G. Hinton, O. Vinyals, J. Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.ã€‚
>
>J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789-1819, 2021.ã€‚

é€šè¿‡å¯¹ä¸Šè¿°æ–¹æ³•è¿›è¡Œè¿›ä¸€æ­¥çš„å®Œå–„ï¼Œæˆ‘ä»¬æå‡ºäº†token-based distillationæ–¹æ³•ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œå¯¹äºteacher modelï¼Œæˆ‘ä»¬éšæœºåˆå§‹åŒ–ä¸€ä¸ªé¢å¤–çš„learnable knowledge token $t$ï¼Œç„¶ååœ¨patch embedding layeråå°†å…¶æ·»åŠ åˆ°visual tokensã€‚ä¹‹åï¼Œæˆ‘ä»¬å†»ç»“è®­ç»ƒå¥½çš„teacher modelï¼Œåªå¯¹knowledge tokenè¿›è¡Œå¾®è°ƒï¼Œå¾®è°ƒä¼šæŒç»­å‡ ä¸ªepochsæ¥è·å¾—knowledgeï¼Œå³ï¼š

$$t^* = \arg \min \limits_{t} (\text{MSE} (T(\{t;X\}), K_{gt}) ) \tag{5}$$

å…¶ä¸­ï¼Œ$K_{gt}$ä¸ºground truth heatmapsï¼Œ$X$ä¸ºè¾“å…¥å›¾åƒï¼Œ$T(\\{ t;X \\})$ä¸ºteacher modelçš„é¢„æµ‹è¾“å‡ºï¼Œ$t^\*$ä¸ºæœ€å°åŒ–losså¾—åˆ°çš„æœ€ä¼˜knowledge tokenã€‚åœ¨è¿™ä¹‹åï¼Œknowledge token $t^\*$ä¼šå’Œstudent networkçš„visual tokens concatåœ¨ä¸€èµ·å‚ä¸student networkçš„è®­ç»ƒï¼Œå¹¶ä¸”åœ¨æ­¤æœŸé—´knowledge token $t^\*$æ˜¯è¢«å†»ç»“çš„çŠ¶æ€ï¼Œå³ä¸å†æ”¹å˜ï¼Œè¿™æ ·å°±å¯ä»¥æŠŠteacher networkçš„knowledgeä¼ é€’ç»™student networkäº†ã€‚å› æ­¤ï¼Œstudent networkçš„losså¯è¡¨ç¤ºä¸ºï¼š

$$L_{t \to s}^{td} = \text{MSE}(S(\{t^*;X \}), K_{gt} ) \tag{6}$$

æˆ–

$$L_{t \to s}^{tod} = \text{MSE}(S( \{t^*;X \}),K_t ) + \text{MSE}(S( \{t^*;X \}),K_{gt} ) \tag{6}$$

$L_{t \to s}^{td}$è¡¨ç¤ºçš„æ˜¯token distillation lossï¼Œ$L_{t \to s}^{tod}$è¡¨ç¤ºçš„æ˜¯output distillation losså’Œtoken distillation lossçš„è”åˆã€‚

# 4.Experiments

## 4.1.Implementation details

ViTPoseéµå¾ªäººä½“å§¿æ€ä¼°è®¡ä¸­å¸¸è§çš„top-down settingï¼Œå³detectorç”¨äºæ£€æµ‹person instancesï¼ŒViTPoseç”¨äºæ£€æµ‹instancesçš„å…³èŠ‚ç‚¹ã€‚æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨[ViT-Bï¼ŒViT-Lï¼ŒViT-H](http://shichaoxin.com/2022/09/22/è®ºæ–‡é˜…è¯»-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/#41setup)ä½œä¸ºbackbonesï¼Œå¹¶å°†ç›¸åº”çš„æ¨¡å‹è¡¨ç¤ºä¸ºViTPose-Bï¼ŒViTPose-Lï¼ŒViTPose-Hã€‚æ¨¡å‹åŸºäºmmpose codebaseï¼Œåœ¨8å—A100 GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨MAEå¯¹backbonesè¿›è¡Œé¢„è®­ç»ƒã€‚ä½¿ç”¨mmposeä¸­çš„é»˜è®¤è®­ç»ƒè®¾ç½®æ¥è®­ç»ƒViTPoseæ¨¡å‹ï¼Œå³ï¼Œè¾“å…¥åˆ†è¾¨ç‡ä¸º$256 \times 192$ï¼ŒAdamW optimizerï¼ˆå­¦ä¹ ç‡ä¸º5e-4ï¼‰ã€‚Udpè¢«ç”¨äºåå¤„ç†ã€‚æ¨¡å‹ä¸€å…±è®­ç»ƒäº†210ä¸ªepochsï¼Œå…¶ä¸­åœ¨ç¬¬170å’Œ200ä¸ªepochæ—¶å­¦ä¹ ç‡è¡°å‡10å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ¯ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨äº†layer-wise learning rate decayï¼ˆä¸€ç§å¯¹å­¦ä¹ ç‡é€å±‚ä¿®æ­£çš„ç­–ç•¥ï¼‰å’Œdrop pathï¼ˆå°†æ·±åº¦å­¦ä¹ ç½‘ç»œä¸­çš„å¤šåˆ†æ”¯ç»“æ„éšæœºåˆ é™¤çš„ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼‰ã€‚ç»è¿‡æˆ‘ä»¬çš„å®éªŒï¼Œåœ¨è¡¨1ä¸­åˆ—å‡ºäº†æœ€ä¼˜çš„å‚æ•°è®¾ç½®ï¼š

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/3.png)

è¡¨1ä¸­åˆ—å‡ºäº†è®­ç»ƒViTPoseçš„æœ€ä¼˜è¶…å‚æ•°ï¼Œå…¶ä¸­ï¼Œæ–œæ å‰çš„å‚æ•°è¡¨ç¤ºä»…åœ¨MS COCOæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ–œæ åçš„å‚æ•°è¡¨ç¤ºåœ¨multi-datasetä¸Šè®­ç»ƒã€‚

>mmpose codebaseï¼šM. Contributors. Openmmlab pose estimation toolbox and benchmark. [https://github.com/open-mmlab/mmpose](https://github.com/open-mmlab/mmpose), 2020.ã€‚
>
>AdamW optimizerï¼šS. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.ã€‚
>
>Udpï¼šJ. Huang, Z. Zhu, F. Guo, and G. Huang. The devil is in the details: Delving into unbiased data processing for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.ã€‚
>
>layer-wise learning rate decayï¼šZ.Yang, Z.Dai, Y.Yang, J.Carbonell, R.R.Salakhutdinov, and Q.V.Le. Xlnet : Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.ã€‚

## 4.2.Ablation study and analysis

ğŸ‘‰**The structure simplicity and scalability.**

æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ç¬¬3.1éƒ¨åˆ†æåˆ°çš„classic decoderå’Œsimple decoderæ¥è®­ç»ƒViTPoseã€‚ä½œä¸ºæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†ä½¿ç”¨[ResNet](http://shichaoxin.com/2022/01/07/è®ºæ–‡é˜…è¯»-Deep-Residual-Learning-for-Image-Recognition/)ä½œä¸ºbackbonesçš„SimpleBaselineï¼Œå¹¶ä¸”ä¹Ÿåˆ†åˆ«æ­é…ä¸¤ç§ä¸åŒçš„decodersã€‚ç»“æœè§è¡¨2ã€‚ä»è¡¨2ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºResNet-50å’ŒResNet-152æ¥è¯´ï¼Œç›¸æ¯”classic decoderï¼Œä½¿ç”¨simple decoderä¼šå¯¼è‡´APé™ä½18ä¸ªç‚¹å·¦å³ã€‚ç„¶è€Œå¯¹äºViTPoseæ¥è¯´ï¼Œç›¸æ¯”classic decoderï¼Œä½¿ç”¨simple decoderåªä¼šå¯¼è‡´APé™ä½0.3ä¸ªç‚¹å·¦å³ã€‚å¯¹äºæŒ‡æ ‡$\text{AP}\_{50}$å’Œ$\text{AR}\_{50}$ï¼Œæ— è®ºä½¿ç”¨å“ªç§decoderï¼ŒViTPoseçš„è¡¨ç°éƒ½å·®ä¸å¤šï¼Œè¿™è¯´æ˜plain vision transformeræœ‰ç€å¾ˆå¼ºçš„representationèƒ½åŠ›ï¼Œå¹¶ä¸”å¤æ‚çš„decoderä¸æ˜¯å¿…é¡»çš„ã€‚æ­¤å¤–ï¼Œä»è¡¨2ä¸­è¿˜å¯ä»¥å¾—å‡ºç»“è®ºï¼ŒViTPoseçš„æ€§èƒ½éšç€æ¨¡å‹å¤§å°çš„å¢åŠ è€Œä¸æ–­æå‡ï¼Œè¿™ä¹Ÿè¯æ˜äº†ViTPoseæœ‰ç€è‰¯å¥½çš„scalabilityã€‚

>SimpleBaselineï¼šB. Xiao, H. Wu, and Y. Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), 2018.ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/4.png)

ğŸ‘‰**The influence of pre-training data.**

ä¸ºäº†è¯„ä¼°ImageNet-1Kæ•°æ®é›†å¯¹äºå§¿æ€ä¼°è®¡ä»»åŠ¡æ˜¯å¦æ˜¯å¿…è¦çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†å¯¹backboneè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¯”å¦‚ï¼šImageNet-1Kï¼ŒMS COCOä»¥åŠMS COCOå’ŒAI Challengerçš„è”åˆæ•°æ®é›†ã€‚ä¸ºäº†å’ŒImageNet-1Kæ•°æ®é›†ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†MS COCOå’ŒAI Challengerä¸­çš„å›¾åƒè£å‰ªå¾—åˆ°person instancesï¼Œç”¨ä½œé¢„è®­ç»ƒçš„æ•°æ®é›†ã€‚æ¨¡å‹åœ¨è¿™ä¸‰ä¸ªæ•°æ®é›†ä¸Šéƒ½åˆ†åˆ«é¢„è®­ç»ƒäº†1600ä¸ªepochï¼Œç„¶ååœ¨MS COCOæ•°æ®é›†ä¸Šfinetuneäº†210ä¸ªepochã€‚ç»“æœè§è¡¨3ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨MS COCOå’ŒAI Challengerè”åˆæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„ç»“æœå’Œä½¿ç”¨ImageNet-1kå·®ä¸å¤šã€‚ä½†æ˜¯å…¶æ•°æ®é‡åªæ˜¯ImageNet-1kçš„ä¸€åŠå·¦å³ã€‚è¿™éªŒè¯äº†ViTPoseåœ¨é¢„è®­ç»ƒæ•°æ®æ–¹é¢çš„flexibilityã€‚ç„¶è€Œï¼Œå¦‚æœä»…ä½¿ç”¨MS COCOæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒä¼šå¯¼è‡´APä¸‹é™1.3ä¸ªç‚¹å·¦å³ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºMS COCOæ•°æ®é›†çš„æ•°æ®é‡è¿‡å°ï¼ŒMS COCOä¸­çš„instancesæ•°é‡æ¯”MS COCOå’ŒAI Challengerè”åˆæ•°æ®é›†å°‘äº†å¤§çº¦3å€ã€‚æ­¤å¤–ï¼Œå¦‚æœä½¿ç”¨MS COCOå’ŒAI Challengerè”åˆæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œæ— è®ºæ˜¯å¦è£å‰ªï¼Œæœ€ç»ˆç»“æœéƒ½å·®ä¸å¤šã€‚è¿™äº›å‘ç°éªŒè¯äº†ä»¥ä¸‹ç»“è®ºï¼šä¸‹æ¸¸ä»»åŠ¡æœ¬èº«çš„æ•°æ®å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µå¸¦æ¥æ›´å¥½çš„æ•°æ®æ•ˆç‡ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/5.png)

ğŸ‘‰**The influence of input resolution.**

ä¸ºäº†è¯„ä¼°ViTPoseæ˜¯å¦å¯ä»¥å¾ˆå¥½çš„é€‚åº”ä¸åŒçš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„è¾“å…¥å›¾åƒå°ºå¯¸æ¥è®­ç»ƒViTPoseï¼Œæœ€ç»ˆç»“æœè§è¡¨4ã€‚éšç€è¾“å…¥åˆ†è¾¨ç‡çš„å¢åŠ ï¼ŒViTPose-Bçš„æ€§èƒ½ä¹Ÿä¸€ç›´åœ¨æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ³¨æ„åˆ°ï¼Œå¹³æ–¹è¾“å…¥è™½ç„¶å…·æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œä½†å¹¶æ²¡æœ‰å¸¦æ¥å¤ªå¤šçš„æ€§èƒ½æå‡ï¼Œæ¯”å¦‚ï¼Œ$256 \times 256$ vs. $256 \times 192$ã€‚åŸå› å¯èƒ½æ˜¯å› ä¸ºMS COCOæ•°æ®é›†ä¸­human instancesçš„å¹³å‡é•¿å®½æ¯”ä¸º$4:3$ï¼Œè€Œå¹³æ–¹è¾“å…¥ä¸æ»¡è¶³è¿™ä¸€ç»Ÿè®¡ä¿¡æ¯ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/6.png)

ğŸ‘‰**The influence of attention type.**

[HRNet](http://shichaoxin.com/2023/05/13/è®ºæ–‡é˜…è¯»-Deep-High-Resolution-Representation-Learning-for-Visual-Recognition/)å’ŒHRFormeræå‡ºé«˜åˆ†è¾¨ç‡feature mapsæœ‰åˆ©äºå§¿æ€ä¼°è®¡ä»»åŠ¡ã€‚ViTPoseå¯ä»¥é€šè¿‡æ”¹å˜patching embedding layerçš„ä¸‹é‡‡æ ·ç‡ï¼ˆæ¯”å¦‚ä»patch size $16 \times 16$åˆ°$8\times 8$ï¼‰æ¥å®¹æ˜“åœ°ç”Ÿæˆé«˜åˆ†è¾¨ç‡featuresã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£ç”±quadratic computational complexityå¯¼è‡´çš„å†…å­˜ä¸è¶³é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ç¬¬3.3éƒ¨åˆ†æåˆ°çš„Shift windowæœºåˆ¶ï¼ˆè¡¨5ä¸­çš„'Shift'ï¼‰å’ŒPooling windowæœºåˆ¶ï¼ˆè¡¨5ä¸­çš„'Pool'ï¼‰ã€‚ç»“æœè§è¡¨5ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨åŸå§‹çš„æ³¨æ„åŠ›æœºåˆ¶æ­é…1/8 feature sizeå¾—åˆ°äº†æœ€é«˜çš„77.4çš„APï¼Œä½†æ˜¯å…¶å†…å­˜å ç”¨ä¹Ÿéå¸¸é«˜ã€‚window attentionå¯ä»¥é™ä½å†…å­˜å ç”¨ï¼Œä½†ç”±äºç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ä¿¡æ¯ï¼ŒAPç”±77.4é™ä½åˆ°66.4ã€‚Shift windowæœºåˆ¶å’ŒPooling windowæœºåˆ¶é€šè¿‡è·¨çª—å£ä¿¡æ¯äº¤æ¢æ¥è·å¾—å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ä¿¡æ¯ï¼Œå› æ­¤ç›¸æ¯”å•çº¯çš„window attentionï¼ŒAPæé«˜äº†10ä¸ªç‚¹ï¼Œå¹¶ä¸”å†…å­˜å ç”¨å¢åŠ ä¸åˆ°10%ã€‚å¦‚æœåŒæ—¶ä½¿ç”¨è¿™ä¸¤ç§æœºåˆ¶ï¼ŒAPè¿›ä¸€æ­¥æå‡è‡³76.8ï¼Œè¿™ä¸ViTDetçš„è¡¨ç°å·®ä¸å¤šï¼ˆè¡¨5ç¬¬6è¡Œï¼‰ï¼Œä½†æ˜¯ViTDetæ˜¯è”åˆä½¿ç”¨äº†full attentionå’Œwindow attentionï¼Œè™½ç„¶è¡¨ç°å·®ä¸å¤šï¼ˆ76.8 AP vs. 76.9 APï¼‰ï¼Œä½†æ˜¯å‰è€…å†…å­˜å ç”¨æ›´ä½ï¼ˆ22.9G memory vs. 28.6G memoryï¼‰ã€‚é€šè¿‡æ¯”è¾ƒè¡¨5çš„ç¬¬5è¡Œå’Œæœ€åä¸€è¡Œï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å¯ä»¥é€šè¿‡å°†çª—å£å¤§å°ä»$8 \times 8$æ‰©å¤§åˆ°$16 \times 12$ï¼Œä½¿æ€§èƒ½è¿›ä¸€æ­¥ä»76.8 APæå‡è‡³77.1 APï¼Œè¿™ä¸€ç»“æœè¦ä¼˜äºViTDetçš„è”åˆç­–ç•¥ã€‚

>ViTDetï¼šY. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision transformer backbones for object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/7.png)

è¡¨5æ˜¯ViTPoseæ­é…1/8 feature sizeåœ¨MS COCO val setä¸Šçš„è¡¨ç°ã€‚`*`è¡¨ç¤ºç”±äºç¡¬ä»¶å†…å­˜é™åˆ¶ï¼Œåœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº†fp16ç²¾åº¦ã€‚å¯¹äºfull attentionï¼ˆ'Full'ï¼‰å’Œwindow attentionï¼ˆ'Window'ï¼‰çš„è”åˆç­–ç•¥ï¼Œæˆ‘ä»¬éµå¾ªViTDetä¸­çš„è®¾ç½®ã€‚

ğŸ‘‰**The influence of partially finetuning.**

ä¸ºäº†è¯„ä¼°ViTæ˜¯å¦å¯ä»¥é€šè¿‡éƒ¨åˆ†finetuneæ¥é€‚åº”å§¿æ€ä¼°è®¡ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡3ç§ä¸åŒçš„æ–¹å¼æ¥finetune ViTPose-Bï¼š1ï¼‰fully finetuningï¼›2ï¼‰å†»ç»“MHSAæ¨¡å—ï¼›3ï¼‰å†»ç»“FFNæ¨¡å—ã€‚ç»“æœè§è¡¨6ï¼Œç›¸æ¯”fully finetuningï¼Œå†»ç»“MHSAæ¨¡å—å¯¼è‡´äº†è½»å¾®çš„æ€§èƒ½ä¸‹é™ï¼ˆ75.1 AP v.s. 75.8 APï¼‰ï¼Œä½†æ˜¯è¿™ä¸¤ç§æ–¹å¼çš„$\text{AP}_{50}$å·®ä¸å¤šã€‚ä½†æ˜¯å¦‚æœå†»ç»“FFNæ¨¡å—ï¼ŒAPä¼šæ˜¾è‘—ä¸‹é™3.0ä¸ªç‚¹ã€‚è¿™ä¸€å‘ç°è¯´æ˜ViTçš„FFNæ¨¡å—ä¼šæ›´è´Ÿè´£ç‰¹å®šä»»åŠ¡ï¼ˆtask-specificï¼‰çš„å»ºæ¨¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMHSAæ¨¡å—æ›´å…·æœ‰ä»»åŠ¡æ— å…³æ€§ï¼ˆtask-agnosticï¼‰ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/8.png)

ğŸ‘‰**The influence of multi-dataset training.**

ç”±äºViTPoseçš„decoderç›¸å½“ç®€å•ä¸”è½»é‡çº§ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªæ•°æ®é›†ä½¿ç”¨å…±äº«çš„backboneå’Œå•ç‹¬çš„decoderï¼Œä»è€Œè½»æ¾çš„å°†ViTPoseæ‰©å±•åˆ°å¤šæ•°æ®é›†è”åˆè®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨MS COCOã€AI Challengerå’ŒMPIIæ¥è¿›è¡Œå¤šæ•°æ®é›†è”åˆè®­ç»ƒå®éªŒã€‚åœ¨MS COCO val setä¸Šçš„å®éªŒç»“æœè§è¡¨7ã€‚æ›´å¤šæ•°æ®é›†çš„å®éªŒç»“æœè§é™„å½•ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ²¡æœ‰åœ¨MS COCOä¸Šè¿›ä¸€æ­¥finetuneã€‚ä»è¡¨7ä¸­å¯ä»¥çœ‹åˆ°ï¼Œéšç€æ›´å¤šçš„æ•°æ®é›†åŠ å…¥è®­ç»ƒï¼ŒViTPoseçš„æ€§èƒ½ä¹Ÿåœ¨ç¨³æ­¥æå‡ï¼ˆ75.8 APåˆ°77.1 APï¼‰ã€‚å°½ç®¡ä¸MS COCO+AI Challengerçš„è”åˆæ•°æ®é›†ç›¸æ¯”ï¼ŒMPIIæ•°æ®é›†çš„æ•°æ®é‡è¦å°å¾ˆå¤šï¼ˆ40K v.s. 500Kï¼‰ï¼Œä½†MPIIçš„åŠ å…¥è¿˜æ˜¯è®©APæå‡äº†0.1ä¸ªç‚¹ï¼Œè¿™è¯´æ˜ViTPoseå¯ä»¥å¾ˆå¥½çš„åˆ©ç”¨ä¸åŒæ•°æ®é›†ä¸­çš„ä¸åŒæ•°æ®ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/9.png)

ğŸ‘‰**The analysis of transferability.**

ä¸ºäº†è¯„ä¼°ViTPoseçš„transferabilityï¼Œæˆ‘ä»¬ä½¿ç”¨äº†2ç§æ–¹æ³•æ¥å°†ViTPose-Lçš„knowledgeè¿ç§»ç»™ViTPose-Bï¼Œä¸€ç§æ–¹æ³•æ˜¯ç¬¬3.4éƒ¨åˆ†ä¸­ç®€å•çš„distillationæ–¹æ³•ï¼ˆè¡¨8ä¸­çš„'Heatmap'ï¼‰ï¼Œå¦ä¸€ç§æ˜¯æˆ‘ä»¬æå‡ºæ¥çš„token-based distillationæ–¹æ³•ï¼ˆè¡¨8ä¸­çš„'Token'ï¼‰ã€‚å®éªŒç»“æœè§è¡¨8ã€‚ä»è¡¨8ä¸­å¯ä»¥çœ‹åˆ°ï¼Œtoken-based distillationæ–¹æ³•ç»™ViTPose-Bå¸¦æ¥äº†0.2 APçš„æå‡ï¼Œå¹¶ä¸”å†…å­˜å ç”¨æ²¡æœ‰å¢åŠ å¾ˆå¤šï¼Œè€Œç®€å•çš„distillationæ–¹æ³•å¸¦æ¥äº†0.5 APçš„å¢é•¿ã€‚æ­¤å¤–ï¼Œè¿™ä¸¤ç§æ–¹æ³•æ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼Œæœ€ç»ˆå¾—åˆ°76.6çš„APï¼Œè¿™äº›ç»“æœè¯´æ˜äº†ViTPoseå…·æœ‰ä¼˜ç§€çš„transferabilityã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/10.png)

## 4.3.Comparison with SOTA methods

åŸºäºå…ˆå‰çš„åˆ†æï¼Œæˆ‘ä»¬ä½¿ç”¨$256 \times 192$çš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œè¿›è¡Œå¤šæ•°æ®é›†è”åˆè®­ç»ƒï¼Œå¹¶åœ¨MS COCO val and test-dev setä¸Šè¿›è¡ŒéªŒè¯ï¼Œç»“æœè§è¡¨9å’Œè¡¨10ã€‚æ‰€æœ‰æ–¹æ³•çš„é€Ÿåº¦æµ‹è¯•éƒ½åŸºäºå•å—A100 GPUï¼Œbatch size=64ã€‚ä»ç»“æœä¸­å¯ä»¥çœ‹å‡ºï¼Œå°½ç®¡ViTPoseçš„æ¨¡å‹å¾ˆå¤§ï¼Œä½†å®ƒåœ¨throughputå’Œaccuracyä¹‹é—´æœ‰ç€å¾ˆå¥½çš„trade-offï¼Œè¿™è¡¨æ˜plain vision transformeræœ‰ç€å¾ˆå¼ºçš„representationèƒ½åŠ›ï¼Œå¹¶ä¸”å¯¹ç¡¬ä»¶å‹å¥½ã€‚æ­¤å¤–ï¼Œbackboneè¶Šå¤§ï¼ŒViTPoseçš„æ€§èƒ½è¶Šå¥½ã€‚æ¯”å¦‚ï¼ŒViTPose-Lçš„è¡¨ç°è¦æ¯”ViTPose-Bå¥½ï¼ˆ78.3 AP v.s. 75.8 APï¼Œ83.5 AR v.s. 81.1 ARï¼‰ã€‚ViTPose-Lçš„è¡¨ç°ä¼˜äºä¹‹å‰SOTAçš„CNNæ¨¡å‹å’Œtransformeræ¨¡å‹ã€‚åœ¨ä»…ä½¿ç”¨MS COCOæ•°æ®é›†ç”¨äºè®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒViTPose-Hçš„æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦å‡ä¼˜äºHRFormer-Bï¼ˆ79.1 AP v.s. 75.6 APï¼Œ241 fps v.s. 158 fpsï¼‰ã€‚ç›¸æ¯”HRFormer-Bï¼ŒViTPoseå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå› ä¸ºå…¶ç»“æ„ä»…åŒ…å«ä¸€ä¸ªbranchï¼Œå¹¶ä¸”åœ¨ç›¸å¯¹è¾ƒå°çš„featureåˆ†è¾¨ç‡ä¸Šæ“ä½œï¼ˆ1/4 v.s. 1/16ï¼‰ã€‚å¦‚æœä½¿ç”¨å¤šæ•°æ®é›†è”åˆè®­ç»ƒï¼ŒViTPoseçš„æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥çš„æå‡ï¼Œè¿™æ„å‘³ç€ViTPoseæœ‰ç€è‰¯å¥½çš„scalabilityå’Œflexibilityã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/11.png)

è¡¨9æ˜¯åœ¨MS COCO val setä¸Šï¼ŒViTPoseå’ŒSOTAæ–¹æ³•çš„æ¯”è¾ƒç»“æœã€‚`*`è¡¨ç¤ºå¤šæ•°æ®é›†è”åˆè®­ç»ƒã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªæ›´å¼ºå£®çš„æ¨¡å‹ViTPose-Gï¼Œå³ä½¿ç”¨ViTAE-Gä½œä¸ºbackboneï¼Œå‚æ•°é‡è¾¾åˆ°äº†1Bï¼Œæœ‰ç€æ›´å¤§çš„è¾“å…¥åˆ†è¾¨ç‡ï¼ˆ$576 \times 432$ï¼‰ï¼Œåœ¨MS COCO+AI Challengerè”åˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å’Œå…¶ä»–SOTAæ–¹æ³•çš„æ¯”è¾ƒè§è¡¨10ï¼Œåœ¨MS COCO test-dev setä¸Šï¼Œå•ä¸ªçš„ViTPose-Gæ¨¡å‹ä¼˜äºä¹‹å‰æ‰€æœ‰çš„SOTAæ–¹æ³•ï¼Œè¾¾åˆ°äº†80.9çš„APï¼Œä¹‹å‰æœ€ä¼˜çš„æ–¹æ³•UDP++ï¼Œé›†æˆäº†17ä¸ªæ¨¡å‹æ‰è¾¾åˆ°80.8çš„APã€‚å¦‚æœé›†æˆ3ä¸ªæ¨¡å‹ï¼ŒViTPoseæœ€ç»ˆè¾¾åˆ°81.1çš„APã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/12.png)

## 4.4.Subjective results

æˆ‘ä»¬è¿˜å¯è§†åŒ–äº†ViTPoseåœ¨MS COCOæ•°æ®é›†ä¸Šçš„å§¿æ€ä¼°è®¡ç»“æœã€‚ç»“æœè§Fig3ï¼Œå¯¹äºä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„caseï¼Œæ¯”å¦‚å¾ˆä¸¥é‡çš„é®æŒ¡ã€ä¸åŒçš„å§¿åŠ¿ã€ä¸åŒçš„å¤§å°ï¼ŒViTPoseæ€»èƒ½é¢„æµ‹å‡ºå‡†ç¡®çš„å§¿æ€ä¼°è®¡ç»“æœã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/13.png)

# 5.Limitation and Discussion

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå§¿æ€ä¼°è®¡çš„ç®€å•ä¸”æœ‰æ•ˆçš„vision transformer baselineï¼šViTPoseã€‚å°½ç®¡åœ¨ç»“æ„ä¸Šæ²¡æœ‰ç²¾å¿ƒè®¾è®¡ï¼Œä½†æ˜¯ViTPoseä¾ç„¶åœ¨MS COCOæ•°æ®é›†ä¸Šè·å¾—äº†SOTAçš„è¡¨ç°ã€‚ä½†æ˜¯ViTPoseçš„æ½œåŠ›å°šæœªè¢«å®Œå…¨å‘æ˜ï¼Œæ¯”å¦‚ä½¿ç”¨æ›´å¤æ‚çš„decodersæˆ–[FPN](http://shichaoxin.com/2023/12/19/è®ºæ–‡é˜…è¯»-Feature-Pyramid-Networks-for-Object-Detection/)ç»“æ„ï¼Œéƒ½æœ‰å¯èƒ½ä½¿å¾—æ€§èƒ½è¢«è¿›ä¸€æ­¥æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç›¸ä¿¡ViTPoseä¹Ÿå¯ä»¥è¢«åº”ç”¨äºå…¶ä»–å§¿æ€ä¼°è®¡æ•°æ®é›†ï¼Œæ¯”å¦‚åŠ¨ç‰©å§¿æ€ä¼°è®¡ï¼Œé¢éƒ¨å…³é”®ç‚¹æ£€æµ‹ç­‰ã€‚æœªæ¥æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚

# 6.Conclusion

æœ¬æ–‡æå‡ºViTPoseå¯ä»¥ä½œä¸ºäººä½“å§¿æ€ä¼°è®¡ä¸­åŸºäºvision transformerçš„ç®€å•baselineã€‚é€šè¿‡åœ¨MS COCOæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†ViTPoseåœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­æœ‰ç€è‰¯å¥½çš„simplicityã€scalabilityã€flexibilityå’Œtransferabilityã€‚å•ä¸ªçš„ViTPose-Gæ¨¡å‹åœ¨MS COCO test-dev setä¸Šè·å¾—äº†æœ€ä¼˜çš„80.9çš„APã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ–‡å¯ä»¥æ¿€å‘å‡ºæ›´å¤šç ”ç©¶æ¥æ¢ç´¢plain vision transformersåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

# 7.A.Additional results of multi-dataset training

ä¸ºäº†æ›´å…¨é¢çš„è¯„ä¼°ViTPoseçš„æ€§èƒ½ï¼Œé™¤äº†åœ¨MS COCO val setä¸Šæµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ViTPose-Bï¼ŒViTPose-Lï¼ŒViTPose-Hä»¥åŠViTPose-Gåˆ†åˆ«åœ¨OCHuman val and test setï¼ŒMPII val setå’ŒAI Challenger val setä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚è¯·æ³¨æ„ï¼ŒViTPoseçš„å˜ä½“éƒ½æ˜¯åœ¨å¤šæ•°æ®é›†è”åˆçš„æƒ…å†µä¸‹è®­ç»ƒçš„ï¼Œå¹¶ä¸”æ²¡æœ‰åœ¨ç‰¹å®šè®­ç»ƒé›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„finetuneã€‚

ğŸ‘‰**OCHuman val and test set.**

ä¸ºäº†è¯„ä¼°äººä½“å§¿æ€æ¨¡å‹åœ¨human instancesè¢«ä¸¥é‡é®æŒ¡æƒ…å†µä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ViTPoseå˜ä½“ä»¥åŠå…¶ä»–ä¸€äº›ä»£è¡¨æ€§æ¨¡å‹åœ¨OCHuman val and test setä¸Šçš„è¡¨ç°ã€‚å› ä¸ºOCHumanæ•°æ®é›†ä¸­å¹¶éæ‰€æœ‰çš„human instanceséƒ½è¢«æ ‡æ³¨äº†å‡ºæ¥ï¼Œæ‰€ä»¥ä¼šå¯¼è‡´å¤§é‡çš„â€œfalse positiveâ€ bounding boxesï¼Œä»è€Œæ— æ³•åæ˜ å§¿æ€ä¼°è®¡æ¨¡å‹çš„çœŸå®èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸MS COCOæ•°æ®é›†ç›¸å¯¹åº”çš„ViTPose decoder headï¼Œå› ä¸ºMS COCOå’ŒOCHumanæ•°æ®é›†ä¸­çš„å…³èŠ‚ç‚¹å®šä¹‰ç›¸åŒã€‚ç»“æœè§è¡¨11ã€‚ä¸ä¹‹å‰å…·æœ‰å¤æ‚ç»“æ„çš„SOTAçš„æ–¹æ³•ç›¸æ¯”ï¼ˆæ¯”å¦‚MIPNetï¼‰ï¼Œåœ¨OCHuman val setä¸Šï¼ŒViTPoseå°†APæå‡äº†è¶…è¿‡10ä¸ªç‚¹ï¼Œå¹¶ä¸”æˆ‘ä»¬å¹¶æœªé’ˆå¯¹é®æŒ¡åšç‰¹æ®Šçš„è®¾è®¡å¤„ç†ï¼Œè¿™è¯´æ˜äº†ViTPoseå…·æœ‰å¼ºå¤§çš„feature representation abilityã€‚å¹¶ä¸”å¯ä»¥æ³¨æ„åˆ°ï¼Œç›¸æ¯”åœ¨MS COCOä¸Šçš„è¡¨ç°ï¼ŒHRFormeråœ¨OCHumanæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰ç€å·¨å¤§çš„ä¸‹æ»‘ã€‚è¿™äº›ç°è±¡æ„å‘³ç€HRFormerå¯èƒ½è¿‡åº¦æ‹ŸåˆMS COCOæ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºlager-scale modelsæ¥è¯´ï¼Œå¹¶ä¸”éœ€è¦é¢å¤–çš„finetuneæ‰èƒ½ä»MS COCOè½¬ç§»åˆ°OCHumanã€‚æ­¤å¤–ï¼Œæ— è®ºæ˜¯val setè¿˜æ˜¯test setï¼ŒViTPoseéƒ½æ˜¾è‘—æå‡äº†ä¹‹å‰çš„æœ€ä¼˜æˆç»©ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒViTPoseå¯ä»¥çµæ´»åœ°å¤„ç†å…·æœ‰ä¸¥é‡é®æŒ¡çš„æ•°æ®ï¼Œå¹¶è·å¾—SOTAçš„æ€§èƒ½ã€‚

>MIPNetï¼šR. Khirodkar, V. Chari, A. Agrawal, and A. Tyagi. Multi-instance pose networks: Rethinking top-down pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3122â€“3131, 2021.ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/14.png)

ğŸ‘‰**MPII val set.**

åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨MPII val setä¸Šåšäº†å®éªŒã€‚éµå¾ªMPIIçš„é»˜è®¤è®¾ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨PCKhä½œä¸ºæ€§èƒ½è¯„ä¼°æŒ‡æ ‡ã€‚å¦‚è¡¨12æ‰€ç¤ºï¼Œæ— è®ºæ˜¯å•ä¸ªå…³èŠ‚ç‚¹è¯„ä¼°è¿˜æ˜¯å¹³å‡è¯„ä¼°ï¼ŒViTPoseéƒ½å–å¾—äº†æ›´å¥½çš„æˆç»©ï¼Œæ¯”å¦‚ViTPose-Bã€ViTPose-Lå’ŒViTPose-Håˆ†åˆ«å–å¾—äº†93.3ã€94.0å’Œ94.1çš„å¹³å‡PCKhï¼Œå¹¶ä¸”è¾“å…¥åˆ†è¾¨ç‡æ›´å°ï¼ˆ$256 \times 192$ v.s. $256 \times 256$ï¼‰ã€‚å¦‚æœä½¿ç”¨æ›´å¤§çš„è¾“å…¥åˆ†è¾¨ç‡å’Œæ›´å¤§çš„backboneï¼Œæ¯”å¦‚ViTPose-Gï¼Œæ€§èƒ½è¾¾åˆ°äº†æ–°çš„SOTAï¼Œå³94.3çš„PCKhã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/15.png)

ğŸ‘‰**AI Challenger val set.**

ç±»ä¼¼çš„ï¼Œåœ¨AI Challenger val setä¸Šï¼Œæˆ‘ä»¬ä¹Ÿè¯„ä¼°äº†ViTPoseï¼ˆæ­é…ç›¸åº”çš„decoder headï¼‰çš„è¡¨ç°ã€‚ç»“æœè§è¡¨13ï¼Œå’Œä¹‹å‰åŸºäºCNNå’ŒåŸºäºtransformerçš„ä¼˜ç§€æ¨¡å‹ç›¸æ¯”ï¼ŒViTPoseçš„è¡¨ç°æ›´å¥½ï¼Œæ¯”å¦‚ï¼ŒViTPose-Hçš„APä¸º35.4ï¼Œ[HRNet-w48](http://shichaoxin.com/2023/05/13/è®ºæ–‡é˜…è¯»-Deep-High-Resolution-Representation-Learning-for-Visual-Recognition/)çš„APä¸º33.5ï¼ŒHRFromer baseçš„APä¸º34.4ã€‚å¦‚æœä½¿ç”¨æ›´å¤§çš„backboneå’Œæ›´å¤§çš„è¾“å…¥åˆ†è¾¨ç‡ï¼ŒViTPose-Gåˆ·æ–°äº†è¿™ä¸ªæ•°æ®é›†çš„æœ€å¥½æˆç»©ï¼Œå–å¾—äº†43.2çš„APã€‚ä½†æ˜¯æ¨¡å‹åœ¨AI Challenger setè¿™ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¾ç„¶ä¸å¤Ÿå¥½ï¼Œåç»­éœ€è¦è¿›ä¸€æ­¥çš„åŠªåŠ›æå‡ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/16.png)

# 7.B.Detailed dataset details.

æˆ‘ä»¬ä½¿ç”¨MS COCOï¼ŒAI Challengerï¼ŒMPIIå’ŒCrowdPoseç­‰å¤šä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚OCHumanæ•°æ®é›†ä»…ç”¨äºè¯„ä¼°é˜¶æ®µï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨å¤„ç†é®æŒ¡æ•°æ®æ—¶çš„è¡¨ç°ã€‚MS COCOæ•°æ®é›†åŒ…å«118Kå¼ å›¾åƒï¼Œ150Kä¸ªhuman instancesï¼Œæ¯ä¸ªinstanceæœ€å¤šæ ‡æ³¨æœ‰17ä¸ªå…³èŠ‚ç‚¹å¯ç”¨äºè®­ç»ƒã€‚è¯¥æ•°æ®é›†çš„licenseä¸ºCC-BY-4.0ã€‚MPIIæ•°æ®é›†ä½¿ç”¨BSD licenseï¼ŒåŒ…å«15Kå¼ å›¾åƒå’Œ22Kä¸ªhuman instanceså¯ç”¨äºè®­ç»ƒã€‚è¯¥æ•°æ®é›†ä¸­æ¯ä¸ªinstanceæœ€å¤šæ ‡æ³¨æœ‰16ä¸ªå…³èŠ‚ç‚¹ã€‚AI Challengeræ•°æ®é›†æ›´å¤§ï¼ŒåŒ…å«è¶…è¿‡200Kå¼ è®­ç»ƒå›¾åƒå’Œ350Kä¸ªhuman instancesï¼Œæ¯ä¸ªinstanceæœ€å¤šæ ‡æ³¨æœ‰14ä¸ªå…³èŠ‚ç‚¹ã€‚OCHumanåŒ…å«è¢«ä¸¥é‡é®æŒ¡çš„human instancesï¼Œä¸”åªè¢«ç”¨ä½œval and test setï¼Œå…±åŒ…æ‹¬4Kå¼ å›¾åƒå’Œ8Kä¸ªinstancesã€‚

# 7.C.Subjective results

æœ¬èŠ‚åˆ—å‡ºäº†ViTPoseçš„ä¸€äº›å¯è§†åŒ–ç»“æœã€‚AI Challengerçš„ç»“æœè§Fig4ï¼ŒOCHumançš„ç»“æœè§Fig5ï¼ŒMPIIçš„ç»“æœè§Fig6ã€‚

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/17.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/18.png)

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/AIPapers/ViTPose/19.png)

# 8.åŸæ–‡é“¾æ¥

ğŸ‘½[ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://github.com/x-jeff/AI_Papers/blob/master/ViTPoseï¼šSimple%20Vision%20Transformer%20Baselines%20for%20Human%20Pose%20Estimation.pdf)