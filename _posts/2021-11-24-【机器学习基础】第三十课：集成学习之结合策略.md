---
layout:     post
title:      【机器学习基础】第三十课：集成学习之结合策略
subtitle:   平均法（简单平均法、加权平均法），投票法（绝对多数投票法、相对多数投票法、加权投票法），学习法（Stacking）
date:       2021-11-24
author:     x-jeff
header-img: blogimg/20211124.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.结合策略

假定集成包含T个基学习器$\\{h_1,h_2,...,h_T \\}$，其中$h_i$在示例$\mathbf x$上的输出为$h_i(\mathbf x)$。本文介绍几种对$h_i$进行结合的常见策略。

# 2.平均法

对数值型输出$h_i(\mathbf x) \in \mathbb{R}$，最常见的结合策略是使用平均法（averaging）。

👉**简单平均法（simple averaging）**

$$H(x) = \frac{1}{T} \sum^T_{i=1} h_i (x)$$

👉**加权平均法（weighted averaging）**

$$H(x) = \sum^T_{i=1} w_i h_i(x)$$

其中$w_i$是个体学习器$h_i$的权重，通常要求$w_i \geqslant 0, \sum^T_{i=1} w_i =1$。

>非负权重才能确保集成性能优于单一最佳个体学习器，因此在集成学习中一般对学习器的权重施以非负约束。

一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。

# 3.投票法

对于分类任务来说，最常见的结合策略是使用投票法（voting），分为：

* **绝对多数投票法（majority voting）**：即若某标记得票过半数，则预测为该标记；否则拒绝预测。
* **相对多数投票法（plurality voting）**：即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。
* **加权投票法（weighted voting）**：与加权平均法类似。

如果分类器输出的是类标记（例如0,1），则称为“硬投票”。如果分类器输出的是类概率，则称为“软投票”。不同类型的分类器输出值不能混用。

# 4.学习法

当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表。这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器（meta-learner）。

Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。Stacking的算法描述如下图所示：

![](https://xjeffblogimg.oss-cn-beijing.aliyuncs.com/BLOGIMG/BlogImage/MachineLearningSeries/Lesson30/30x1.png)

这里我们假定初级学习器使用不同学习算法产生，即初级集成是异质的。

>初级学习器也可是同质的。

* 过程2：使用初级学习算法$\mathcal{L} _t$产生初级学习器$h_t$。
* 过程4：生成次级训练集。
* 过程11：在$\mathcal{D} ' $上用次级学习算法$\mathcal{L}$产生次级学习器$h'$。

在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；因此，一般是通过使用交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。

>以k折交叉验证为例，初始训练集D被随机划分为k个大小相似的集合$D_1,D_2,...,D_k$。令$D_j$和$\bar{D}\_j=D\backslash D\_j$分别表示第j折的测试集和训练集。给定T个初级学习算法，初级学习器$h_t^{(j)}$通过在$\bar{D}_j$上使用第t个学习算法而得。对$D_j$中每个样本$\mathbf{x}_i$，令$z\_{it}=h\_t^{(j)} (\mathbf{x}\_i)$，则由$\mathbf{x}_i$所产生的次级训练样例的示例部分为$\mathbf{z}\_i=(z\_{i1};z\_{i2};...;z\_{iT})$，标记部分为$y_i$。于是，在整个交叉验证过程结束后，从这T个初级学习器产生的次级训练集是$D'=\\{(\mathbf{z}\_i,y\_i) \\}^m\_{i=1}$，然后$D'$将用于训练次级学习器。

次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能有很大影响。有研究表明，将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归（Multi-response Linear Regression，简称MLR）作为次级学习算法效果较好，在MLR中使用不同的属性集更佳。

>MLR是基于线性回归的分类器，它对每个类分别进行线性回归，属于该类的训练样例所对应的输出被置为1，其他类置为0；测试示例将被分给输出值最大的类。