---
layout:     post
title:      【机器学习基础】第五课：偏差与方差
subtitle:   偏差-方差分解，噪声，偏差-方差窘境，代价的类型
date:       2019-04-17
author:     x-jeff
header-img: blogimg/20190417.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.偏差-方差分解

对学习算法除了通过实验估计其泛化能力，人们往往还希望了解它“为什么”具有这样的性能。“偏差-方差分解”(bias-variance decomposition)是解释学习算法泛化性能的一种重要工具。

偏差-方差分解试图对学习算法的期望泛化错误率进行拆解。

|符号|解释|
|:-:|:-:|
|$x$|测试样本|
|$D$|训练集|
|$y_D$|训练集中样本的标签|
|$y$|训练集中样本的真实标记|
|$f$|训练集$D$学得的模型|
|$f(x;D)$|由训练集$D$学得的模型$f$对$x$的预测输出|
|$\overline{f}(x)$|模型$f$对$x$的期望预测输出|

⚠️有可能出现噪声使得$y_D\neq y$。

👉以回归任务为例：

学习算法的**期望**预测为($\mathbb{E}$为期望符号)：

$$\overline{f}(x)=\mathbb{E}_D[f(x;D)]$$

使用**样本数相同**的**不同训练集**产生的**方差**为：

$$var(x)=\mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]$$

噪声为：

$$\varepsilon^2=\mathbb{E}_D[(y_D-y)^2]$$

期望输出与真实标记的差别称为**偏差**，即：

$$bias^2(x)=(\overline{f}(x)-y)^2$$

为便于讨论，假定噪声期望为零，即$\mathbb{E}_D[y_D-y]=0$。

对算法的期望泛化误差进行分解：

$$\begin{align} \sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\ & = \sqrt{\frac{73^2}{12^2} \cdot \frac{73^2-1}{73^2}} \\ & = \frac{73}{12} \sqrt{1 - \frac{1}{73^2}} \\ & \approx \frac{73}{12} \left( 1 - \frac{1}{2 \cdot 73^2} \right) \end{align}$$