---
layout:     post
title:      【机器学习基础】第二十九课：集成学习之Bagging与随机森林
subtitle:   Bagging，“包外估计”（out-of-bag estimate），随机森林（Random Forest）
date:       2021-11-09
author:     x-jeff
header-img: blogimg/20211109.jpg
catalog: true
tags:
    - Machine Learning Series
---
>【机器学习基础】系列博客为参考周志华老师的《机器学习》一书，自己所做的读书笔记。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.前言

由[【机器学习基础】第二十七课：集成学习之个体与集成](http://shichaoxin.com/2021/10/12/机器学习基础-第二十七课-集成学习之个体与集成/)可知，欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考虑使用相互有交叠的采样子集。

# 2.Bagging

给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。初始训练集中约有63.2%的样本出现在采样集中（[推导过程](http://shichaoxin.com/2018/11/27/机器学习基础-第二课-模型评估方法/#23自助法)）。

照这样，我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是Bagging的基本流程。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。Bagging的算法描述见下：

![](https://github.com/x-jeff/BlogImage/raw/master/MachineLearningSeries/Lesson29/29x1.png)

>$\mathcal{D}_{bs}$是自助采样产生的样本分布。

Bagging是一个很高效的集成学习算法。值得一提的是，自助采样过程还给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对泛化性能进行“包外估计”（out-of-bag estimate）。为此需记录每个基学习器所使用的训练样本。不妨令$D_t$表示$h_t$实际使用的训练样本集，令$H^{oob}(\mathbf x)$表示对样本$\mathbf x$的包外预测，即仅考虑那些未使用$\mathbf x$训练的基学习器在$\mathbf x$上的预测，有：

$$H^{oob}(\mathbf x)=\arg \max \limits_{y \in \mathcal{Y}} \sum^T_{t=1} \mathbb{I} (h_t(\mathbf x)=y) \cdot \mathbb{I} (\mathbf x \notin D_t)$$

则Bagging泛化误差的包外估计为：

$$\epsilon^{oob} = \frac{1}{\lvert D \rvert} \sum_{(\mathbf x,y)\in D} \mathbb{I} (H^{oob}(\mathbf x) \neq y)$$

事实上，包外样本还有许多其他用途。例如当基学习器是决策树时，可使用包外样本来辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合风险。

从[偏差-方差分解](http://shichaoxin.com/2019/04/17/机器学习基础-第五课-偏差与方差/)的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。

# 3.随机森林

**随机森林（Random Forest，简称RF）**是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐值$k=\log_2 d$。

随机森林简单、容易实现、计算开销小，令人惊奇的是，它在很多现实任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”。可以看出，随机森林对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。

值得一提的是，随机森林的训练效率常优于Bagging，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。