---
layout:     post
title:      【CUDA编程】【14】【3.Programming Interface】【3.2.CUDA Runtime】【3.2.11.Interprocess Communication】
subtitle:   Interprocess Communication，IPC
date:       2024-12-08
author:     x-jeff
header-img: blogimg/20210714.jpg
catalog: true
tags:
    - CUDA C++ Programming Guide
---
>【CUDA编程】系列博客参考NVIDIA官方文档[“CUDA C++ Programming Guide（v12.6）”](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)。  
>本文为原创文章，未经本人允许，禁止转载。转载请注明出处。

# 1.Interprocess Communication

由host线程创建的任何device内存指针或event句柄都可以被同一进程中的其他线程直接引用。然而，它在该进程之外是无效的，所以无法被属于不同进程的线程直接引用。

为了在进程之间共享device内存指针和event，应用程序必须使用进程间通信（Inter Process Communication，IPC）的API。IPC API仅支持Linux上的64位进程，并且仅适用于计算能力大于等于2.0的device。需要注意的是，IPC API不支持`cudaMallocManaged`分配的内存。

通过使用IPC API，应用程序可以使用`cudaIpcGetMemHandle()`获取给定device的内存指针的IPC句柄，通过标准的IPC机制（比如，进程间共享内存或文件）将其传递给另一个进程，并使用`cudaIpcOpenMemHandle()`从IPC句柄中检索device指针，该指针在另一个进程中是有效的。event句柄也可以使用类似的方式进行共享。

需要注意的是，为了性能考虑，通过`cudaMalloc()`分配的内存可能来自于更大内存块的子分配。这种情况下，CUDA IPC API会共享整个底层内存块，这可能导致其他子分配也被共享，从而可能导致进程间的信息泄露。为了防止这种情况，建议仅共享对齐到2MiB大小的内存（因为如果未对齐到2MiB边界，可能会暴漏其他进程的数据）。

IPC API的一个使用示例是单个主进程生成一批输入数据，使这些数据可供多个次级进程使用，而无需重新生成或拷贝。

使用CUDA IPC进行通信的应用程序应使用相同的CUDA驱动程序和运行时进行编译、链接和运行。

>注意：
>
>自CUDA 11.5起，仅在具有计算能力7.x或更高的L4T和嵌入式Linux Tegra device上支持event共享的IPC API。内存共享的IPC API仍然不支持Tegra平台。
